\chapter{Factor Interpretation}

\section{Why this chapter}

Factor interpretation is a difficult and subjective process. Yet it forms a key
link between the objective, uninterpreted results reported in the previous 
section and the conclusions and observations in the discussion chapter. So, I
have written this chapter to report the interpretation process as fully as 
possible.

\section{Smaller Word Lists}

The factors of the smaller word-lists are, at best, only interpretable in 
hindsight. I looked at them first in isolation and was unable to
come up with coherent descriptions. Even after seeing components generated from
the larger lists, only the components from MDS-transformed smaller lists have 
a recognizable organizing principle. For example, it is clear that the third 
component is sorted on formality and the fourth is related to how often the 
word is used to describe nonhuman versus human characteristics. Based on Figure 
\ref{fig:100vs2797First22} I believe that dimensions 1, 2, 6, 7, 8, 9 and 11
are also be interpretable in hindsight.

\section{Interpreting 2797 Word with MDS}

The first straightforwardly interpretable set of components came from the 2797 
word list based on \citep{Norman1967}. I think more than the 14 components I 
examined might be interpretable. My interpretations are in Table
\ref{tab:2797mdsinterp}.

\subsection{Component 1 (Parliament usage)}

How I came to interpret the first dimension as being related to parliamentary 
usage needs a bit of explanation. First, I observed that the dimension was 
related to word frequency. In the corpus, the 10 highest ranked words appear in 
526,515/117,296,559 sentences (0.4\%), the 10 lowest ranked words appear in only 
5,871 (0.005\%). This relationship is even more pronounced in the minutes of the 
European parliament where the highly ranked words appear in 19,196/2,015,440 
sentences (1\%) but the lowest ranked words appear in 14 (0.0006\%) (And, in all 
of those sentences, the only word appearing is sly\_jj - the highest ranked word 
among the lowest 10). Thus, it seemed that these words are not just more common, 
but that they are more common in political speech.

To further investigate what I had observed, I went from using ranked frequencies 
to using full correlations. I looked at the Spearman correlation between number 
of occurrences of the words in the entire corpus and the values on the first 
dimension. It was 0.5739 (which gives a $p$ value of $1.9120 \times 10^{-163}$ 
against the null hypothesis that there is no relationship). There is no other 
component which is so strongly correlated with word frequency. However, the 
correlation is not perfect. For example, the most frequent word in the list has 
a rank of 204 when you look at its score on the first dimension.

To investigate the hypothesis that these words are not just more common but more 
common in political speech, I also looked at the correlation with frequency in 
the European parliament records. The correlation between this component and 
parliamentary frequency is 0.6881 ($p=3.8268 \times 10^{-261}$).  It rises even 
farther to 0.7027 when looking at only words that appear in the parliamentary 
records. That it is not a perfect correlation indicates that there are still 
things I am overlooking. However, describing this dimension as ``uncommon in 
parliament vs common'' seems to have some explanatory power.

\begin{table}[tbp]
\begin{tabular}{ | l |p{2in}|p{2in}|}
 \hline
 C & Low & High \\
 \hline
 1 & Uncommon in parliament & Common in parliament\\
 2 & Negative & Positive\\
 3 & Mainly describes humans & Describes non-humans also \\
 4 & Informal & Formal \\
 5 & Used in conflict & Used in peace \\
 6 & Unrelated to patriotism or authority-group loyalty & Related to 
     patriotism or authority-group loyalty \\
 7 & Stereotypical rebellious intellectual & Stereotypical opponent of 
     rebellious intellectual \\
 8 & Less emotional words & Emotionally charged words \\
 9 & Social / ostentation & Private  \\
 10 & Business context & Religion / family / clan context \\
 11 & Qualities relevant to violent situations & Qualities relevant to 
       non-violent situations \\
 12 & Unsettled exploration & Restaurant and food \\
 13 & Travel words & Interview words \\
 14 & Words that can describe both food and politicians &  Unknown \\
 \hline
\end{tabular}
 \caption{Interpretations of the different components identified from the 2797
 word list after MDS}
 \label{tab:2797mdsinterp}
\end{table}

\section{All combined with MDS}

When I combine all the word lists and apply MDS, the factors look similar to 
the 2797 word list by itself. See Table \ref{tab:2797and438and101mdsinterp}. 
There is a bit of variation in factors 5, 6, and 9 but not enough to really 
change the interpretation that much.

\begin{table}[tbp]
\begin{tabular}{ | l |p{2in}|p{2in}|}
 \hline
 C & Low & High \\
 \hline
 1* & Uncommon in parliament & Common in parliament\\
 2* & Negative & Positive\\
 3* & Mainly describes humans & Describes non-humans also \\
 4* & Informal & Formal \\
 5* & Used in peace & Used in conflict \\
 6 & Unrelated to group devotion & Group devotion \\
 7* & Stereotypical rebellious intellectual & Stereotypical opponent of 
     rebellious intellectual \\
 8* & Less emotional words & Emotionally charged words \\
 9* & Social / ostentation & Private  \\
 \hline
\end{tabular}
 \caption{Interpretations of the different components identified from the combining
 all three word lists after MDS. * means the same as the 2797 interpretation.}
 \label{tab:2797and438and101mdsinterp}
\end{table}


\section{2797 with z-score}

The z-score normalized factors, even in the 2797 word list were much harder to 
interpret than the MDS normalized ones. There are also more exceptions to each 
rule. This is to be expected from the difference between the dot-product 
space in which the similarity actually is measured and the euclidean space in 
which the z-score and PCA combination operates. The rough interpretation of 
the first four factors is in Table \ref{tab:2797zscoreinterp}.

\begin{table}[tbp]
\begin{tabular}{ | l |p{2in}|p{2in}|}
 \hline
 C & Low & High \\
 \hline
 1 & Business & Fashion \\
 2 & Socially pleasant & Socially unpleasant \\
 3 & Leadership qualities & Words unrelated to leadership \\
 4 & Irrational commitment & Words that describe art \\
 \hline
\end{tabular}
 \caption{Interpretations of the different components identified from the 
 2797 word list after z-score normalization.}
 \label{tab:2797zscoreinterp}
\end{table}

\chapter{Discussion}

\section{Many ways to approach meaning}

In the first 14 components of the MDS basis for the 2797 word set, the word
``sly\_jj'' can be represented in percentiles as (2, 59, 59, 85, 20, 59, 41, 
53, 36, 81, 46, 25, 68, 40). With a range of 30-70\% for average, this means 
that sly as an adjective is very non-parliamentary, quite informal, useful in 
conflict, is seen more often when discussing matters of social allegiance than
economic, and is characteristic of unsettled exploration. Otherwise, sly is an
average word. It is hard to deny that these categories capture some 
understanding of the word sly and how it is used. However, this description of
the word sly is quite different from it's dictionary definitions: 
``clever in concealing one's aims or ends,'' ``lacking in straightforwardness 
and candor,'' and ``lightly mischievous.'' \citep{SlyMerriamWebster2014}

One could approach this result by saying that the computer is wrong, that the
algorithm has not captured the meaning of the word sly. But this is not very
attractive because using this same computer definition of the meaning of sly,
the computer can find other words which everyone would agree have similar 
meanings to sly and also find words in other languages that by general 
consensus have the same meaning as sly. To say that it can do this without
having captured at least the majority of the meaning of sly is a hard claim
to believe. 

Instead, I propose that the computer is working in an analogous semantic space 
to humans, but it has divided it up differently. A simple version of this 
happens across different languages. A native Spanish speaker from Colombia calls 
many things ``verde'' that I, a native English speaker, call ``blue.'' ``verde'' 
is usually glossed as ``green.'' The Colombians divide up the color world 
slightly differently, so that although ``verde'' is normally ``green,'' for some 
things it is ``blue.'' Similarly, the process of training the \modelname{} model 
has produced a division of the semantic space focusing on the characteristics of 
the words and their usage. Humans focus instead on the referents of the words. 
In its training, the computer algorithm did not have access to the embodied 
experiences that humans take for granted and could not take them into account in 
its model. These would have to be additional latent variables of a different 
type.

I see the computer's model in a similar way to the earlier example of PCA in 
Figure \ref{fig:pcarotation}. The principal components captured very important
characteristics of the data that was presented. And, provided information in 
the right 
form so accurate predictions could be made. However, because it was the wrong
type of model, it it couldn't capture the clumpiness of the data or the 
shape of those clumps. Thus it missed many important features of the space. In 
the same way
the \modelname{} model captures important features of the lexical semantic space
in a useful form. However, it is not a human brain and does not have the 
experiences that affect how a human responds to a given word. As such, it has
both the wrong model and incomplete input data. However, given those limitations
it does a remarkable job.

\section{Things captured by \modelname{}}

I think the first six dimensions of the MDS \modelname{} model captured
 interesting characteristics of personality language in the WMT11 dataset.
%
\begin{enumerate}
 \item Difference in usage for the parliamentary subset
 \item Negative/positive
 \item Human/general
 \item Fighting words
 \item Group loyalty
\end{enumerate}

\subsection{Parliamentary usage}

On the one hand, this dimension is an artifact of how the model was trained.
The parliamentary dataset was not mixed in with the other data but rather was
first. Since the model has its largest learning rate at the beginning, it moved
swiftly to represent parliamentary usage. Only words that did not appear often
in the parliamentary dataset escaped this initial movement. Because the 
parliamentary records are a small fraction of the entire dataset, if I had 
shuffled the entire dataset rather than just concatenating the files as they 
came I believe it would have had much less influence.

On the other hand, the presence of this artifact is another confirmation that
indeed the model is picking up the nuances of the text. It was able to pick up
the subtle differences in word sense between news writing and political speech.
This sort of sensitivity is exactly what makes it such a good tool for exploring
language and its usage.

It also implies that we might be able to get at usage differences by starting a
model with a small dataset whose characteristics we are interested in and then
continuing with a larger dataset. The differences between a dataset trained in
this way and one trained exclusively with the larger dataset would reflect 
differences in the usage between them while still taking advantage of the richer
semantic network available with the larger dataset. It would be interesting to
compare this procedure with those of: just training one model for each dataset
and training one model for the smaller dataset and one an equal-sized subset of
the larger dataset.

\subsection{Negative/Positive}

As was noted earlier, humans have a tendency to rate everything as negative and
positive \citep{Samsonovich2010}. Thus, its appearance in personality words
is not surprising. Studies have detected negative and positive factors
in Hebrew, Spanish, Filipino, and Greek \citep[p. 134]{DeRaad2009}. 

\subsection{Human/General}

The most interesting dimension to come out of the analysis is the third one.
It seems to measure the degree to which a word is specific to humans rather than
only being applied to them by analogy. 
If it indeed measures the degree of polysemy in personality words, we may be 
able to focus in on just the words that most matter to the study of personality
and look at their characteristics. Additionally, we may be able to 
semi-automatically produce a list of candidate words in an objective manner by
finding other words that are very human specific. This would help make up for
the original, human word lists' arbitrary exclusion of particular forms of 
different
personality words and help make up for the meaning shift that occurs in the 
natural evolution of language.

\subsection{Formality, Belligerence, and Group devotion}

The final three categories are all related to the social context of speech. In
American English, it is easy to recognize that many word forms exist to express
nuances of formality\footnote{To put it less formally: ``We change our
talk so we can be one of the guys.''}. Conflict and loyalty are also important
parts of our social lives, so it makes sense that they would be important in
our description of others.

\section{Superiority of MDS}

These important dimensions only were apparent in the MDS scaled vectors. I think
it is safe to say that in this case, the Euclidean distance metric was not a
good enough approximation to the cosine distance. Part of that may be because
we were not working with unit vectors. If one were to normalize all the vectors 
to unit length before doing the z-score transformation, it might make the 
dimensions obtained through standard PCA more usable.

\section{Where are the personality dimensions?}

Even with unit vectors, through, I believe that we will not find the Big Five or 
another major personality model in this data. First, those models come about 
because of the covariance structure of personality in single individuals; if an 
individual is trusting, he is more likely to be altruistic. In the WMT11 
dataset, there are descriptions of individuals, but they are all mashed up 
together. Using several adjectives to describe a single individual is not done 
very often. And when it is done, it is frequently done in different sentences 
(which will completely remove the connection due to sentence order 
randomization). Since most occurrences of the trait words are describing 
different individuals, there is no way for the algorithm to pick up on the 
commonalities. Second, a less certain reason: as was stated above, the model is 
looking more at the characteristics of the words rather than at what they refer 
to. I think that the personality models are more closely related to the referent 
than to the way in which it is described.

\section{Consistency across different word-sets}

Before doing this experiment, it would have been conceivable that each word list
would produce a different set of factors. You quadruple the size of the 101
word list to get the 438 word list and you more than quintuple the 438 word list
to get the 2797 word list. Even after attrition due to having to select only 
words in the vocabulary, each step up in size more than trebles the quantity of
words. If the new words' vectors were not related to the old words the factors
due to the new words would swamp the existing words and become the new principal
components. However, Section \ref{sec:dimensionmapping} demonstrates 
that most of the first 20 dimensions are shared between all of the sets. 
Additionally, the comparison between the 438 and 2797 word sets in Table 
\ref{439and100-vs-2797-from-800dim-lowercase-wmt-model-significant.tex} makes
me believe that many of the first 60-70 dimensions are also shared.

Because of possible non-linearity, I could not automatically determine the 
number of significant dimensions. However, since the 438 word set shares 37 of
its dimensions with the 2797 word sets, up to 37 significant dimensions seems 
possible. To really know how many are significant and how many are an artifact
of the particular training set, one would need to repeat the experiment with 
a different training set. The more different the training set is from WMT11, 
the more confidence you can have that any common dimensions are due to real,
underlying factors rather than accidental corpus peculiarities.


\chapter{Future Work}


There are myriad options that can be pursued to improve and extend this work.
I think that three would be first steps: utilize the human-nonhuman dimension,
re-do the experiment with improved procedure, and see what dimensions come
out of \citep{Samsonovich2010}'s embedding.

\section{Utilize the human-nonhuman dimension}

Using the third MDS component, I can separate the low-polysemy words from the 
rest. Then, I can analyze these words as another word-list. It is possible
that this will eliminate some of the odder dimensions like ``restaurant'' and
``travel'' and make some of the rest more interpretable.

\section{Redo with improved procedure}

There are a number of minor problems with this work that I did not have time to
fix: no clear number of personality factors, no good way of choosing how many 
dimensions the original model should have, the parliament dimension, the 
MDS analysis lacks any analog to z-score scaling, the covariance PCA is not
preceded by normalizing the vectors to unit length, and the dimensions might
be more interpretable with varimax rotation.

So, it would be nice to enact the following procedure:
\begin{enumerate}
 \item Train on small datasets and extrapolate to the number of dimensions 
 needed for the large dataset
 \begin{enumerate}
    \item Choose a starting subset size $s$ (say 5000 sentences) and a starting
    dimensionality $d$ (say 10)
    \item \label{itm:dotimewarpagain} Select a random subset of $s$ sentences 
    from the Gigaword corpus and also from the WMT11 corpus
    \item \label{itm:innerloop} Train two models of dimension $d$ and calculate
    how many dimensions they have in common using Canonical Correlations Analysis.
    \item Adjust $d$ and go back to \ref{itm:innerloop} to maximize the number 
    of common dimensions.
    \item Record $s$ and $d$.
    \item $s = s\times 1.1$ and go back to \ref{itm:dotimewarpagain} until
    the outer loop takes longer than a threshold
    \item Finally, fit a function to the $(s,d)$ pairs and extrapolate to the 
    number of sentences in the smaller of the Gigaword and WMT11 corpus to find
    out the best number of dimensions.
 \end{enumerate}
 \item Preprocess and shuffle the lines of the two corpora.
 \item Train two models, one on each corpus and extract the vectors for each.
 \item Use a randomly selected list of 5000 words from the vocabulary common 
 to both models to rotate the models so that they align.
 \item Unitize the vectors and use directional statistics to reduce the angles
 to have mean 0 and unit standard deviation.
 \item Perform MDS on both sets of vectors
 \item Use CCA to extract factors until we fail a to reject the hypothesis that
 the factor is random (and use sequential multiple test correction to ensure
 that we control the family-wise error rate)
 \item Take the factors extracted from one of the datasets and make a new matrix
 then take the transpose of that matrix (so we have a matrix in which the 
 columns are the words and the rows are the \modelname{} dimensions)
 \item Perform PCA on that matrix. 
 \item Take the loadings matrix (which will be factors x items) and performe
 varimax rotation to enhance interpretability
 \item Look at the words with $|\text{loading}| \ge 0.4$ on a particular 
 dimension to characterize that dimension.
\end{enumerate}


\section{Look at Samsonovich embedding from }

\citep{Samsonovich2010} only found 4 dimensions. However, I think that may have been 
because he had too many semantic categories in his model. I would like to
write to him and ask for his trained vectors to see if more dimensions come out
when only a semantically coherent subset of the words are used. Because his
data are based on direct human meaning assertions (synonym and antonym 
relations) it should better reflect the human concept of meaning. I am 
curious whether it thus will reflect the human concept of personality.
