Automatically generated by Mendeley Desktop 1.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Mikolov2013,
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40\% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.},
address = {Atlanta, GA},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
booktitle = {Proceedings of NAACL-HLT},
file = {:home/eric/Papers/Proceedings of NAACL-HLT/2013 - Linguistic regularities in continuous space word representations - Mikolov, Yih, Zweig.pdf:pdf;:home/eric/Papers/Proceedings of NAACL-HLT/2013 - Linguistic regularities in continuous space word representations - Mikolov, Yih, Zweig(2).pdf:pdf},
keywords = {natural language processing,neural networks,semantics},
mendeley-tags = {natural language processing,neural networks,semantics},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://acl.eldoc.ub.rug.nl/mirror/N/N13/N13-1090.pdf},
year = {2013}
}
@techreport{Norman1967,
abstract = {All terms in contemporary American English which pertain to aspects of human behavior or personal characteristics were assembled from available lexicons. These terms were categorized into 15 rubrics on the basis of judgments of their familiarity, specificity, and certain broad semantic criteria. Some 2,800 terms were identified which seemingly referred to relatively stable and specific "biophysical" traits of individuals. These words were presented to groups of university undergraduates to determine familiarity levels, specificity of connotative meaning, and a variety of psychometric operating characteristics (e.g., endorsement rates for self and for others, desirability, etc.). Results of the analysis of these data are presented and some of their potential uses for test development and personality description are suggested. Additional analyses currently in progress directed toward further refinement of the set and the development of a structured taxonomy based on these descriptors are briefly outlined.},
address = {Ann Arbor, MI},
author = {Norman, WT},
file = {:home/eric/Papers/Unknown/1967 - 2800 PERSONALITY TRAIT DESCRIPTORS--NORMATIVE OPERATING CHARACTERISTICS FOR A UNIVERSITY POPULATION. - Norman.pdf:pdf},
institution = {University of Michigan Department of Psychology},
keywords = {lexical hypothesis,personality,psychology},
mendeley-tags = {lexical hypothesis,personality,psychology},
title = {{2800 PERSONALITY TRAIT DESCRIPTORS--NORMATIVE OPERATING CHARACTERISTICS FOR A UNIVERSITY POPULATION.}},
url = {http://files.eric.ed.gov/fulltext/ED014738.pdf},
year = {1967}
}
@inproceedings{Sergienya2013,
abstract = {There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.},
archivePrefix = {arXiv},
arxivId = {1312.5559},
author = {Sergienya, Irina and Sch\"{u}tze, Hinrich},
booktitle = {ICLR Workshop},
eprint = {1312.5559},
file = {:home/eric/Papers/ICLR Workshop/2013 - Distributional Models and Deep Learning Embeddings Combining the Best of Both Worlds - Sergienya, Sch\"{u}tze.pdf:pdf},
keywords = {natural language processing},
mendeley-tags = {natural language processing},
month = dec,
pages = {4},
title = {{Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds}},
url = {http://arxiv.org/abs/1312.5559},
year = {2013}
}
@inproceedings{Faruqui2014,
abstract = {The distributional hypothesis of Harris (1954), according to which the meaning of words is evidenced by the contexts they occur in, has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora. This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually. We evaluate the resultingword representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques},
author = {Faruqui, Manaal and Dyer, Chris},
booktitle = {Proc. of EACL},
file = {:home/eric/Papers/Proc. of EACL/2014 - Improving Vector Space Word Representations Using Multilingual Correlation - Faruqui, Dyer.pdf:pdf},
keywords = {continuous skip-gram citation,machine translation},
mendeley-tags = {continuous skip-gram citation,machine translation},
title = {{Improving Vector Space Word Representations Using Multilingual Correlation}},
url = {http://www.cs.cmu.edu/~mfaruqui/papers/eacl14-vectors.pdf},
year = {2014}
}
@misc{Osendorfer2013,
abstract = {Recent research from Microsoft shows that “King - Man + Woman” almost equals “Queen” [1]. An open source project from Google that comes to the same conclusion offers an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. We are looking into expanding this work to non-verbal domains, in particular to music. While Google trains their models on data sets of billions of words, we will train our models on data sets of up to hundreds of millions of notes. Apart from providing a sound and scalable way of computing musical similarity and introducing the notions of analogy and metaphor to music - a significant contribution to musicology in its own right - this highly publishable research will have many synthetic/creative applications. For example, using the intermediate vector representation for translating language to music and back, which has never been done before.},
author = {Osendorfer, Christian and Viro, Vladimir},
file = {:home/eric/Papers/Unknown/2013 - Scalable Vector Arithmetics for Music using RNNs - Osendorfer, Viro.pdf:pdf},
keywords = {continuous skip-gram citation},
mendeley-tags = {continuous skip-gram citation},
pages = {1},
title = {{Scalable Vector Arithmetics for Music using RNNs}},
year = {2013}
}
@inproceedings{Nowson2005,
abstract = {Blogs are personal online diaries, and a relatively recent form of computer-mediated communication. What kind of writing do they contain? This paper adopts ameasure of linguistic contextuality/formality, due to Heylighen and Dewaele, and applies it to a corpus of weblogs. It first compares the corpus with sub-corpora from the British National Corpus, and weblogs are shown to be more formal than e-mail, but less formal than biographies. Then, the paper explores the impact of individual differences between writers on their texts’ contextuality/formality. It appears that Extraversion and Neuroticism are less influential than previously supposed, and it is argued that gender and Agreeableness account for more of the variability in the extent to which weblog writers take their readers’ contexts into account.},
address = {Stresa, Italy},
author = {Nowson, Scott and Oberlander, Jon and Gill, Alastair J},
booktitle = {Proceedings of the 27th \ldots},
file = {:home/eric/Papers/Proceedings of the 27th \ldots/2005 - Weblogs, genres and individual differences - Nowson, Oberlander, Gill.pdf:pdf},
pages = {1666--1671},
title = {{Weblogs, genres and individual differences}},
url = {http://csjarchive.cogsci.rpi.edu/Proceedings/2005/docs/p1666.pdf},
year = {2005}
}
@article{Samsonovich2013,
abstract = {The emergent consensus on dimensional models of sentiment, appraisal, emotions, and values is on the semantics of the principal dimensions, typically interpreted as valence, arousal, and dominance. The notion of weak semantic maps was introduced recently as distribution of representations in abstract spaces that are not derived from human judgments, psychometrics, or any other a priori information about their semantics. Instead, they are defined entirely by binary semantic relations among representations, such as synonymy and antonymy. An interesting question concerns the ability of the antonymy-based semantic maps to capture all "universal" semantic dimensions. The present work shows that those narrow weak semantic maps are not complete in this sense and can be augmented with other semantic relations. Specifically, including hyponym-hypernym relations yields a new semantic dimension of the map labeled here "abstractness" (or ontological generality) that is not reducible to any dimensions represented by antonym pairs or to traditional affective space dimensions. It is expected that including other semantic relations (e.g., meronymy/holonymy) will also result in the addition of new semantic dimensions to the map. These findings have broad implications for automated quantitative evaluation of the meaning of text and may shed light on the nature of human subjective experience.},
author = {Samsonovich, Alexei V and Ascoli, Giorgio a},
doi = {10.1155/2013/308176},
file = {:home/eric/Papers/Computational intelligence and neuroscience/2013 - Augmenting weak semantic cognitive maps with an abstractness dimension. - Samsonovich, Ascoli.pdf:pdf},
issn = {1687-5273},
journal = {Computational intelligence and neuroscience},
keywords = {Brain,Brain Mapping,Brain: physiology,Comprehension,Comprehension: physiology,Humans,Semantics,Vocabulary},
month = jan,
pages = {308176},
pmid = {23840200},
title = {{Augmenting weak semantic cognitive maps with an "abstractness" dimension.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3694378\&tool=pmcentrez\&rendertype=abstract},
volume = {2013},
year = {2013}
}
@inproceedings{Mikolov2013b,
abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {arXiv preprint arXiv:1301.3781},
eprint = {1301.3781},
file = {:home/eric/Papers/arXiv preprint arXiv1301.3781/2013 - Efficient Estimation of Word Representations in Vector Space - Mikolov et al.pdf:pdf},
month = jan,
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781v3},
year = {2013}
}
@article{Perozzi2014,
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide \$F\_1\$ scores up to 10\% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60\% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
archivePrefix = {arXiv},
arxivId = {1403.6652},
author = {Perozzi, Bryan and Al-Rfou', Rami and Skiena, Steven},
eprint = {1403.6652},
file = {:home/eric/Papers/Unknown/2014 - DeepWalk Online Learning of Social Representations - Perozzi, Al-Rfou', Skiena.pdf:pdf},
keywords = {continuous skip-gram citation},
mendeley-tags = {continuous skip-gram citation},
month = mar,
pages = {10},
title = {{DeepWalk: Online Learning of Social Representations}},
url = {http://arxiv.org/abs/1403.6652},
year = {2014}
}
@misc{Dao2013,
abstract = {The task of computationally learning the linguistic application of a word requires a robust analysis of both the word’s semantic presence and its independent characteristics. Word vectors are learned, functional representation of words which can then be analyzed for linguistic regularities. We examine whether the synonyms of a word can be recognized from their word vectors by training a neural network on a large corpus of text, then implementing k-means clustering to check whether synonyms have a statistically significant word vector similarity. We illustrate the output using a simplified version of the vectors generated by principal component analysis. Results suggest that increasing vector length, which improves syntactic and semantic accuracy when performing comparisons of words, negatively correlates with synonym recognition.},
author = {Dao, Tri and Keller, Sam and Bejnood, Alborz},
file = {:home/eric/Papers/Unknown/2013 - Alternate Equivalent Substitutes Recognition of Synonyms Using Word Vectors - Dao, Keller, Bejnood.pdf:pdf},
keywords = {continuous skip-gram citation},
mendeley-tags = {continuous skip-gram citation},
pages = {2--6},
publisher = {Stanford University},
title = {{Alternate Equivalent Substitutes : Recognition of Synonyms Using Word Vectors}},
year = {2013}
}
@inproceedings{Schwartz2013a,
abstract = {Language in social media reveals a lot about people’s personality and mood as they discuss the activities and relationships that constitute their everyday lives. Although social media are widely studied, researchers in computational linguistics have mostly focused on prediction tasks such as sentiment analysis and authorship attribution. In this paper, we showhowsocial media can also be used to gain psychological insights. We demonstrate an exploration of language use as a function of age, gender, and personality from a dataset of Facebook posts from 75,000 people who have also taken personality tests, and we suggest how more sophisticated tools could be brought to bear on such data.},
author = {Schwartz, H Andrew and Eichstaedt, Johannes C and Dziurzynski, Lukasz and Kern, Margaret L and Seligman, Martin E. P. and Ungar, Lyle H. and Blanco, Eduardo and Kosinski, Michal and Stillwell, David},
booktitle = {2013 AAAI Spring Symposium},
file = {:home/eric/Papers/2013 AAAI Spring Symposium/2013 - Toward Personality Insights from Language Exploration in Social Media - Schwartz et al.pdf:pdf},
keywords = {facebook,personality,social media},
mendeley-tags = {facebook,personality,social media},
pages = {72--79},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{Toward Personality Insights from Language Exploration in Social Media}},
url = {http://www.aaai.org/ocs/index.php/SSS/SSS13/paper/download/5764/5915},
year = {2013}
}
@inproceedings{Pennacchiotti2011,
abstract = {This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user’s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.},
author = {Pennacchiotti, Marco and Popescu, Ana-Maria},
booktitle = {ICWSM},
file = {:home/eric/Papers/ICWSM/2011 - A Machine Learning Approach to Twitter User Classification. - Pennacchiotti, Popescu.pdf:pdf},
keywords = {machine learning,natural language processing},
mendeley-tags = {machine learning,natural language processing},
pages = {281--288},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{A Machine Learning Approach to Twitter User Classification.}},
url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2886/3262},
year = {2011}
}
@article{Allport1936,
author = {Allport, Gordon W. and Odbert, Henry S.},
doi = {10.1037/h0093360},
file = {:home/eric/Papers/Psychological Monographs/1936 - Trait-names A psycho-lexical study. - Allport, Odbert(2).pdf:pdf},
issn = {0096-9753},
journal = {Psychological Monographs},
keywords = {lexical hypothesis,personality,psychology},
mendeley-tags = {lexical hypothesis,personality,psychology},
number = {1},
pages = {i--171},
title = {{Trait-names: A psycho-lexical study.}},
url = {http://psycnet.apa.org/journals/mon/47/1/i/ http://doi.apa.org/getdoi.cfm?doi=10.1037/h0093360},
volume = {47},
year = {1936}
}
@article{Norouzi2013,
abstract = {Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional $\backslash$nway\{\} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing $\backslash$nway\{\} image classifier and a semantic word embedding model, which contains the \$\backslash n\$ class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.},
archivePrefix = {arXiv},
arxivId = {1312.5650},
author = {Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S. and Dean, Jeffrey},
eprint = {1312.5650},
file = {:home/eric/Papers/arXiv1312.5650/2013 - Zero-Shot Learning by Convex Combination of Semantic Embeddings - Norouzi et al.pdf:pdf},
journal = {arXiv:1312.5650},
keywords = {continuous skip-gram citation},
mendeley-tags = {continuous skip-gram citation},
month = dec,
title = {{Zero-Shot Learning by Convex Combination of Semantic Embeddings}},
url = {http://arxiv.org/abs/1312.5650},
year = {2013}
}
@article{Tupes1992,
abstract = {Intercorrelations among ratings on 35 personality traits, selected as representative of the personality domain, were obtained for eight samples. These samples differed in length of acquaintanceship from 3 days to more than a year; in kind of acquaintanceship from assessment programs in a military training course to a fraternity house situation; in type of subject from airmen with only a high-school education to male and female undergraduate students to first-year graduate students; and in type of rater from very naive persons to clinical psychologists and psychiatrists with years of experience in the evaluation of personality. Centroid or multiple-group factors were extracted and rotated orthogonally to simple structure. For one study, an independent solution was obtained in which analytic rotations were accomplished on an IBM 650 computer using Kaiser's normal varimax criterion. Five fairly strong and recurrent factors emerged from each analysis, labeled as (a) Surgency, (b) Agreeableness, (c) Dependability, (d) Emotional Stability, and (e) Culture.},
author = {Tupes, E C and Christal, R E},
institution = {Armstrong Laboratory, Brooks Air Force Base, TX 78235-5601.},
journal = {Journal of personality},
number = {2},
pages = {225--251},
pmid = {1635043},
title = {{Recurrent personality factors based on trait ratings.}},
volume = {60},
year = {1992}
}
@inproceedings{Mikolov2013a,
abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and re- fine dictionaries and translation tables for any language pairs.},
address = {Mountain View, CA},
archivePrefix = {arXiv},
arxivId = {1309.4168},
author = {Mikolov, Tomas and Le, Quoc V and Sutskever, Ilya},
booktitle = {arXiv preprint arXiv:1309.4168v1},
eprint = {1309.4168},
file = {:home/eric/Papers/arXiv preprint arXiv1309.4168v1/2013 - Exploiting Similarities among Languages for Machine Translation - Mikolov, Le, Sutskever.pdf:pdf},
keywords = {continuous skip-gram citation,machine learning,machine translation,natural language processing,neural networks,semantics},
mendeley-tags = {continuous skip-gram citation,machine learning,machine translation,natural language processing,neural networks,semantics},
month = sep,
organization = {Google Inc.},
pages = {1--10},
series = {arXive},
title = {{Exploiting Similarities among Languages for Machine Translation}},
url = {http://arxiv.org/abs/1309.4168v1 http://arxiv.org/abs/1309.4168},
year = {2013}
}
@inproceedings{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18\% across thou- sands of novel labels never seen by the visual model. 1},
author = {Frome, Andrea and Corrado, Greg S and Shlens, Jonathon and Bengio, Samy and Dean, Jeffrey and Ranzato, Marc'Aurelio and Mikolov, Tomas},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/eric/Papers/Advances in Neural Information Processing Systems/2013 - Devise A deep visual-semantic embedding model - Frome et al.pdf:pdf},
keywords = {continuous skip-gram citation,image analysis,image processing},
mendeley-tags = {continuous skip-gram citation,image analysis,image processing},
pages = {1--9},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@inproceedings{Yano2009,
abstract = {In this paper we model discussions in online political blogs. To do this, we extend Latent Dirichlet Allocation (Blei et al., 2003), in various ways to capture different characteristics of the data. Our models jointly describe the generation of the primary documents (posts) as well as the authorship and, optionally, the contents of the blog community’s verbal reactions to each post (comments). We evaluate our model on a novel comment prediction task where the models are used to predict which blog users will leave comments on a given post. We also provide a qualitative discussion about what the models discover.},
address = {Boulder, Colorado},
author = {Yano, Tae and Cohen, WW and Smith, NA},
booktitle = {Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL},
file = {:home/eric/Papers/Human Language Technologies The 2009 Annual Conference of the North American Chapter of the ACL/2009 - Predicting response to political blog posts with topic models - Yano, Cohen, Smith.pdf:pdf},
number = {June},
pages = {477--485},
publisher = {Association for Computational Linguistics},
title = {{Predicting response to political blog posts with topic models}},
url = {http://dl.acm.org/citation.cfm?id=1620824},
year = {2009}
}
@inproceedings{Samsonovich2006,
abstract = {The notion of a human value system can be quantified as a cognitive map, the dimensions of which capture the semantics of concepts and the associated values. This can be done, if one knows (i) how to define the dimensions of the map, and (ii) how to allocate concepts in those dimensions. Regarding the first question, experimental studies with linguistic material using psychometrics have revealed that valence, arousal and dominance are primary dimensions characterizing human values. The same or similar dimensions are used in popular models of emotions and affects. In these studies, the choice of principal dimensions, as well as scoring concepts, was based on subjective reports or psycho-physiological measurements. Can a cognitive map of human values be constructed without testing human subjects? Here we show that the answer is positive, using generally available dictionaries of synonyms and antonyms. By applying a simple statistical-mechanic model to English and French dictionaries, we constructed multidimensional cognitive maps that capture the semantics of words. We calculated the principal dimensions of the resultant maps and found their semantics consistent across two languages as well as with previously known main cognitive dimensions. These results suggest that the linguistically derived cognitive map of the human value system is language-invariant and, being closely related to psychometrically derived maps, is likely to reflect fundamental aspects of the human mind.},
author = {Samsonovich, A V and Ascoli, G A},
booktitle = {Proc AGI Workship Advances in Artificial General Intelligence Concepts Architectures and Algorithms},
pages = {111--124},
title = {{Cognitive map dimensions of the human value system extracted from natural language}},
year = {2006}
}
@inproceedings{Shen2013,
abstract = {Email is a ubiquitous communication tool and constitutes a significant portion of social interactions. In this paper, we attempt to infer the personality of users based on the content of their emails. Such inference can enable valuable applications such as better personalization, recommendation, and targeted advertising. Considering the private and sensitive nature of email content, we propose a privacy-preserving approach for collecting email and personality data. We then frame personality prediction based on the well-known Big Five personality model and train predictors based on extracted email features. We report prediction performance of 3 generative models with different assumptions. Our results show that personality prediction is feasible, and our email feature set can predict personality with reasonable accuracies.},
address = {Heidelberg},
author = {Shen, Jianqiang and Brdiczka, Oliver and Liu, Juan},
booktitle = {User Modeling, Adaptation, and Personalization},
editor = {Carberry, Sandra and Weibelzahl, Stephan and Micarelli, Alessandro and Semeraro, Giovanni},
file = {:home/eric/Papers/User Modeling, Adaptation, and Personalization/2013 - Understanding email writers Personality prediction from email messages - Shen, Brdiczka, Liu.pdf:pdf},
keywords = {behavior analysis,email,personality,text processing},
pages = {318--330},
publisher = {Springer Verlag},
title = {{Understanding email writers: Personality prediction from email messages}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-38844-6\_29},
year = {2013}
}
@inproceedings{Mikolov2013c,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/eric/Papers/Advances in Neural Information Processing Systems/2013 - Distributed Representations of Words and Phrases and their Compositionality - Mikolov et al.pdf:pdf},
keywords = {continuous skip-gram citation,machine learning,natural language processing},
mendeley-tags = {continuous skip-gram citation,machine learning,natural language processing},
pages = {3111--3119},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality},
year = {2013}
}
@article{Samsonovich2010,
abstract = {Metric systems for semantics, or semantic cognitive maps, are allocations of words or other representations in a metric space based on their meaning. Existing methods for semantic mapping, such as Latent Semantic Analysis and Latent Dirichlet Allocation, are based on paradigms involving dissimilarity metrics. They typically do not take into account relations of antonymy and yield a large number of domain-specific semantic dimensions. Here, using a novel self-organization approach, we construct a low-dimensional, context-independent semantic map of natural language that represents simultaneously synonymy and antonymy. Emergent semantics of the map principal components are clearly identifiable: the first three correspond to the meanings of "good/bad" (valence), "calm/excited" (arousal), and "open/closed" (freedom), respectively. The semantic map is sufficiently robust to allow the automated extraction of synonyms and antonyms not originally in the dictionaries used to construct the map and to predict connotation from their coordinates. The map geometric characteristics include a limited number ( approximately 4) of statistically significant dimensions, a bimodal distribution of the first component, increasing kurtosis of subsequent (unimodal) components, and a U-shaped maximum-spread planar projection. Both the semantic content and the main geometric features of the map are consistent between dictionaries (Microsoft Word and Princeton's WordNet), among Western languages (English, French, German, and Spanish), and with previously established psychometric measures. By defining the semantics of its dimensions, the constructed map provides a foundational metric system for the quantitative analysis of word meaning. Language can be viewed as a cumulative product of human experiences. Therefore, the extracted principal semantic dimensions may be useful to characterize the general semantic dimensions of the content of mental states. This is a fundamental step toward a universal metric system for semantics of human experiences, which is necessary for developing a rigorous science of the mind.},
author = {Samsonovich, Alexei V and Ascoli, Giorgio a},
doi = {10.1371/journal.pone.0010921},
file = {:home/eric/Papers/PloS one/2010 - Principal semantic components of language and the measurement of meaning. - Samsonovich, Ascoli.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Language,Semantics,natural language processing,personality,semantics},
mendeley-tags = {natural language processing,personality,semantics},
month = jan,
number = {6},
pages = {e10921},
pmid = {20552009},
title = {{Principal semantic components of language and the measurement of meaning.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2883995\&tool=pmcentrez\&rendertype=abstract},
volume = {5},
year = {2010}
}
@article{Imperio2008,
abstract = {Lexical studies have focused on traits. In the Filipino language, we investigated whether additional dimensions can be identified when personality-relevant terms for social roles, statuses, and effects, plus physical attributes, are included. Filipino students (N = 496) rated themselves on 268 such terms, plus 253 markers of trait and evaluative dimensions. We identified 10 dimensions of social and physical attributes-Prominence, Uselessness, Attractiveness, Respectability, Uniqueness, Destructiveness, Presentableness, Strength, Dangerousness, and Charisma. Most of these dimensions did not correspond in a one-to-one manner to Filipino or alternative trait models (Big Five, HEXACO, ML7). However, considerable redundancy was observed between the social and physical attribute dimensions and trait and evaluative dimensions. Thus, social and physical attributes communicate information about personality traits, and vice-versa.},
author = {Imperio, Shellah Myra and Church, A Timothy and Katigbak, Marcia S and Reyes, Jose Alberto S},
doi = {10.1002/per.673},
file = {:home/eric/Papers/European journal of personality/2008 - Lexical Studies of Filipino Person Descriptors Adding Personality-Relevant Social and Physical Attributes. - Imperio et al.pdf:pdf},
issn = {0890-2070},
journal = {European journal of personality},
keywords = {a useful starting point,an important task in,and individual differences,and number of,for describing the nature,indigenous,lexical hypothesis,lexical studies,personality,personality structure,philippines,the natural language provides,the study of personality,ways that people differ},
mendeley-tags = {lexical hypothesis,personality},
month = jun,
number = {4},
pages = {291--321},
pmid = {19779603},
title = {{Lexical Studies of Filipino Person Descriptors: Adding Personality-Relevant Social and Physical Attributes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2749699\&tool=pmcentrez\&rendertype=abstract},
volume = {22},
year = {2008}
}
@inproceedings{Hill2001,
abstract = {Design as a social activity characterized by information exchange, compromise and negotiation frames much of our understanding of the design process. At the heart of this social activity is the development of a shared understanding of the design problem. The design stakeholders jointly form a shared understanding through a process of defining the problem, exploring the space of solutions and searching for information. A shared understanding is a critical element in successful, collaborative design. This paper describes a formal approach for identifying shared understanding in design by analyzing the documentation. By mining the documentation for signatures of a shared understanding, specifically a common frame of reference and a similar voice, a shared language and vocabulary of the design could emerge. Information management tools built around the shared understanding would be more effective in directing and alerting the design team of relevant information.},
address = {Pittsburgh, Pennsylvania},
author = {Hill, Andrew and Song, Shuang and Dong, Andy and Agogino, Alice},
booktitle = {Proceedings of the 13-th International Conference on Design Theory and Methodology},
file = {:home/eric/Papers/Proceedings of the 13-th International Conference on Design Theory and Methodology/2001 - Identifying shared understanding in design using document analysis - Hill et al.pdf:pdf},
pages = {1--7},
title = {{Identifying shared understanding in design using document analysis}},
year = {2001}
}
@article{Peabody2002,
abstract = {The psycholexical approach to personality structure in American English has led to the Big Five factors. The present study considers whether this result is similar or different in other languages. Instead of placing the usual emphasis on quantitative indices, this study examines the substantive nature of the factors. Six studies in European languages were used to develop a taxonomy of content categories. The English translations of the relevant terms were then classified under this taxonomy. The results support the generality of Big Five Factor III (Conscientiousness). Factors IV (Emotional Stability) and V (Intellect) generally did not cohere. Factors I (Extraversion) and II (Agreeableness) tended to split when this was necessary to produce 5 factors. The analysis was extended to several additional studies.},
author = {Peabody, Dean and {De Raad}, Boele},
doi = {10.1037//0022-3514.83.4.983},
file = {:home/eric/Papers/Journal of Personality and Social Psychology/2002 - The substantive nature of psycholexical personality factors A comparison across languages. - Peabody, De Raad.pdf:pdf},
issn = {0022-3514},
journal = {Journal of Personality and Social Psychology},
keywords = {lexical hypothesis,personality},
mendeley-tags = {lexical hypothesis,personality},
number = {4},
pages = {983--997},
title = {{The substantive nature of psycholexical personality factors: A comparison across languages.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.83.4.983},
volume = {83},
year = {2002}
}
@article{Cattell1947,
abstract = {In connection with a study bridging rating, questionnaire, and objective test factors, confirmation was sought with respect to the twelve personality factors previously found for young adult men. Variables were chosen to clarify and discriminate the nature of related factors. Ratings of and by 373 students were obtained, and the present study describes the separate factorization for the 1333 men among them. Factorization yielded eleven factors, of which, on "blind" rotation for simple structure, 9 or 10 proved to be identical with those of the previous study. A new factor M is described.},
author = {Cattell, RB},
file = {:home/eric/Papers/Psychometrika/1947 - Confirmation and clarification of primary personality factors - Cattell.pdf:pdf},
journal = {Psychometrika},
keywords = {personality},
mendeley-tags = {personality},
number = {3},
pages = {197--220},
title = {{Confirmation and clarification of primary personality factors}},
url = {http://link.springer.com/article/10.1007/BF02289253},
volume = {12},
year = {1947}
}
@article{Wolf,
abstract = {We extend the word2vec framework to capture meaning across languages. The input consists of a source text and a word-aligned parallel text in a second language. The joint word2vec tool then represents words in both languages within a common “semantic” vector space. The result can be used to enrich lexicons of under-resourced languages, to identify ambiguities, and to perform clustering and classification. Experiments were conducted on a parallel English-Arabic corpus, as well as on English and Hebrew Biblical texts.},
author = {Wolf, Lior and Hanani, Yair and Bar, Kfir and Dershowitz, Nachum},
file = {:home/eric/Papers/cs.tau.ac.il/2013 - Joint word2vec Networks for Bilingual Semantic Representations - Wolf et al.pdf:pdf},
journal = {cs.tau.ac.il},
keywords = {continuous skip-gram citation,machine translation},
mendeley-tags = {continuous skip-gram citation,machine translation},
title = {{Joint word2vec Networks for Bilingual Semantic Representations}},
url = {http://www.cs.tau.ac.il/~nachumd/papers/jw2v.pdf},
year = {2013}
}
