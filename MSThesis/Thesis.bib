Automatically generated by Mendeley Desktop 1.10.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Monte1995,
abstract = {A developmental account of a personality theory incorporates the personal origins of ideas, the sequence of the theorist's thinking, and the changes that such thinking undergoes as a theorist expands or modifies ideas.},
address = {Orlando, FL, US},
author = {Monte, Christopher F.},
edition = {5th},
keywords = {personality},
mendeley-tags = {personality},
pages = {893},
publisher = {Harcourt Brace College Publishers},
title = {{Beneath the mask: An introduction to theories of personality}},
year = {1995}
}
@inproceedings{Qian2004,
abstract = {Understanding the relationship among different distance measures is helpful in choosing a proper one for a particular application. In this paper, we compare two commonly used distance measures in vector models, namely, Euclidean distance (EUD) and cosine angle distance (CAD), for nearest neighbor (NN) queries in high dimensional data spaces. Using theoretical analysis and experimental results, we show that the retrieval results based on EUD are similar to those based on CAD when dimension is high. We have applied CAD for content based image retrieval (CBIR). Retrieval results show that CAD works no worse than EUD, which is a commonly used distance measure for CBIR, while providing other advantages, such as naturally normalized distance.},
address = {New York, New York, USA},
author = {Qian, Gang and Sural, Shamik and Gu, Yuelong and Pramanik, Sakti},
booktitle = {Proceedings of the 2004 ACM symposium on Applied computing - SAC '04},
doi = {10.1145/967900.968151},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian et al. - 2004 - Similarity between Euclidean and cosine angle distance for nearest neighbor queries.pdf:pdf},
isbn = {1581138121},
keywords = {content based image retrieval,cosine angle distance,distance measures,euclidean distance,inter-feature normalization,machine learning,vector model},
mendeley-tags = {distance measures,machine learning},
pages = {1232},
publisher = {ACM Press},
title = {{Similarity between Euclidean and cosine angle distance for nearest neighbor queries}},
url = {http://portal.acm.org/citation.cfm?doid=967900.968151},
year = {2004}
}
@inproceedings{Cardie2013,
abstract = {Recently, deep architectures, such as recurrent and recursive neural networks have been successfully applied to various natural language processing tasks. Inspired by bidirectional recurrent neural networks which use representations that summarize the past and future around an instance, we propose a novel architecture that aims to capture the structural information around an input, and use it to label instances. We apply our method to the task of opinion expression extraction, where we employ the binary parse tree of a sentence as the structure, and word vector representations as the initial representation of a single token. We conduct preliminary experiments to investigate its performance and compare it to the sequential approach.},
annote = {They use an embedding - but it is not mikolov. They cite him just to mention that the embeddings might have interesting semantic properties in themselves.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0493v1},
author = {Cardie, Claire and Irsoy, Ozan ˙},
booktitle = {Advances in neural information processing systems},
eprint = {arXiv:1312.0493v1},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cardie, Irsoy - 2013 - Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure.pdf:pdf},
keywords = {continuous skip-gram citation,thecat-nlp-app},
mendeley-tags = {continuous skip-gram citation,thecat-nlp-app},
pages = {1--9},
title = {{Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure}},
year = {2013}
}
@inproceedings{Shen2013,
abstract = {Email is a ubiquitous communication tool and constitutes a significant portion of social interactions. In this paper, we attempt to infer the personality of users based on the content of their emails. Such inference can enable valuable applications such as better personalization, recommendation, and targeted advertising. Considering the private and sensitive nature of email content, we propose a privacy-preserving approach for collecting email and personality data. We then frame personality prediction based on the well-known Big Five personality model and train predictors based on extracted email features. We report prediction performance of 3 generative models with different assumptions. Our results show that personality prediction is feasible, and our email feature set can predict personality with reasonable accuracies.},
address = {Heidelberg},
author = {Shen, Jianqiang and Brdiczka, Oliver and Liu, Juan},
booktitle = {User Modeling, Adaptation, and Personalization},
editor = {Carberry, Sandra and Weibelzahl, Stephan and Micarelli, Alessandro and Semeraro, Giovanni},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen, Brdiczka, Liu - 2013 - Understanding email writers Personality prediction from email messages.pdf:pdf},
keywords = {behavior analysis,email,personality,text processing},
pages = {318--330},
publisher = {Springer Verlag},
title = {{Understanding email writers: Personality prediction from email messages}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-38844-6\_29},
year = {2013}
}
@techreport{Samsonovich2012a,
abstract = {Abstract Web personalization involves automated content analysis of text, and modern technologies of semantic analysis of text rely on a number of scales. Among them is the abstractness of meaning, which is not captured by more traditional measures of sentiment, such as valence, arousal and dominance. The present work introduces a physics-inspired approach to constructing the abstractness scale based on databases of hypernym-hyponym relations, e.g., WordNet 3.0. The idea is to define an energy as a function of word coordinates that are distributed in one dimension, and then to find a global minimum of this energy function by relocating words in this dimension. The result is a one- dimensional distribution that assigns “abstractness” values to words. While positions of individual words on this scale are subject to noise, the entire distribution globally defines the universal semantic dimension associated with the notion of hypernym-hyponym relations, called here “abstractness”.},
author = {Samsonovich, AV},
booktitle = {Intelligent Techniques for Web Personalization and Recommender Systems},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samsonovich - 2012 - A Metric Scale for 'Abstractness' of the Word Meaning.pdf:pdf},
institution = {Association for the Advancement of Artificial Intelligence},
keywords = {quantification of meaning,recommender systems,semantic mapping,sentiment analysis},
pages = {48--52},
title = {{A Metric Scale for 'Abstractness' of the Word Meaning}},
url = {http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/viewFile/5340/5632},
year = {2012}
}
@misc{Norman1967,
abstract = {All terms in contemporary American English which pertain to aspects of human behavior or personal characteristics were assembled from available lexicons. These terms were categorized into 15 rubrics on the basis of judgments of their familiarity, specificity, and certain broad semantic criteria. Some 2,800 terms were identified which seemingly referred to relatively stable and specific "biophysical" traits of individuals. These words were presented to groups of university undergraduates to determine familiarity levels, specificity of connotative meaning, and a variety of psychometric operating characteristics (e.g., endorsement rates for self and for others, desirability, etc.). Results of the analysis of these data are presented and some of their potential uses for test development and personality description are suggested. Additional analyses currently in progress directed toward further refinement of the set and the development of a structured taxonomy based on these descriptors are briefly outlined.},
address = {Ann Arbor, MI},
author = {Norman, W. T.},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Norman - 1967 - 2800 Personality Trait Descriptors --Normative Operating Characteristics for a University Population.pdf:pdf},
institution = {University of Michigan Department of Psychology},
keywords = {lexical hypothesis,personality,psychology},
mendeley-tags = {lexical hypothesis,personality,psychology},
title = {{2800 Personality Trait Descriptors --Normative Operating Characteristics for a University Population}},
url = {http://files.eric.ed.gov/fulltext/ED014738.pdf},
year = {1967}
}
@article{Saucier1996,
abstract = {Studies of the natural language are a prime source of the Big-Five model, yet the factor analysis of a large, representative, and non-clustered set of English-language personality adjectives in a large sample has not yet been published. In order to test the hypothesis that fmding the Big Five dependrs on biasing the variable selection with an investigator’s preferred non-familiar terms, we present the factor analysis of 435 familiar adjectives in a combined sample (N = 899) of 507 self- and 392 peer ratings. The five-factor solution reproduced the Big Five with high clarity, demonstrating generally very high correlations with Goldberg’s adjective markers of the Big Five. The Intellect factor had a more moderate correlation, due to its de-emphasis of the creativity components of Factor V, a phenomenon that may occur commonly with the lexical Intellect factor.},
author = {Saucier, Gerard and Goldberg, Lewis R.},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saucler, Goldberg - 1996 - Evidence for the Big Five in analyses of familiar English personality adjectives.pdf:pdf},
journal = {European Journal of Personality},
keywords = {personality},
mendeley-tags = {personality},
number = {1},
pages = {61--77},
title = {{Evidence for the Big Five in analyses of familiar English personality adjectives}},
volume = {10},
year = {1996}
}
@phdthesis{Mikolov2007,
author = {Mikolov, Tomas},
keywords = {machine learning,natural language processing,neural networks},
mendeley-tags = {machine learning,natural language processing,neural networks},
school = {Brno Uni- versity of Technology},
title = {{Language Modeling for Speech Recognition in Czech}},
type = {Masters thesis},
year = {2007}
}
@article{Norouzi2013,
abstract = {Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional $\backslash$nway\{\} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing $\backslash$nway\{\} image classifier and a semantic word embedding model, which contains the \$\backslash n\$ class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.},
archivePrefix = {arXiv},
arxivId = {1312.5650},
author = {Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S. and Dean, Jeffrey},
eprint = {1312.5650},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Norouzi et al. - 2013 - Zero-Shot Learning by Convex Combination of Semantic Embeddings.pdf:pdf},
journal = {arXiv:1312.5650},
keywords = {computer-vision,continuous skip-gram citation,image analysis,thecat-non-nlp-app},
mendeley-tags = {computer-vision,continuous skip-gram citation,image analysis,thecat-non-nlp-app},
month = dec,
title = {{Zero-Shot Learning by Convex Combination of Semantic Embeddings}},
url = {http://arxiv.org/abs/1312.5650},
year = {2013}
}
@article{Samsonovich2010,
abstract = {Metric systems for semantics, or semantic cognitive maps, are allocations of words or other representations in a metric space based on their meaning. Existing methods for semantic mapping, such as Latent Semantic Analysis and Latent Dirichlet Allocation, are based on paradigms involving dissimilarity metrics. They typically do not take into account relations of antonymy and yield a large number of domain-specific semantic dimensions. Here, using a novel self-organization approach, we construct a low-dimensional, context-independent semantic map of natural language that represents simultaneously synonymy and antonymy. Emergent semantics of the map principal components are clearly identifiable: the first three correspond to the meanings of "good/bad" (valence), "calm/excited" (arousal), and "open/closed" (freedom), respectively. The semantic map is sufficiently robust to allow the automated extraction of synonyms and antonyms not originally in the dictionaries used to construct the map and to predict connotation from their coordinates. The map geometric characteristics include a limited number ( approximately 4) of statistically significant dimensions, a bimodal distribution of the first component, increasing kurtosis of subsequent (unimodal) components, and a U-shaped maximum-spread planar projection. Both the semantic content and the main geometric features of the map are consistent between dictionaries (Microsoft Word and Princeton's WordNet), among Western languages (English, French, German, and Spanish), and with previously established psychometric measures. By defining the semantics of its dimensions, the constructed map provides a foundational metric system for the quantitative analysis of word meaning. Language can be viewed as a cumulative product of human experiences. Therefore, the extracted principal semantic dimensions may be useful to characterize the general semantic dimensions of the content of mental states. This is a fundamental step toward a universal metric system for semantics of human experiences, which is necessary for developing a rigorous science of the mind.},
author = {Samsonovich, Alexei V and Ascoli, Giorgio A.},
doi = {10.1371/journal.pone.0010921},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samsonovich, Ascoli - 2010 - Principal semantic components of language and the measurement of meaning.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Language,Semantics,natural language processing,personality,semantics},
mendeley-tags = {natural language processing,personality,semantics},
month = jan,
number = {6},
pages = {e10921},
pmid = {20552009},
title = {{Principal semantic components of language and the measurement of meaning.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2883995\&tool=pmcentrez\&rendertype=abstract},
volume = {5},
year = {2010}
}
@inproceedings{Bordes2013,
abstract = {We consider the problem of embedding entities and relations of knowledge bases in low-dimensional vector spaces. Unlike most existing approaches, which are primarily efficient for modeling equivalence relations, our approach is designed to explicitly model irreflexive relations, such as hierarchies, by interpreting them as translations operating on the low-dimensional embeddings of the entities. Preliminary experiments show that, despite its simplicity and a smaller number of parameters than previous approaches, our approach achieves state-of-the-art performance according to standard evaluation protocols on data from WordNet and Freebase.},
archivePrefix = {arXiv},
arxivId = {1304.7158},
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
booktitle = {arXiv:1304.7158},
eprint = {1304.7158},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes et al. - 2013 - Irreflexive and Hierarchical Relations as Translations.pdf:pdf},
keywords = {continuous skip-gram citation,natural language processing,thecat-alg-improvement},
mendeley-tags = {continuous skip-gram citation,natural language processing,thecat-alg-improvement},
month = apr,
title = {{Irreflexive and Hierarchical Relations as Translations}},
url = {http://arxiv.org/abs/1304.7158},
year = {2013}
}
@inproceedings{Schwartz2013a,
abstract = {Language in social media reveals a lot about people’s personality and mood as they discuss the activities and relationships that constitute their everyday lives. Although social media are widely studied, researchers in computational linguistics have mostly focused on prediction tasks such as sentiment analysis and authorship attribution. In this paper, we showhowsocial media can also be used to gain psychological insights. We demonstrate an exploration of language use as a function of age, gender, and personality from a dataset of Facebook posts from 75,000 people who have also taken personality tests, and we suggest how more sophisticated tools could be brought to bear on such data.},
author = {Schwartz, H Andrew and Eichstaedt, Johannes C and Dziurzynski, Lukasz and Kern, Margaret L and Seligman, Martin E. P. and Ungar, Lyle H. and Blanco, Eduardo and Kosinski, Michal and Stillwell, David},
booktitle = {2013 AAAI Spring Symposium},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwartz et al. - 2013 - Toward Personality Insights from Language Exploration in Social Media.pdf:pdf},
keywords = {facebook,personality,social media},
mendeley-tags = {facebook,personality,social media},
pages = {72--79},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{Toward Personality Insights from Language Exploration in Social Media}},
url = {http://www.aaai.org/ocs/index.php/SSS/SSS13/paper/download/5764/5915},
year = {2013}
}
@misc{PrincetonUniversity2010,
author = {University, Princeton},
publisher = {Princeton University},
title = {{About WordNet}},
url = {http://wordnet.princeton.edu},
year = {2010}
}
@article{Cattell1947,
abstract = {In connection with a study bridging rating, questionnaire, and objective test factors, confirmation was sought with respect to the twelve personality factors previously found for young adult men. Variables were chosen to clarify and discriminate the nature of related factors. Ratings of and by 373 students were obtained, and the present study describes the separate factorization for the 1333 men among them. Factorization yielded eleven factors, of which, on "blind" rotation for simple structure, 9 or 10 proved to be identical with those of the previous study. A new factor M is described.},
author = {Cattell, Raymond B},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cattell - 1947 - Confirmation and clarification of primary personality factors.pdf:pdf},
journal = {Psychometrika},
keywords = {personality},
mendeley-tags = {personality},
number = {3},
pages = {197--220},
title = {{Confirmation and clarification of primary personality factors}},
url = {http://link.springer.com/article/10.1007/BF02289253},
volume = {12},
year = {1947}
}
@article{Wolf,
abstract = {We extend the word2vec framework to capture meaning across languages. The input consists of a source text and a word-aligned parallel text in a second language. The joint word2vec tool then represents words in both languages within a common “semantic” vector space. The result can be used to enrich lexicons of under-resourced languages, to identify ambiguities, and to perform clustering and classification. Experiments were conducted on a parallel English-Arabic corpus, as well as on English and Hebrew Biblical texts.},
annote = {lexicographers; others are substantive and are unlikely to share the same words in multiple languages. Accordingly, it has been argued (e.g., [6,9,8]) that sense distinctions can be derived from co-occurrence statistics across languages. To quote [13]: “Homograph distinctions do not require a lexicographer to locate them, since they are basically those that can be found easily in parallel texts in different languages”.

        
First does monolingual training. Then considers neighborhood of word in language A to be those words in the neighborhood of its aligned pair in language B and vice-versa. Uses the CBOW variant.},
author = {Wolf, Lior and Hanani, Yair and Bar, Kfir and Dershowitz, Nachum},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolf et al. - 2013 - Joint word2vec Networks for Bilingual Semantic Representations.pdf:pdf},
journal = {cs.tau.ac.il},
keywords = {continuous skip-gram citation,machine translation,thecat-alg-improvement},
mendeley-tags = {continuous skip-gram citation,machine translation,thecat-alg-improvement},
title = {{Joint word2vec Networks for Bilingual Semantic Representations}},
url = {http://www.cs.tau.ac.il/~nachumd/papers/jw2v.pdf},
year = {2013}
}
@inproceedings{Faruqui2014,
abstract = {The distributional hypothesis of Harris (1954), according to which the meaning of words is evidenced by the contexts they occur in, has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora. This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually. We evaluate the resultingword representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques},
author = {Faruqui, Manaal and Dyer, Chris},
booktitle = {Proc. of EACL},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Faruqui, Dyer - 2014 - Improving Vector Space Word Representations Using Multilingual Correlation.pdf:pdf},
keywords = {continuous skip-gram citation,machine translation,thecat-alg-improvement},
mendeley-tags = {continuous skip-gram citation,machine translation,thecat-alg-improvement},
title = {{Improving Vector Space Word Representations Using Multilingual Correlation}},
url = {http://www.cs.cmu.edu/~mfaruqui/papers/eacl14-vectors.pdf},
year = {2014}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality:a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such largemodels (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Jauvin, Christian},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
journal = {Journal ofMachine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@inproceedings{Fernandes2011,
abstract = {Nowadays the size of collections of information achieved considerable sizes, making the finding and exploration of a particular subject hard to achieve. One way to solve this problem is through text classification, where a theme or category is assigned to a text based on the analysis of its content. However, existing approaches to text classification require some effort and a high level of knowledge on this subject by the users, making them inaccessible to the common user. Another problem of current approaches is that they are optimized for a specific problem and can not easily be adapted to another context. In particular, unsupervised methods based on the LSA algorithm require users to define the dimension to use in the algorithm. In this paper we describe an approach to make the use of text classification more accessible to common users, by providing a formula to estimate the dimension of the LSA based on the number of texts used during the boot-strapping process. Experimental results show that our formula for estimation of the LSA dimension allows us to create unsupervised solutions able to achieve results similar to supervised approaches.},
address = {Paris, France},
author = {Fernandes, Jorge and Art\'{\i}fice, A and Fonseca, MJ},
booktitle = {International Conference on Knowledge Discovery and Information Retrieval},
file = {:home/eric/Papers/NaturalLanguageProcessing/2011-Automatic\_estimation\_of\_the\_LSA\_Dimension-Fernandes\_J+Artifice\_A+Fonseca\_M.pdf:pdf},
keywords = {Bootstrapping,LSA,LSA Dimension,Unsupervised Text Classification,dimensionality reduction,information retrieval,lsa,nlp},
mendeley-tags = {dimensionality reduction,information retrieval,lsa,nlp},
publisher = {Springer},
title = {{Automatic Estimation of the LSA Dimension.}},
url = {http://www.di.fc.ul.pt/~mjf/publications/2014-2010/pdf/kdir11.pdf},
year = {2011}
}
@inproceedings{Hofmann1999,
abstract = {Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
author = {Hofmann, Thomas},
booktitle = {UAI'99 Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hofmann - 1999 - Probabilistic latent semantic analysis.pdf:pdf},
isbn = {1-55860-614-9},
keywords = {artificial intelligence,computing methodologies,knowledge representation and reasoning,mathematics in computing,models of computation,natural language processing,plsa,probabilistic computation,probabilistic reasoning,probability and statistics,theory of computation,vagueness and fuzzy logic},
mendeley-tags = {natural language processing,plsa},
month = jul,
pages = {289--296},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Probabilistic latent semantic analysis}},
url = {http://dl.acm.org/citation.cfm?id=2073796.2073829},
year = {1999}
}
@incollection{Deary2009,
address = {Cambridge, UK},
author = {Deary, Ian J.},
booktitle = {The Cambridge Handbook of Personality Psychology},
chapter = {6},
edition = {1},
editor = {Corr, Philip J. and Matthews, Gerald},
isbn = {978-0-521-86218-9},
keywords = {lexical hypothesis,personality},
mendeley-tags = {lexical hypothesis,personality},
pages = {89--109},
title = {{The trait approach to personality}},
year = {2009}
}
@article{Goldberg1992,
abstract = {To satisfy the need in personality research for factorially univocal measures of each of the 5 domains that subsume most English-language terms for personality-traits, new sets of Big-Five factor markers were investigated. In studies of adjective-anchored bipolar rating scales, a transparent format was found to produce factor markers that were more univocal than the same scales administered in the traditional format. Nonetheless, even the transparent bipolar scales proved less robust as factor markers than did parallel sets of adjectives administered in unipolar format. A set of 100 unipolar terms proved to be highly robust across quite diverse samples of self and peer descriptions. These new markers were compared with previously developed ones based on far larger sets of trait adjectives, as well as with the scales from the NEO and Hogan personality inventories.},
author = {Goldberg, Lewis R.},
doi = {10.1037//1040-3590.4.1.26},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldberg - 1992 - The development of markers for the Big-Five factor structure.pdf:pdf},
issn = {1040-3590},
journal = {Psychological Assessment},
keywords = {personality},
mendeley-tags = {personality},
number = {1},
pages = {26--42},
title = {{The development of markers for the Big-Five factor structure.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1040-3590.4.1.26},
volume = {4},
year = {1992}
}
@article{Arvidsson2011,
abstract = {We propose a theory-neutral, computational and data-driven method for assessing changes in semantic content of object representations following long-term psychodynamic psychotherapy. Young adults in psychotherapy are compared with an age-matched, non-clinical sample at three time points. Verbatim transcripts of descriptions of the self and parents were quantified in a semantic space constructed by Latent Semantic Analysis. In the psychotherapy group, all representations changed from baseline to follow-up, whereas no comparable changes could be observed in the comparison group. The semantic space method supports the hypothesis that long-term psychodynamic psychotherapy contributes to sustained change of affective-cognitive schemas of self and others.},
annote = {I couldn't get read the article, but it has to do with personality (or at least psychology) and LSA},
author = {Arvidsson, David and Sikstr\"{o}m, Sverker and Werbart, Andrzej},
doi = {10.1080/10503307.2011.577824},
issn = {1468-4381},
journal = {Psychotherapy research : journal of the Society for Psychotherapy Research},
keywords = {Adolescent,Adult,Case-Control Studies,Female,Humans,Interview, Psychological,Male,Psychotherapy,Psychotherapy, Group,Self Concept,Semantics,Socioeconomic Factors,Time Factors,Treatment Outcome,Young Adult},
month = jul,
number = {4},
pages = {430--46},
pmid = {21623547},
publisher = {Routledge},
title = {{Changes in self and object representations following psychotherapy measured by a theory-free, computational, semantic space method.}},
url = {http://dx.doi.org/10.1080/10503307.2011.577824},
volume = {21},
year = {2011}
}
@article{Perozzi2014,
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide \$F\_1\$ scores up to 10\% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60\% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
archivePrefix = {arXiv},
arxivId = {1403.6652},
author = {Perozzi, Bryan and Al-Rfou', Rami and Skiena, Steven},
eprint = {1403.6652},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perozzi, Al-Rfou', Skiena - 2014 - DeepWalk Online Learning of Social Representations.pdf:pdf},
keywords = {continuous skip-gram citation,social network,thecat-non-nlp-app},
mendeley-tags = {continuous skip-gram citation,social network,thecat-non-nlp-app},
month = mar,
pages = {10},
title = {{DeepWalk: Online Learning of Social Representations}},
url = {http://arxiv.org/abs/1403.6652},
year = {2014}
}
@inproceedings{Mikolov2013b,
abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {arXiv preprint arXiv:1301.3781},
eprint = {1301.3781},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space(3).pdf:pdf},
month = jan,
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781v3},
year = {2013}
}
@article{Imperio2008,
abstract = {Lexical studies have focused on traits. In the Filipino language, we investigated whether additional dimensions can be identified when personality-relevant terms for social roles, statuses, and effects, plus physical attributes, are included. Filipino students (N = 496) rated themselves on 268 such terms, plus 253 markers of trait and evaluative dimensions. We identified 10 dimensions of social and physical attributes-Prominence, Uselessness, Attractiveness, Respectability, Uniqueness, Destructiveness, Presentableness, Strength, Dangerousness, and Charisma. Most of these dimensions did not correspond in a one-to-one manner to Filipino or alternative trait models (Big Five, HEXACO, ML7). However, considerable redundancy was observed between the social and physical attribute dimensions and trait and evaluative dimensions. Thus, social and physical attributes communicate information about personality traits, and vice-versa.},
author = {Imperio, Shellah Myra and Church, A Timothy and Katigbak, Marcia S and Reyes, Jose Alberto S},
doi = {10.1002/per.673},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Imperio et al. - 2008 - Lexical Studies of Filipino Person Descriptors Adding Personality-Relevant Social and Physical Attributes.pdf:pdf},
issn = {0890-2070},
journal = {European journal of personality},
keywords = {a useful starting point,an important task in,and individual differences,and number of,for describing the nature,indigenous,lexical hypothesis,lexical studies,personality,personality structure,philippines,the natural language provides,the study of personality,ways that people differ},
mendeley-tags = {lexical hypothesis,personality},
month = jun,
number = {4},
pages = {291--321},
pmid = {19779603},
title = {{Lexical Studies of Filipino Person Descriptors: Adding Personality-Relevant Social and Physical Attributes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2749699\&tool=pmcentrez\&rendertype=abstract},
volume = {22},
year = {2008}
}
@inproceedings{Pennacchiotti2011,
abstract = {This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user’s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.},
author = {Pennacchiotti, Marco and Popescu, Ana-Maria},
booktitle = {ICWSM},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennacchiotti, Popescu - 2011 - A Machine Learning Approach to Twitter User Classification.pdf:pdf},
keywords = {machine learning,natural language processing},
mendeley-tags = {machine learning,natural language processing},
pages = {281--288},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{A Machine Learning Approach to Twitter User Classification.}},
url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2886/3262},
year = {2011}
}
@article{Goldberg1990,
abstract = {In the 45 years since Cattell used English trait terms to begin the formulation of his "description of personality," a number of investigators have proposed an alternative structure based on 5 orthogonal factors. The generality of this 5-factor model is here demonstrated across unusually comprehensive sets of trait terms. In the first of 3 studies, 1,431 trait adjectives grouped into 75 clusters were analyzed; virtually identical structures emerged in 10 replications, each based on a different factor-analytic procedure. A 2nd study of 479 common terms grouped into 133 synonym clusters revealed the same structure in 2 samples of self-ratings and in 2 samples of peer ratings. None of the factors beyond the 5th generalized across the samples. In the 3rd study, analyses of 100 clusters derived from 339 trait terms suggest their potential utility as Big-Five markers in future studies.},
author = {Goldberg, Lewis R and John, Oliver P and Kaiser, Henry and Lanning, Kevin and Peabody, Dean},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldberg et al. - 1990 - An Alternative Description of Personality The Big-Five Factor Structure.pdf:pdf},
journal = {Journal of Personality and Social Psychology},
keywords = {personality},
mendeley-tags = {personality},
number = {6},
pages = {1216--1229},
title = {{An Alternative " Description of Personality ": The Big-Five Factor Structure}},
volume = {59},
year = {1990}
}
@article{Allport1936,
author = {Allport, Gordon W. and Odbert, Henry S.},
doi = {10.1037/h0093360},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Allport, Odbert - 1936 - Trait-names A psycho-lexical study.pdf:pdf},
issn = {0096-9753},
journal = {Psychological Monographs},
keywords = {lexical hypothesis,personality,psychology},
mendeley-tags = {lexical hypothesis,personality,psychology},
number = {1},
pages = {i--171},
title = {{Trait-names: A psycho-lexical study.}},
url = {http://psycnet.apa.org/journals/mon/47/1/i/ http://doi.apa.org/getdoi.cfm?doi=10.1037/h0093360},
volume = {47},
year = {1936}
}
@inproceedings{Dumais1988,
abstract = {This paper describes a new approach for dealing with the vocabulary problem in human-computer interaction. Most approaches to retrieving textual materials depend on a lexical match between words in users' requests and those in or assigned to database objects. Because of the tremendous diversity in the words people use to describe the same object, lexical matching methods are necessarily incomplete and imprecise [5]. The latent semantic indexing approach tries to overcome these problems by automatically organizing text objects into a semantic structure more appropriate for matching user requests. This is done by taking advantage of implicit higher-order structure in the association of terms with text objects. The particular technique used is singular-value decomposition, in which a large term by text-object matrix is decomposed into a set of about 50 to 150 orthogonal factors from which the original matrix can be approximated by linear combination. Terms and objects are represented by 50 to 150 dimensional vectors and matched against user queries in this “semantic” space. Initial tests find this completely automatic method widely applicable and a promising way to improve users' access to many kinds of textual materials, or to objects and services for which textual descriptions are available.},
address = {New York, New York, USA},
author = {Dumais, S. T. and Furnas, G. W. and Landauer, T. K. and Deerwester, S. and Harshman, R.},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '88},
doi = {10.1145/57167.57214},
editor = {O'Hare, J J},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dumais et al. - 1988 - Using latent semantic analysis to improve access to textual information.pdf:pdf},
isbn = {0201142376},
keywords = {applied computing,artificial intelligence,arts and humanities,computing methodologies,document management and text processing,document preparation,document representation,document scripting languages,human computer interaction,human-centered computing,information retrieval,information systems,language translation,natural language processing,retrieval models and ranking,search engine architectures and scalability,search engine indexing},
pages = {281--285},
publisher = {ACM Press},
title = {{Using latent semantic analysis to improve access to textual information}},
url = {http://dl.acm.org/citation.cfm?id=57214 http://portal.acm.org/citation.cfm?doid=57167.57214},
year = {1988}
}
@book{Bishop2006a,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
keywords = {machine learning,probability,statistics},
mendeley-tags = {machine learning,probability,statistics},
pages = {738},
pmid = {8943268},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@inproceedings{Hill2001,
abstract = {Design as a social activity characterized by information exchange, compromise and negotiation frames much of our understanding of the design process. At the heart of this social activity is the development of a shared understanding of the design problem. The design stakeholders jointly form a shared understanding through a process of defining the problem, exploring the space of solutions and searching for information. A shared understanding is a critical element in successful, collaborative design. This paper describes a formal approach for identifying shared understanding in design by analyzing the documentation. By mining the documentation for signatures of a shared understanding, specifically a common frame of reference and a similar voice, a shared language and vocabulary of the design could emerge. Information management tools built around the shared understanding would be more effective in directing and alerting the design team of relevant information.},
address = {Pittsburgh, Pennsylvania},
author = {Hill, Andrew and Song, Shuang and Dong, Andy and Agogino, Alice},
booktitle = {Proceedings of the 13-th International Conference on Design Theory and Methodology},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hill et al. - 2001 - Identifying shared understanding in design using document analysis.pdf:pdf},
pages = {1--7},
title = {{Identifying shared understanding in design using document analysis}},
year = {2001}
}
@article{Saucier2005,
abstract = {Personality descriptors--3,302 adjectives--were extracted from a dictionary of the modern Greek language. Those terms with the highest frequency were administered to large samples in Greece to test the universality of the Big-Five dimensions of personality in comparison to alternative models. One- and 2-factor structures were the most stable across variable selections and subsamples and replicated such structures found in previous studies. Among models with more moderate levels of replication, recently proposed 6- and 7-lexical-factor models were approximately as well replicated as the Big Five. An emic 6-factor structure showed relative stability; these factors were labeled Negative-Valence/Honesty, Agreeableness/Positive Affect, Prowess/Heroism, Introversion/Melancholia, Even Temper, and Conscientiousness.},
author = {Saucier, Gerard and Georgiades, Stelios and Tsaousis, Ioannis and Goldberg, Lewis R.},
doi = {10.1037/0022-3514.88.5.856},
issn = {0022-3514},
journal = {Journal of personality and social psychology},
keywords = {Culture,Factor Analysis,Greece,Humans,Language,Personality,Statistical,Vocabulary,personality},
mendeley-tags = {personality},
month = may,
number = {5},
pages = {856--75},
pmid = {15898880},
title = {{The factor structure of Greek personality adjectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15898880},
volume = {88},
year = {2005}
}
@inproceedings{Nowson2005,
abstract = {Blogs are personal online diaries, and a relatively recent form of computer-mediated communication. What kind of writing do they contain? This paper adopts ameasure of linguistic contextuality/formality, due to Heylighen and Dewaele, and applies it to a corpus of weblogs. It first compares the corpus with sub-corpora from the British National Corpus, and weblogs are shown to be more formal than e-mail, but less formal than biographies. Then, the paper explores the impact of individual differences between writers on their texts’ contextuality/formality. It appears that Extraversion and Neuroticism are less influential than previously supposed, and it is argued that gender and Agreeableness account for more of the variability in the extent to which weblog writers take their readers’ contexts into account.},
address = {Stresa, Italy},
author = {Nowson, Scott and Oberlander, Jon and Gill, Alastair J},
booktitle = {Proceedings of the 27th \ldots},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nowson, Oberlander, Gill - 2005 - Weblogs, genres and individual differences.pdf:pdf},
pages = {1666--1671},
title = {{Weblogs, genres and individual differences}},
url = {http://csjarchive.cogsci.rpi.edu/Proceedings/2005/docs/p1666.pdf},
year = {2005}
}
@inproceedings{Mikolov2013a,
abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
address = {Mountain View, CA},
archivePrefix = {arXiv},
arxivId = {1309.4168},
author = {Mikolov, Tomas and Le, Quoc V and Sutskever, Ilya},
booktitle = {arXiv preprint arXiv:1309.4168v1},
eprint = {1309.4168},
file = {:home/eric/Papers/NaturalLanguageProcessing/2013-Exploiting\_similarities\_among\_languages\_for\_machine\_translation-Mikolov\_T+Le\_Q+Sutskever\_I.pdf:pdf},
keywords = {continuous skip-gram citation,machine learning,machine translation,natural language processing,neural networks,semantics,thecat-nlp-app},
mendeley-tags = {continuous skip-gram citation,machine learning,machine translation,natural language processing,neural networks,semantics,thecat-nlp-app},
month = sep,
organization = {Google Inc.},
pages = {1--10},
series = {arXive},
title = {{Exploiting Similarities among Languages for Machine Translation}},
url = {http://arxiv.org/abs/1309.4168v1 http://arxiv.org/abs/1309.4168},
year = {2013}
}
@inproceedings{Mikolov2013,
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40\% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.},
address = {Atlanta, GA},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
booktitle = {Proceedings of NAACL-HLT},
file = {:home/eric/Papers/NaturalLanguageProcessing/2013-Linguistic\_regularities\_in\_continuous\_space\_word\_representations-Mikolov\_T+Yih\_W+Zweig\_G.pdf:pdf;:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Yih, Zweig - 2013 - Linguistic regularities in continuous space word representations.pdf:pdf},
keywords = {natural language processing,neural networks,semantics},
mendeley-tags = {natural language processing,neural networks,semantics},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://acl.eldoc.ub.rug.nl/mirror/N/N13/N13-1090.pdf},
year = {2013}
}
@book{Cattell1946,
address = {New York, New York, USA},
author = {Cattell, Raymond B},
publisher = {Harcourt, Brace \& World},
title = {{The description and measurement of personality}},
year = {1946}
}
@article{Erk2012,
abstract = {Distributional models represent a word through the contexts in which it has been observed. They can be used to predict similarity in meaning, based on the distributional hypothesis, which states that two words that occur in similar contexts tend to have similar meanings. Distributional approaches are often implemented in vector space models. They represent a word as a point in high-dimensional space, where each dimension stands for a context item, and a word's coordinates represent its context counts. Occurrence in similar contexts then means proximity in space. In this survey we look at the use of vector space models to describe the meaning of words and phrases: the phenomena that vector space models address, and the techniques that they use to do so. Many word meaning phenomena can be described in terms of semantic similarity: synonymy, priming, categorization, and the typicality of a predicate's arguments. But vector space models can do more than just predict semantic similarity. They are a very flexible tool, because they can make use of all of linear algebra, with all its data structures and operations. The dimensions of a vector space can stand for many things: context words, or non-linguistic context like images, or properties of a concept. And vector space models can use matrices or higher-order arrays instead of vectors for representing more complex relationships. Polysemy is a tough problem for distributional approaches, as a representation that is learned from all of a word's contexts will conflate the different senses of the word. It can be addressed, using either clustering or vector combination techniques. Finally, we look at vector space models for phrases, which are usually constructed by combining word vectors. Vector space models for phrases can predict phrase similarity, and some argue that they can form the basis for a general-purpose representation framework for natural language semantics.},
author = {Erk, Katrin},
doi = {10.1002/lnco.362},
file = {:home/eric/Papers/NaturalLanguageProcessing/2012-Vector\_space\_models\_of\_word\_meaning\_and\_phrase\_meaning\_a\_survey-Erk\_K.pdf:pdf},
issn = {1749818X},
journal = {Language and Linguistics Compass},
keywords = {lexical semantics,natural language processing,semantics},
mendeley-tags = {lexical semantics,natural language processing,semantics},
month = oct,
number = {10},
pages = {635--653},
title = {{Vector Space Models of Word Meaning and Phrase Meaning: A Survey}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/lnco.362/full http://doi.wiley.com/10.1002/lnco.362},
volume = {6},
year = {2012}
}
@article{Tupes1992,
abstract = {Intercorrelations among ratings on 35 personality traits, selected as representative of the personality domain, were obtained for eight samples. These samples differed in length of acquaintanceship from 3 days to more than a year; in kind of acquaintanceship from assessment programs in a military training course to a fraternity house situation; in type of subject from airmen with only a high-school education to male and female undergraduate students to first-year graduate students; and in type of rater from very naive persons to clinical psychologists and psychiatrists with years of experience in the evaluation of personality. Centroid or multiple-group factors were extracted and rotated orthogonally to simple structure. For one study, an independent solution was obtained in which analytic rotations were accomplished on an IBM 650 computer using Kaiser's normal varimax criterion. Five fairly strong and recurrent factors emerged from each analysis, labeled as (a) Surgency, (b) Agreeableness, (c) Dependability, (d) Emotional Stability, and (e) Culture.},
author = {Tupes, E C and Christal, R E},
institution = {Armstrong Laboratory, Brooks Air Force Base, TX 78235-5601.},
journal = {Journal of personality},
number = {2},
pages = {225--251},
pmid = {1635043},
title = {{Recurrent personality factors based on trait ratings.}},
volume = {60},
year = {1992}
}
@inproceedings{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18\% across thou- sands of novel labels never seen by the visual model. 1},
author = {Frome, Andrea and Corrado, Greg S and Shlens, Jonathon and Bengio, Samy and Dean, Jeffrey and Ranzato, Marc'Aurelio and Mikolov, Tomas},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frome et al. - 2013 - Devise A deep visual-semantic embedding model.pdf:pdf},
keywords = {computer-vision,continuous skip-gram citation,image analysis,image processing,thecat-non-nlp-app},
mendeley-tags = {computer-vision,continuous skip-gram citation,image analysis,image processing,thecat-non-nlp-app},
pages = {1--9},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@misc{Osendorfer2013,
abstract = {Recent research from Microsoft shows that “King - Man + Woman” almost equals “Queen” [1]. An open source project from Google that comes to the same conclusion offers an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. We are looking into expanding this work to non-verbal domains, in particular to music. While Google trains their models on data sets of billions of words, we will train our models on data sets of up to hundreds of millions of notes. Apart from providing a sound and scalable way of computing musical similarity and introducing the notions of analogy and metaphor to music - a significant contribution to musicology in its own right - this highly publishable research will have many synthetic/creative applications. For example, using the intermediate vector representation for translating language to music and back, which has never been done before.},
author = {Osendorfer, Christian and Viro, Vladimir},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Osendorfer, Viro - 2013 - Scalable Vector Arithmetics for Music using RNNs.pdf:pdf},
keywords = {continuous skip-gram citation,information retrieval,music,thecat-non-nlp-app},
mendeley-tags = {continuous skip-gram citation,information retrieval,music,thecat-non-nlp-app},
pages = {1},
title = {{Scalable Vector Arithmetics for Music using RNNs}},
year = {2013}
}
@techreport{Iofciu2006,
author = {Iofciu, T. and Zhou, X. and Giesbers, B. and Rusman, E. and van Bruggen, J. and Ceri, S.},
institution = {Open University of the Netherlands (OUNL); Cooper Consortium},
title = {{State of the Art Report in Knowledge Sharing, Recommendation and Latent Semantic Analysis. Cooper Consortium Deliverable 3.1}},
year = {2006}
}
@book{Leary1957,
abstract = {The book is concerned with interpersonal behavior as observed in the psychotherapeutic setting. The approach might be called dynamic behaviorism, which has two attributes: the impact one person has in interaction with others and the interaction of psychological pressures among different levels of personality. The book is subdivided as follows: Part I. Basic assumptions about personality theory. Part II. Interpersonal dimension of personality—variables, levels, diagnostic categories. Part III. Variability dimension of personality theory and variables. Part IV. Interpersonal diagnosis of personality. Part V. Applications of the interpersonal system. An appendix contains appropriate tables and illustrative materials.},
address = {Oxford, England},
author = {Leary, Timothy},
keywords = {personality},
mendeley-tags = {personality},
pages = {518},
publisher = {Ronald Press},
title = {{Interpersonal diagnosis of personality; a functional theory and methodology for personality evaluation.}},
year = {1957}
}
@inproceedings{Mikolov2013c,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
keywords = {continuous skip-gram citation,machine learning,natural language processing,thecat-alg-improvement},
mendeley-tags = {continuous skip-gram citation,machine learning,natural language processing,thecat-alg-improvement},
pages = {3111--3119},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality},
year = {2013}
}
@incollection{DeRaad2009,
address = {New York, New York, USA},
author = {{De Raad}, Boele},
booktitle = {The Cambridge Handbook of Personality Psychology},
chapter = {8},
edition = {1st},
editor = {Corr, Philip J. and Matthews, Gerald},
isbn = {978-0-521-86218-9},
keywords = {personality},
mendeley-tags = {personality},
pages = {127--147},
publisher = {Cambridge University Press},
title = {{Structural Models of Personality}},
year = {2009}
}
@article{Peabody2002,
abstract = {The psycholexical approach to personality structure in American English has led to the Big Five factors. The present study considers whether this result is similar or different in other languages. Instead of placing the usual emphasis on quantitative indices, this study examines the substantive nature of the factors. Six studies in European languages were used to develop a taxonomy of content categories. The English translations of the relevant terms were then classified under this taxonomy. The results support the generality of Big Five Factor III (Conscientiousness). Factors IV (Emotional Stability) and V (Intellect) generally did not cohere. Factors I (Extraversion) and II (Agreeableness) tended to split when this was necessary to produce 5 factors. The analysis was extended to several additional studies.},
author = {Peabody, Dean and {De Raad}, Boele},
doi = {10.1037//0022-3514.83.4.983},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peabody, De Raad - 2002 - The substantive nature of psycholexical personality factors A comparison across languages.pdf:pdf},
issn = {0022-3514},
journal = {Journal of Personality and Social Psychology},
keywords = {lexical hypothesis,personality},
mendeley-tags = {lexical hypothesis,personality},
number = {4},
pages = {983--997},
title = {{The substantive nature of psycholexical personality factors: A comparison across languages.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.83.4.983},
volume = {83},
year = {2002}
}
@article{Hiemstra2000,
abstract = {This paper presents a new probabilistic model of information retrieval. The most important modeling assumption made is that documents and queries are defined by an ordered sequence of single terms. This assumption is not made in well-known existing models of information retrieval, but is essential in the field of statistical natural language processing. Advances already made in statistical natural language processing will be used in this paper to formulate a probabilistic justification for using tf×idf term weighting. The paper shows that the new probabilistic interpretation of tf×idf term weighting might lead to better understanding of statistical ranking mechanisms, for example by explaining how they relate to coordination level ranking. A pilot experiment on the TREC collection shows that the linguistically motivated weighting algorithm outperforms the popular BM25 weighting algorithm.},
author = {Hiemstra, Djoerd},
doi = {10.1007/s007999900025},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hiemstra - 2000 - A probabilistic justification for using tf x idf term weighting in information retrieval.pdf:pdf},
issn = {1432-5012},
journal = {International Journal on Digital Libraries},
keywords = {Information retrieval theory,Statistical information retrieval,Statistical natural language processing},
month = aug,
number = {2},
pages = {131--139},
title = {{A probabilistic justification for using tf x idf term weighting in information retrieval}},
url = {http://link.springer.com/10.1007/s007999900025},
volume = {3},
year = {2000}
}
@inproceedings{Samsonovich2007,
abstract = {The notion of a human value system can be quantified as a cognitive map, the dimensions of which capture the semantics of concepts and the associated values. This can be done, if one knows (i) how to define the dimensions of the map, and (ii) how to allocate concepts in those dimensions. Regarding the first question, experimental studies with linguistic material using psychometrics have revealed that valence, arousal and dominance are primary dimensions characterizing human values. The same or similar dimensions are used in popular models of emotions and affects. In these studies, the choice of principal dimensions, as well as scoring concepts, was based on subjective reports or psycho-physiological measurements. Can a cognitive map of human values be constructed without testing human subjects? Here we show that the answer is positive, using generally available dictionaries of synonyms and antonyms. By applying a simple statistical-mechanic model to English and French dictionaries, we constructed multidimensional cognitive maps that capture the semantics of words. We calculated the principal dimensions of the resultant maps and found their semantics consistent across two languages as well as with previously known main cognitive dimensions. These results suggest that the linguistically derived cognitive map of the human value system is language-invariant and, being closely related to psychometrically derived maps, is likely to reflect fundamental aspects of the human mind.},
address = {Amsterdam, The Netherlands},
author = {Samsonovich, Alexei V and Ascoli, Giorgio A.},
booktitle = {Proc AGI Workship Advances in Artificial General Intelligence Concepts Architectures and Algorithms},
editor = {Goertzel, B.},
pages = {111--124},
publisher = {IOS Press},
title = {{Cognitive map dimensions of the human value system extracted from natural language}},
year = {2007}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
annote = {The original paper on LDA},
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, Ng, Jordan - 2003 - Latent dirichlet allocation.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {Artificial intelligence,Computing methodologies,Language resources,Learning settings,Machine learning,Machine learning approaches,Natural language processing,Neural networks,lda,machine learning,natural language processing},
mendeley-tags = {lda,machine learning,natural language processing},
month = mar,
number = {1},
pages = {993--1022},
publisher = {JMLR.org},
title = {{Latent dirichlet allocation}},
url = {http://dl.acm.org/citation.cfm?id=944919.944937},
volume = {3},
year = {2003}
}
@inproceedings{Gill2007,
abstract = {Increasingly our perception of others is based on short samples of written text, for example, in e-mail or chat rooms. In this paper we will examine the extent to which text co- occurrence techniques, such as LSA, HAL, and PMI-IR, can be successfully applied to human personality perception based on short written texts. In particular, we compare two approaches: The first compares a “surface similarity” judgment of the text being rated to a description by the author of the text of his/her personality (Simulation 1). The second relies on extracting a very simple representation of author personality from extreme texts and judging the experimental texts on the basis of this representation (Simulation 2). Both of these approaches fail to distinguish personality type. We conclude that co-occurrence techniques, at least used in a relatively canonical way to assess personality from small text samples, are not only inadequate but, most probably, are not doing this in a way that is similar to how we humans rate personality from short text samples. Introduction},
author = {Gill, AJ and French, RM},
booktitle = {Proceedings of the 2nd European Cognitive Science Conference},
title = {{Level of representation and semantic distance: Rating author personality from texts}},
url = {http://leadserv.u-bourgogne.fr/IMG/pdf/Gill\_French.EuroCogSci2007.pdf},
year = {2007}
}
@article{Spelke1994,
abstract = {Although debates continue, studies of cognition in infancy suggest that knowledge begins to emerge early in life and constitutes part of humans' innate endowment. Early-developing knowledge appears to be both domain-specific and task-specific, it appears to capture fundamental constraints on ecologically important classes of entities in the child's environment, and it appears to remain central to the common-sense knowledge systems of adults.},
author = {Spelke, Elizabeth},
doi = {10.1016/0010-0277(94)90039-6},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spelke - 1994 - Initial knowledge six suggestions.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Child Psychology,Cognition,Concept Formation,Humans,Infant,Mental Recall,Motion Perception,Newborn,Problem Solving,Psychomotor Performance,Visual Perception},
month = apr,
number = {1-3},
pages = {431--445},
pmid = {8039373},
title = {{Initial knowledge: six suggestions}},
url = {http://www.sciencedirect.com/science/article/pii/0010027794900396 http://linkinghub.elsevier.com/retrieve/pii/0010027794900396 http://www.ncbi.nlm.nih.gov/pubmed/8039373},
volume = {50},
year = {1994}
}
@article{DeRaad2008,
abstract = {A list of 2,365 personality descriptive items was selected from a computerized database of the Dutch language. The list included terms from various word classes, such as trait adjectives, trait nouns, and trait verbs, and from expressions in which the meaning was drawn from a combination of words. The items were administered to 1,466 participants, who provided self- or other-ratings. Principal components analyses were performed on both original and ipsatized data. The data set was split to investigate the invariance of the factors. The analyses yielded a final 8-factorial structure that included the Big 5. Three new trait factors were discovered, namely Virtue, Competence, and Hedonism.},
author = {{De Raad}, Boele and Barelds, Dick P. H.},
doi = {10.1037/0022-3514.94.2.347},
issn = {0022-3514},
journal = {Journal of personality and social psychology},
keywords = {Ethnic Groups,Humans,Netherlands,Personality,Social Perception,Vocabulary,personality},
mendeley-tags = {personality},
month = feb,
number = {2},
pages = {347--64},
pmid = {18211182},
title = {{A new taxonomy of Dutch personality traits based on a comprehensive and unrestricted list of descriptors.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18211182},
volume = {94},
year = {2008}
}
@inproceedings{Yano2009,
abstract = {In this paper we model discussions in online political blogs. To do this, we extend Latent Dirichlet Allocation (Blei et al., 2003), in various ways to capture different characteristics of the data. Our models jointly describe the generation of the primary documents (posts) as well as the authorship and, optionally, the contents of the blog community’s verbal reactions to each post (comments). We evaluate our model on a novel comment prediction task where the models are used to predict which blog users will leave comments on a given post. We also provide a qualitative discussion about what the models discover.},
address = {Boulder, Colorado},
author = {Yano, Tae and Cohen, WW and Smith, NA},
booktitle = {Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yano, Cohen, Smith - 2009 - Predicting response to political blog posts with topic models.pdf:pdf},
number = {June},
pages = {477--485},
publisher = {Association for Computational Linguistics},
title = {{Predicting response to political blog posts with topic models}},
url = {http://dl.acm.org/citation.cfm?id=1620824},
year = {2009}
}
@article{Cattell1948,
abstract = {A reply to Professor Godfrey Thomson's review of the description and measurement of personality},
author = {Cattell, Raymond B},
doi = {10.1037/h0055271},
file = {:home/eric/Papers/Psychology/Personality/1948-The\_integration\_of\_factor\_analysis\_with\_psychology-Cattell\_R.pdf:pdf},
issn = {0022-0663},
journal = {Journal of Educational Psychology},
keywords = {factor analysis,psychology,statistics},
mendeley-tags = {factor analysis,psychology,statistics},
number = {4},
pages = {227--236},
title = {{The integration of factor analysis with psychology; a reply to Professor Godfrey Thomson's review of "The Description and Measurement of Personality."}},
url = {http://content.apa.org/journals/edu/39/4/227},
volume = {39},
year = {1948}
}
@techreport{Giesbers2006,
abstract = {This State of the art on Latent Semantic Analysis (LSA) captures current knowledge on and applications of LSA. Furthermore, it tries to connect this knowledge to other applications in education by identifying useful ways in which LSA can be used and what benefits it offers. Rather than being exhaustive, the deliverable tries to capture the essence of each topic. When appropriate, references for further reading are given.},
annote = {Cite as:
Iofciu, T., Zhou, X., Giesbers, B., Rusman, E., van Bruggen, J., Ceri, S. (2006). State of the Art Report in Knowledge Sharing, Recommendation and Latent Semantic Analysis.

      },
author = {Giesbers, B and Rusman, E and Bruggen, J Van},
file = {:home/eric/Papers/NaturalLanguageProcessing/2006-State\_of\_the\_art\_LSA.pdf:pdf},
institution = {Open University of the Netherlands},
keywords = {lsa,natural language processing,nlp},
mendeley-tags = {lsa,natural language processing,nlp},
pages = {76},
title = {{State of the Art LSA}},
url = {http://lnx-hrl-075v.web.pwo.ou.nl/handle/1820/685},
year = {2006}
}
@inproceedings{Sergienya2013,
abstract = {There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.},
archivePrefix = {arXiv},
arxivId = {1312.5559},
author = {Sergienya, Irina and Sch\"{u}tze, Hinrich},
booktitle = {ICLR Workshop},
eprint = {1312.5559},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sergienya, Sch\"{u}tze - 2013 - Distributional Models and Deep Learning Embeddings Combining the Best of Both Worlds.pdf:pdf},
keywords = {natural language processing},
mendeley-tags = {natural language processing},
month = dec,
pages = {4},
title = {{Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds}},
url = {http://arxiv.org/abs/1312.5559},
year = {2013}
}
@misc{SlyMerriamWebster2014,
author = {Merriam-Webster},
booktitle = {Merriam-Webster.com},
title = {"sly"},
url = {http://www.merriam-webster.com/dictionary/sly},
urldate = {24 April 2014},
year = {2014}
}
@article{Galton1884,
abstract = {first proposal of psycholexical aproach and sedimentationhypothesis - p.179: I tried to gain an idea of the number of the more conspicuous aspects the character by counting in an appropriate dictionary the words used to express them.},
author = {Galton, F},
journal = {Fortnightly Review},
pages = {179--185},
title = {{Measurement of Character}},
volume = {36},
year = {1884}
}
@book{Bartholomew2011,
address = {Hoboken, NJ, USA},
author = {Bartholomew, David J and Knott, Martin and Moustaki, Irini},
edition = {3rd Editio},
isbn = {9781119970590},
keywords = {Factor analysis,Latent structure analysis,Latent variables,statistics},
mendeley-tags = {statistics},
pages = {295},
publisher = {Wiley},
title = {{Latent Variable Models and Factor Analysis: A Unified Approach}},
year = {2011}
}
@article{Samsonovich2013,
abstract = {The emergent consensus on dimensional models of sentiment, appraisal, emotions, and values is on the semantics of the principal dimensions, typically interpreted as valence, arousal, and dominance. The notion of weak semantic maps was introduced recently as distribution of representations in abstract spaces that are not derived from human judgments, psychometrics, or any other a priori information about their semantics. Instead, they are defined entirely by binary semantic relations among representations, such as synonymy and antonymy. An interesting question concerns the ability of the antonymy-based semantic maps to capture all "universal" semantic dimensions. The present work shows that those narrow weak semantic maps are not complete in this sense and can be augmented with other semantic relations. Specifically, including hyponym-hypernym relations yields a new semantic dimension of the map labeled here "abstractness" (or ontological generality) that is not reducible to any dimensions represented by antonym pairs or to traditional affective space dimensions. It is expected that including other semantic relations (e.g., meronymy/holonymy) will also result in the addition of new semantic dimensions to the map. These findings have broad implications for automated quantitative evaluation of the meaning of text and may shed light on the nature of human subjective experience.},
author = {Samsonovich, Alexei V and Ascoli, Giorgio A.},
doi = {10.1155/2013/308176},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samsonovich, Ascoli - 2013 - Augmenting weak semantic cognitive maps with an abstractness dimension.pdf:pdf},
issn = {1687-5273},
journal = {Computational intelligence and neuroscience},
keywords = {Brain,Brain Mapping,Brain: physiology,Comprehension,Comprehension: physiology,Humans,Semantics,Vocabulary},
month = jan,
pages = {308176},
pmid = {23840200},
title = {{Augmenting weak semantic cognitive maps with an "abstractness" dimension.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3694378\&tool=pmcentrez\&rendertype=abstract},
volume = {2013},
year = {2013}
}
@inproceedings{Guthrie2006,
abstract = {Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams.},
author = {Guthrie, D and Allison, Ben and Liu, Wei},
booktitle = {Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006)},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guthrie, Allison, Liu - 2006 - A closer look at skip-gram modelling.pdf:pdf},
keywords = {language modeling,natural language processing},
mendeley-tags = {language modeling,natural language processing},
pages = {1222--1225},
title = {{A closer look at skip-gram modelling}},
url = {http://gandalf.aksis.uib.no/lrec2006/pdf/357\_pdf.pdf},
year = {2006}
}
@misc{Dao2013,
abstract = {The task of computationally learning the linguistic application of a word requires a robust analysis of both the word’s semantic presence and its independent characteristics. Word vectors are learned, functional representation of words which can then be analyzed for linguistic regularities. We examine whether the synonyms of a word can be recognized from their word vectors by training a neural network on a large corpus of text, then implementing k-means clustering to check whether synonyms have a statistically significant word vector similarity. We illustrate the output using a simplified version of the vectors generated by principal component analysis. Results suggest that increasing vector length, which improves syntactic and semantic accuracy when performing comparisons of words, negatively correlates with synonym recognition.},
annote = {Results are not good enough quality to cite.},
author = {Dao, Tri and Keller, Sam and Bejnood, Alborz},
file = {:home/eric/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dao, Keller, Bejnood - 2013 - Alternate Equivalent Substitutes Recognition of Synonyms Using Word Vectors.pdf:pdf},
keywords = {continuous skip-gram citation,thecat-nlp-app},
mendeley-tags = {continuous skip-gram citation,thecat-nlp-app},
pages = {2--6},
publisher = {Stanford University},
title = {{Alternate Equivalent Substitutes : Recognition of Synonyms Using Word Vectors}},
year = {2013}
}
@incollection{Goldberg1982,
abstract = {This chapter contains both the rationale for, and a brief progress report from, a research project whose goal is the development of a compelling taxonomic structure for the personality-descriptive terms in the natural languages. Our initial efforts, focused primarily on the English language, began with the development of a number of taxonomies of English trait-descriptive terms, one of which is described here in some detail. Also discussed are some of our preliminary efforts to structure the domain of other English personality terms, including those describing temporary states and activities, and those describing social roles, relationships, and effects. The basic motivation for this research project is the need that exists in theoretical and applied research contexts for a means of describing individuals in terms of their characteristic traits, dispositions, or styles of behavior. Our ultimate goal is to discover as much as possible about the nature of the processes involved in the description of oneself and others, and of the role of language and culture in those processes. Hopefully, this chapter will stimulate potential collaborators, including scientists who are fluent in languages other than English, to join in the quest for a scientifically compelling structure for those individual differences encoded into the natural languages.},
address = {Hillsdale, New Jersey},
author = {Goldberg, LR},
booktitle = {Advances in personality assessment},
chapter = {6},
editor = {Spielberger, Charles D. and Butcher, James N.},
file = {:home/eric/Papers/Psychology/Personality/1982-From\_ace\_to\_zombie\_some\_explorations\_in\_the\_language\_of\_personality-Goldberg\_L.pdf:pdf},
keywords = {personality},
mendeley-tags = {personality},
pages = {203--234},
publisher = {Lawrence Erlbaum Associates},
title = {{From Ace to Zombie: Some explorations in the language of personality}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:From+ace+to+zombie+some+explorations+in+the+language+of+personality\#0},
volume = {1},
year = {1982}
}
