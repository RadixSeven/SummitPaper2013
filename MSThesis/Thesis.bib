Automatically generated by Mendeley Desktop 1.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Norman1967,
abstract = {All terms in contemporary American English which pertain to aspects of human behavior or personal characteristics were assembled from available lexicons. These terms were categorized into 15 rubrics on the basis of judgments of their familiarity, specificity, and certain broad semantic criteria. Some 2,800 terms were identified which seemingly referred to relatively stable and specific "biophysical" traits of individuals. These words were presented to groups of university undergraduates to determine familiarity levels, specificity of connotative meaning, and a variety of psychometric operating characteristics (e.g., endorsement rates for self and for others, desirability, etc.). Results of the analysis of these data are presented and some of their potential uses for test development and personality description are suggested. Additional analyses currently in progress directed toward further refinement of the set and the development of a structured taxonomy based on these descriptors are briefly outlined.},
address = {Ann Arbor, MI},
author = {Norman, W. T.},
file = {:home/eric/Papers/Unknown/1967 - 2800 Personality Trait Descriptors --Normative Operating Characteristics for a University Population - Norman.pdf:pdf},
institution = {University of Michigan Department of Psychology},
keywords = {lexical hypothesis,personality,psychology},
mendeley-tags = {lexical hypothesis,personality,psychology},
title = {{2800 Personality Trait Descriptors --Normative Operating Characteristics for a University Population}},
url = {http://files.eric.ed.gov/fulltext/ED014738.pdf},
year = {1967}
}
@inproceedings{Sergienya2013,
abstract = {There are two main approaches to the distributed representation of words: low-dimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional-model vectors - as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.},
archivePrefix = {arXiv},
arxivId = {1312.5559},
author = {Sergienya, Irina and Sch\"{u}tze, Hinrich},
booktitle = {ICLR Workshop},
eprint = {1312.5559},
file = {:home/eric/Papers/ICLR Workshop/2013 - Distributional Models and Deep Learning Embeddings Combining the Best of Both Worlds - Sergienya, Sch\"{u}tze.pdf:pdf},
keywords = {natural language processing},
mendeley-tags = {natural language processing},
month = dec,
pages = {4},
title = {{Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds}},
url = {http://arxiv.org/abs/1312.5559},
year = {2013}
}
@inproceedings{Faruqui2014,
abstract = {The distributional hypothesis of Harris (1954), according to which the meaning of words is evidenced by the contexts they occur in, has motivated several effective techniques for obtaining vector space semantic representations of words using unannotated text corpora. This paper argues that lexico-semantic content should additionally be invariant across languages and proposes a simple technique based on canonical correlation analysis (CCA) for incorporating multilingual evidence into vectors generated monolingually. We evaluate the resultingword representations on standard lexical semantic evaluation tasks and show that our method produces substantially better semantic representations than monolingual techniques},
author = {Faruqui, Manaal and Dyer, Chris},
booktitle = {Proc. of EACL},
file = {:home/eric/Papers/Proc. of EACL/2014 - Improving Vector Space Word Representations Using Multilingual Correlation - Faruqui, Dyer.pdf:pdf},
keywords = {continuous skip-gram citation,machine translation,thecat-alg-improvement},
mendeley-tags = {continuous skip-gram citation,machine translation,thecat-alg-improvement},
title = {{Improving Vector Space Word Representations Using Multilingual Correlation}},
url = {http://www.cs.cmu.edu/~mfaruqui/papers/eacl14-vectors.pdf},
year = {2014}
}
@techreport{Samsonovich2012a,
abstract = {Abstract Web personalization involves automated content analysis of text, and modern technologies of semantic analysis of text rely on a number of scales. Among them is the abstractness of meaning, which is not captured by more traditional measures of sentiment, such as valence, arousal and dominance. The present work introduces a physics-inspired approach to constructing the abstractness scale based on databases of hypernym-hyponym relations, e.g., WordNet 3.0. The idea is to define an energy as a function of word coordinates that are distributed in one dimension, and then to find a global minimum of this energy function by relocating words in this dimension. The result is a one- dimensional distribution that assigns “abstractness” values to words. While positions of individual words on this scale are subject to noise, the entire distribution globally defines the universal semantic dimension associated with the notion of hypernym-hyponym relations, called here “abstractness”.},
author = {Samsonovich, AV},
booktitle = {Intelligent Techniques for Web Personalization and Recommender Systems},
file = {:home/eric/Papers/Intelligent Techniques for Web Personalization and Recommender Systems/2012 - A Metric Scale for 'Abstractness' of the Word Meaning - Samsonovich.pdf:pdf},
institution = {Association for the Advancement of Artificial Intelligence},
keywords = {quantification of meaning,recommender systems,semantic mapping,sentiment analysis},
pages = {48--52},
title = {{A Metric Scale for 'Abstractness' of the Word Meaning}},
url = {http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/viewFile/5340/5632},
year = {2012}
}
@misc{Osendorfer2013,
abstract = {Recent research from Microsoft shows that “King - Man + Woman” almost equals “Queen” [1]. An open source project from Google that comes to the same conclusion offers an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. We are looking into expanding this work to non-verbal domains, in particular to music. While Google trains their models on data sets of billions of words, we will train our models on data sets of up to hundreds of millions of notes. Apart from providing a sound and scalable way of computing musical similarity and introducing the notions of analogy and metaphor to music - a significant contribution to musicology in its own right - this highly publishable research will have many synthetic/creative applications. For example, using the intermediate vector representation for translating language to music and back, which has never been done before.},
author = {Osendorfer, Christian and Viro, Vladimir},
file = {:home/eric/Papers/Unknown/2013 - Scalable Vector Arithmetics for Music using RNNs - Osendorfer, Viro.pdf:pdf},
keywords = {continuous skip-gram citation,information retrieval,music,thecat-non-nlp-app},
mendeley-tags = {continuous skip-gram citation,information retrieval,music,thecat-non-nlp-app},
pages = {1},
title = {{Scalable Vector Arithmetics for Music using RNNs}},
year = {2013}
}
@inproceedings{Cardie2013,
abstract = {Recently, deep architectures, such as recurrent and recursive neural networks have been successfully applied to various natural language processing tasks. Inspired by bidirectional recurrent neural networks which use representations that summarize the past and future around an instance, we propose a novel architecture that aims to capture the structural information around an input, and use it to label instances. We apply our method to the task of opinion expression extraction, where we employ the binary parse tree of a sentence as the structure, and word vector representations as the initial representation of a single token. We conduct preliminary experiments to investigate its performance and compare it to the sequential approach.},
annote = {They use an embedding - but it is not mikolov. They cite him just to mention that the embeddings might have interesting semantic properties in themselves.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0493v1},
author = {Cardie, Claire and Irsoy, Ozan ˙},
booktitle = {Advances in neural information processing systems},
eprint = {arXiv:1312.0493v1},
file = {:home/eric/Papers/Advances in neural information processing systems/2013 - Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure - Cardie, Irsoy.pdf:pdf},
keywords = {continuous skip-gram citation,thecat-nlp-app},
mendeley-tags = {continuous skip-gram citation,thecat-nlp-app},
pages = {1--9},
title = {{Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure}},
year = {2013}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
annote = {The original paper on LDA},
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
file = {:home/eric/Papers/The Journal of Machine Learning Research/2003 - Latent dirichlet allocation - Blei, Ng, Jordan.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {Artificial intelligence,Computing methodologies,Language resources,Learning settings,Machine learning,Machine learning approaches,Natural language processing,Neural networks,lda,machine learning,natural language processing},
mendeley-tags = {lda,machine learning,natural language processing},
month = mar,
number = {1},
pages = {993--1022},
publisher = {JMLR.org},
title = {{Latent dirichlet allocation}},
url = {http://dl.acm.org/citation.cfm?id=944919.944937},
volume = {3},
year = {2003}
}
@inproceedings{Nowson2005,
abstract = {Blogs are personal online diaries, and a relatively recent form of computer-mediated communication. What kind of writing do they contain? This paper adopts ameasure of linguistic contextuality/formality, due to Heylighen and Dewaele, and applies it to a corpus of weblogs. It first compares the corpus with sub-corpora from the British National Corpus, and weblogs are shown to be more formal than e-mail, but less formal than biographies. Then, the paper explores the impact of individual differences between writers on their texts’ contextuality/formality. It appears that Extraversion and Neuroticism are less influential than previously supposed, and it is argued that gender and Agreeableness account for more of the variability in the extent to which weblog writers take their readers’ contexts into account.},
address = {Stresa, Italy},
author = {Nowson, Scott and Oberlander, Jon and Gill, Alastair J},
booktitle = {Proceedings of the 27th \ldots},
file = {:home/eric/Papers/Proceedings of the 27th \ldots/2005 - Weblogs, genres and individual differences - Nowson, Oberlander, Gill.pdf:pdf},
pages = {1666--1671},
title = {{Weblogs, genres and individual differences}},
url = {http://csjarchive.cogsci.rpi.edu/Proceedings/2005/docs/p1666.pdf},
year = {2005}
}
@article{Hiemstra2000,
abstract = {This paper presents a new probabilistic model of information retrieval. The most important modeling assumption made is that documents and queries are defined by an ordered sequence of single terms. This assumption is not made in well-known existing models of information retrieval, but is essential in the field of statistical natural language processing. Advances already made in statistical natural language processing will be used in this paper to formulate a probabilistic justification for using tf×idf term weighting. The paper shows that the new probabilistic interpretation of tf×idf term weighting might lead to better understanding of statistical ranking mechanisms, for example by explaining how they relate to coordination level ranking. A pilot experiment on the TREC collection shows that the linguistically motivated weighting algorithm outperforms the popular BM25 weighting algorithm.},
author = {Hiemstra, Djoerd},
doi = {10.1007/s007999900025},
file = {:home/eric/Papers/International Journal on Digital Libraries/2000 - A probabilistic justification for using tf x idf term weighting in information retrieval - Hiemstra.pdf:pdf},
issn = {1432-5012},
journal = {International Journal on Digital Libraries},
keywords = {Information retrieval theory,Statistical information retrieval,Statistical natural language processing},
month = aug,
number = {2},
pages = {131--139},
title = {{A probabilistic justification for using tf x idf term weighting in information retrieval}},
url = {http://link.springer.com/10.1007/s007999900025},
volume = {3},
year = {2000}
}
@article{Samsonovich2013,
abstract = {The emergent consensus on dimensional models of sentiment, appraisal, emotions, and values is on the semantics of the principal dimensions, typically interpreted as valence, arousal, and dominance. The notion of weak semantic maps was introduced recently as distribution of representations in abstract spaces that are not derived from human judgments, psychometrics, or any other a priori information about their semantics. Instead, they are defined entirely by binary semantic relations among representations, such as synonymy and antonymy. An interesting question concerns the ability of the antonymy-based semantic maps to capture all "universal" semantic dimensions. The present work shows that those narrow weak semantic maps are not complete in this sense and can be augmented with other semantic relations. Specifically, including hyponym-hypernym relations yields a new semantic dimension of the map labeled here "abstractness" (or ontological generality) that is not reducible to any dimensions represented by antonym pairs or to traditional affective space dimensions. It is expected that including other semantic relations (e.g., meronymy/holonymy) will also result in the addition of new semantic dimensions to the map. These findings have broad implications for automated quantitative evaluation of the meaning of text and may shed light on the nature of human subjective experience.},
author = {Samsonovich, Alexei V and Ascoli, Giorgio A.},
doi = {10.1155/2013/308176},
file = {:home/eric/Papers/Computational intelligence and neuroscience/2013 - Augmenting weak semantic cognitive maps with an abstractness dimension. - Samsonovich, Ascoli.pdf:pdf},
issn = {1687-5273},
journal = {Computational intelligence and neuroscience},
keywords = {Brain,Brain Mapping,Brain: physiology,Comprehension,Comprehension: physiology,Humans,Semantics,Vocabulary},
month = jan,
pages = {308176},
pmid = {23840200},
title = {{Augmenting weak semantic cognitive maps with an "abstractness" dimension.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3694378\&tool=pmcentrez\&rendertype=abstract},
volume = {2013},
year = {2013}
}
@inproceedings{Mikolov2013b,
abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {arXiv preprint arXiv:1301.3781},
eprint = {1301.3781},
file = {:home/eric/Papers/arXiv preprint arXiv1301.3781/2013 - Efficient Estimation of Word Representations in Vector Space - Mikolov et al.pdf:pdf},
month = jan,
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781v3},
year = {2013}
}
@inproceedings{Mikolov2013,
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40\% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.},
address = {Atlanta, GA},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
booktitle = {Proceedings of NAACL-HLT},
file = {:home/eric/Papers/Proceedings of NAACL-HLT/2013 - Linguistic regularities in continuous space word representations - Mikolov, Yih, Zweig.pdf:pdf;:home/eric/Papers/Proceedings of NAACL-HLT/2013 - Linguistic regularities in continuous space word representations - Mikolov, Yih, Zweig(2).pdf:pdf},
keywords = {natural language processing,neural networks,semantics},
mendeley-tags = {natural language processing,neural networks,semantics},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://acl.eldoc.ub.rug.nl/mirror/N/N13/N13-1090.pdf},
year = {2013}
}
@inproceedings{Samsonovich2007,
abstract = {The notion of a human value system can be quantified as a cognitive map, the dimensions of which capture the semantics of concepts and the associated values. This can be done, if one knows (i) how to define the dimensions of the map, and (ii) how to allocate concepts in those dimensions. Regarding the first question, experimental studies with linguistic material using psychometrics have revealed that valence, arousal and dominance are primary dimensions characterizing human values. The same or similar dimensions are used in popular models of emotions and affects. In these studies, the choice of principal dimensions, as well as scoring concepts, was based on subjective reports or psycho-physiological measurements. Can a cognitive map of human values be constructed without testing human subjects? Here we show that the answer is positive, using generally available dictionaries of synonyms and antonyms. By applying a simple statistical-mechanic model to English and French dictionaries, we constructed multidimensional cognitive maps that capture the semantics of words. We calculated the principal dimensions of the resultant maps and found their semantics consistent across two languages as well as with previously known main cognitive dimensions. These results suggest that the linguistically derived cognitive map of the human value system is language-invariant and, being closely related to psychometrically derived maps, is likely to reflect fundamental aspects of the human mind.},
address = {Amsterdam, The Netherlands},
author = {Samsonovich, Alexei V and Ascoli, Giorgio A.},
booktitle = {Proc AGI Workship Advances in Artificial General Intelligence Concepts Architectures and Algorithms},
editor = {Goertzel, B.},
pages = {111--124},
publisher = {IOS Press},
title = {{Cognitive map dimensions of the human value system extracted from natural language}},
year = {2007}
}
@article{Perozzi2014,
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide \$F\_1\$ scores up to 10\% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60\% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
archivePrefix = {arXiv},
arxivId = {1403.6652},
author = {Perozzi, Bryan and Al-Rfou', Rami and Skiena, Steven},
eprint = {1403.6652},
file = {:home/eric/Papers/Unknown/2014 - DeepWalk Online Learning of Social Representations - Perozzi, Al-Rfou', Skiena.pdf:pdf},
keywords = {continuous skip-gram citation,social network,thecat-non-nlp-app},
mendeley-tags = {continuous skip-gram citation,social network,thecat-non-nlp-app},
month = mar,
pages = {10},
title = {{DeepWalk: Online Learning of Social Representations}},
url = {http://arxiv.org/abs/1403.6652},
year = {2014}
}
@misc{Dao2013,
abstract = {The task of computationally learning the linguistic application of a word requires a robust analysis of both the word’s semantic presence and its independent characteristics. Word vectors are learned, functional representation of words which can then be analyzed for linguistic regularities. We examine whether the synonyms of a word can be recognized from their word vectors by training a neural network on a large corpus of text, then implementing k-means clustering to check whether synonyms have a statistically significant word vector similarity. We illustrate the output using a simplified version of the vectors generated by principal component analysis. Results suggest that increasing vector length, which improves syntactic and semantic accuracy when performing comparisons of words, negatively correlates with synonym recognition.},
annote = {Results are not good enough quality to cite.},
author = {Dao, Tri and Keller, Sam and Bejnood, Alborz},
file = {:home/eric/Papers/Unknown/2013 - Alternate Equivalent Substitutes Recognition of Synonyms Using Word Vectors - Dao, Keller, Bejnood.pdf:pdf},
keywords = {continuous skip-gram citation,thecat-nlp-app},
mendeley-tags = {continuous skip-gram citation,thecat-nlp-app},
pages = {2--6},
publisher = {Stanford University},
title = {{Alternate Equivalent Substitutes : Recognition of Synonyms Using Word Vectors}},
year = {2013}
}
@book{Bartholomew2011,
address = {Hoboken, NJ, USA},
author = {Bartholomew, David J and Knott, Martin and Moustaki, Irini},
edition = {3rd Editio},
isbn = {9781119970590},
keywords = {Factor analysis,Latent structure analysis,Latent variables,statistics},
mendeley-tags = {statistics},
pages = {295},
publisher = {Wiley},
title = {{Latent Variable Models and Factor Analysis: A Unified Approach}},
year = {2011}
}
@inproceedings{Schwartz2013a,
abstract = {Language in social media reveals a lot about people’s personality and mood as they discuss the activities and relationships that constitute their everyday lives. Although social media are widely studied, researchers in computational linguistics have mostly focused on prediction tasks such as sentiment analysis and authorship attribution. In this paper, we showhowsocial media can also be used to gain psychological insights. We demonstrate an exploration of language use as a function of age, gender, and personality from a dataset of Facebook posts from 75,000 people who have also taken personality tests, and we suggest how more sophisticated tools could be brought to bear on such data.},
author = {Schwartz, H Andrew and Eichstaedt, Johannes C and Dziurzynski, Lukasz and Kern, Margaret L and Seligman, Martin E. P. and Ungar, Lyle H. and Blanco, Eduardo and Kosinski, Michal and Stillwell, David},
booktitle = {2013 AAAI Spring Symposium},
file = {:home/eric/Papers/2013 AAAI Spring Symposium/2013 - Toward Personality Insights from Language Exploration in Social Media - Schwartz et al.pdf:pdf},
keywords = {facebook,personality,social media},
mendeley-tags = {facebook,personality,social media},
pages = {72--79},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{Toward Personality Insights from Language Exploration in Social Media}},
url = {http://www.aaai.org/ocs/index.php/SSS/SSS13/paper/download/5764/5915},
year = {2013}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality:a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such largemodels (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Jauvin, Christian},
file = {:home/eric/Papers/Journal ofMachine Learning Research/2003 - A Neural Probabilistic Language Model - Bengio et al.pdf:pdf},
journal = {Journal ofMachine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@inproceedings{Pennacchiotti2011,
abstract = {This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user’s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.},
author = {Pennacchiotti, Marco and Popescu, Ana-Maria},
booktitle = {ICWSM},
file = {:home/eric/Papers/ICWSM/2011 - A Machine Learning Approach to Twitter User Classification. - Pennacchiotti, Popescu.pdf:pdf},
keywords = {machine learning,natural language processing},
mendeley-tags = {machine learning,natural language processing},
pages = {281--288},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{A Machine Learning Approach to Twitter User Classification.}},
url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2886/3262},
year = {2011}
}
@article{Allport1936,
author = {Allport, Gordon W. and Odbert, Henry S.},
doi = {10.1037/h0093360},
file = {:home/eric/Papers/Psychological Monographs/1936 - Trait-names A psycho-lexical study. - Allport, Odbert(2).pdf:pdf},
issn = {0096-9753},
journal = {Psychological Monographs},
keywords = {lexical hypothesis,personality,psychology},
mendeley-tags = {lexical hypothesis,personality,psychology},
number = {1},
pages = {i--171},
title = {{Trait-names: A psycho-lexical study.}},
url = {http://psycnet.apa.org/journals/mon/47/1/i/ http://doi.apa.org/getdoi.cfm?doi=10.1037/h0093360},
volume = {47},
year = {1936}
}
@article{Norouzi2013,
abstract = {Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional $\backslash$nway\{\} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing $\backslash$nway\{\} image classifier and a semantic word embedding model, which contains the \$\backslash n\$ class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.},
archivePrefix = {arXiv},
arxivId = {1312.5650},
author = {Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S. and Dean, Jeffrey},
eprint = {1312.5650},
file = {:home/eric/Papers/arXiv1312.5650/2013 - Zero-Shot Learning by Convex Combination of Semantic Embeddings - Norouzi et al.pdf:pdf},
journal = {arXiv:1312.5650},
keywords = {computer-vision,continuous skip-gram citation,image analysis,thecat-non-nlp-app},
mendeley-tags = {computer-vision,continuous skip-gram citation,image analysis,thecat-non-nlp-app},
month = dec,
title = {{Zero-Shot Learning by Convex Combination of Semantic Embeddings}},
url = {http://arxiv.org/abs/1312.5650},
year = {2013}
}
@article{Tupes1992,
abstract = {Intercorrelations among ratings on 35 personality traits, selected as representative of the personality domain, were obtained for eight samples. These samples differed in length of acquaintanceship from 3 days to more than a year; in kind of acquaintanceship from assessment programs in a military training course to a fraternity house situation; in type of subject from airmen with only a high-school education to male and female undergraduate students to first-year graduate students; and in type of rater from very naive persons to clinical psychologists and psychiatrists with years of experience in the evaluation of personality. Centroid or multiple-group factors were extracted and rotated orthogonally to simple structure. For one study, an independent solution was obtained in which analytic rotations were accomplished on an IBM 650 computer using Kaiser's normal varimax criterion. Five fairly strong and recurrent factors emerged from each analysis, labeled as (a) Surgency, (b) Agreeableness, (c) Dependability, (d) Emotional Stability, and (e) Culture.},
author = {Tupes, E C and Christal, R E},
institution = {Armstrong Laboratory, Brooks Air Force Base, TX 78235-5601.},
journal = {Journal of personality},
number = {2},
pages = {225--251},
pmid = {1635043},
title = {{Recurrent personality factors based on trait ratings.}},
volume = {60},
year = {1992}
}
@inproceedings{Mikolov2013a,
abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
address = {Mountain View, CA},
archivePrefix = {arXiv},
arxivId = {1309.4168},
author = {Mikolov, Tomas and Le, Quoc V and Sutskever, Ilya},
booktitle = {arXiv preprint arXiv:1309.4168v1},
eprint = {1309.4168},
file = {:home/eric/Papers/arXiv preprint arXiv1309.4168v1/2013 - Exploiting Similarities among Languages for Machine Translation - Mikolov, Le, Sutskever.pdf:pdf},
keywords = {continuous skip-gram citation,machine learning,machine translation,natural language processing,neural networks,semantics,thecat-nlp-app},
mendeley-tags = {continuous skip-gram citation,machine learning,machine translation,natural language processing,neural networks,semantics,thecat-nlp-app},
month = sep,
organization = {Google Inc.},
pages = {1--10},
series = {arXive},
title = {{Exploiting Similarities among Languages for Machine Translation}},
url = {http://arxiv.org/abs/1309.4168v1 http://arxiv.org/abs/1309.4168},
year = {2013}
}
@inproceedings{Hofmann1999,
abstract = {Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
author = {Hofmann, Thomas},
booktitle = {UAI'99 Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence},
file = {:home/eric/Papers/UAI'99 Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence/1999 - Probabilistic latent semantic analysis - Hofmann.pdf:pdf},
isbn = {1-55860-614-9},
keywords = {artificial intelligence,computing methodologies,knowledge representation and reasoning,mathematics in computing,models of computation,natural language processing,plsa,probabilistic computation,probabilistic reasoning,probability and statistics,theory of computation,vagueness and fuzzy logic},
mendeley-tags = {natural language processing,plsa},
month = jul,
pages = {289--296},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Probabilistic latent semantic analysis}},
url = {http://dl.acm.org/citation.cfm?id=2073796.2073829},
year = {1999}
}
@phdthesis{Mikolov2007,
author = {Mikolov, Tomas},
keywords = {machine learning,natural language processing,neural networks},
mendeley-tags = {machine learning,natural language processing,neural networks},
school = {Brno Uni- versity of Technology},
title = {{Language Modeling for Speech Recognition in Czech}},
type = {Masters thesis},
year = {2007}
}
@inproceedings{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18\% across thou- sands of novel labels never seen by the visual model. 1},
author = {Frome, Andrea and Corrado, Greg S and Shlens, Jonathon and Bengio, Samy and Dean, Jeffrey and Ranzato, Marc'Aurelio and Mikolov, Tomas},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/eric/Papers/Advances in Neural Information Processing Systems/2013 - Devise A deep visual-semantic embedding model - Frome et al.pdf:pdf},
keywords = {computer-vision,continuous skip-gram citation,image analysis,image processing,thecat-non-nlp-app},
mendeley-tags = {computer-vision,continuous skip-gram citation,image analysis,image processing,thecat-non-nlp-app},
pages = {1--9},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@inproceedings{Dumais1988,
abstract = {This paper describes a new approach for dealing with the vocabulary problem in human-computer interaction. Most approaches to retrieving textual materials depend on a lexical match between words in users' requests and those in or assigned to database objects. Because of the tremendous diversity in the words people use to describe the same object, lexical matching methods are necessarily incomplete and imprecise [5]. The latent semantic indexing approach tries to overcome these problems by automatically organizing text objects into a semantic structure more appropriate for matching user requests. This is done by taking advantage of implicit higher-order structure in the association of terms with text objects. The particular technique used is singular-value decomposition, in which a large term by text-object matrix is decomposed into a set of about 50 to 150 orthogonal factors from which the original matrix can be approximated by linear combination. Terms and objects are represented by 50 to 150 dimensional vectors and matched against user queries in this “semantic” space. Initial tests find this completely automatic method widely applicable and a promising way to improve users' access to many kinds of textual materials, or to objects and services for which textual descriptions are available.},
address = {New York, New York, USA},
author = {Dumais, S. T. and Furnas, G. W. and Landauer, T. K. and Deerwester, S. and Harshman, R.},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '88},
doi = {10.1145/57167.57214},
editor = {O'Hare, J J},
file = {:home/eric/Papers/Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '88/1988 - Using latent semantic analysis to improve access to textual information - Dumais et al.pdf:pdf},
isbn = {0201142376},
keywords = {applied computing,artificial intelligence,arts and humanities,computing methodologies,document management and text processing,document preparation,document representation,document scripting languages,human computer interaction,human-centered computing,information retrieval,information systems,language translation,natural language processing,retrieval models and ranking,search engine architectures and scalability,search engine indexing},
pages = {281--285},
publisher = {ACM Press},
title = {{Using latent semantic analysis to improve access to textual information}},
url = {http://dl.acm.org/citation.cfm?id=57214 http://portal.acm.org/citation.cfm?doid=57167.57214},
year = {1988}
}
@inproceedings{Guthrie2006,
abstract = {Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams.},
author = {Guthrie, D and Allison, Ben and Liu, Wei},
booktitle = {Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006)},
file = {:home/eric/Papers/Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006)/2006 - A closer look at skip-gram modelling - Guthrie, Allison, Liu.pdf:pdf},
keywords = {language modeling,natural language processing},
mendeley-tags = {language modeling,natural language processing},
pages = {1222--1225},
title = {{A closer look at skip-gram modelling}},
url = {http://gandalf.aksis.uib.no/lrec2006/pdf/357\_pdf.pdf},
year = {2006}
}
@inproceedings{Yano2009,
abstract = {In this paper we model discussions in online political blogs. To do this, we extend Latent Dirichlet Allocation (Blei et al., 2003), in various ways to capture different characteristics of the data. Our models jointly describe the generation of the primary documents (posts) as well as the authorship and, optionally, the contents of the blog community’s verbal reactions to each post (comments). We evaluate our model on a novel comment prediction task where the models are used to predict which blog users will leave comments on a given post. We also provide a qualitative discussion about what the models discover.},
address = {Boulder, Colorado},
author = {Yano, Tae and Cohen, WW and Smith, NA},
booktitle = {Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL},
file = {:home/eric/Papers/Human Language Technologies The 2009 Annual Conference of the North American Chapter of the ACL/2009 - Predicting response to political blog posts with topic models - Yano, Cohen, Smith.pdf:pdf},
number = {June},
pages = {477--485},
publisher = {Association for Computational Linguistics},
title = {{Predicting response to political blog posts with topic models}},
url = {http://dl.acm.org/citation.cfm?id=1620824},
year = {2009}
}
@inproceedings{Shen2013,
abstract = {Email is a ubiquitous communication tool and constitutes a significant portion of social interactions. In this paper, we attempt to infer the personality of users based on the content of their emails. Such inference can enable valuable applications such as better personalization, recommendation, and targeted advertising. Considering the private and sensitive nature of email content, we propose a privacy-preserving approach for collecting email and personality data. We then frame personality prediction based on the well-known Big Five personality model and train predictors based on extracted email features. We report prediction performance of 3 generative models with different assumptions. Our results show that personality prediction is feasible, and our email feature set can predict personality with reasonable accuracies.},
address = {Heidelberg},
author = {Shen, Jianqiang and Brdiczka, Oliver and Liu, Juan},
booktitle = {User Modeling, Adaptation, and Personalization},
editor = {Carberry, Sandra and Weibelzahl, Stephan and Micarelli, Alessandro and Semeraro, Giovanni},
file = {:home/eric/Papers/User Modeling, Adaptation, and Personalization/2013 - Understanding email writers Personality prediction from email messages - Shen, Brdiczka, Liu.pdf:pdf},
keywords = {behavior analysis,email,personality,text processing},
pages = {318--330},
publisher = {Springer Verlag},
title = {{Understanding email writers: Personality prediction from email messages}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-38844-6\_29},
year = {2013}
}
@inproceedings{Mikolov2013c,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/eric/Papers/Advances in Neural Information Processing Systems/2013 - Distributed Representations of Words and Phrases and their Compositionality - Mikolov et al.pdf:pdf},
keywords = {continuous skip-gram citation,machine learning,natural language processing,thecat-alg-improvement},
mendeley-tags = {continuous skip-gram citation,machine learning,natural language processing,thecat-alg-improvement},
pages = {3111--3119},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality},
year = {2013}
}
@inproceedings{Bordes2013,
abstract = {We consider the problem of embedding entities and relations of knowledge bases in low-dimensional vector spaces. Unlike most existing approaches, which are primarily efficient for modeling equivalence relations, our approach is designed to explicitly model irreflexive relations, such as hierarchies, by interpreting them as translations operating on the low-dimensional embeddings of the entities. Preliminary experiments show that, despite its simplicity and a smaller number of parameters than previous approaches, our approach achieves state-of-the-art performance according to standard evaluation protocols on data from WordNet and Freebase.},
archivePrefix = {arXiv},
arxivId = {1304.7158},
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
booktitle = {arXiv:1304.7158},
eprint = {1304.7158},
file = {:home/eric/Papers/arXiv1304.7158/2013 - Irreflexive and Hierarchical Relations as Translations - Bordes et al.pdf:pdf},
keywords = {continuous skip-gram citation,natural language processing,thecat-alg-improvement},
mendeley-tags = {continuous skip-gram citation,natural language processing,thecat-alg-improvement},
month = apr,
title = {{Irreflexive and Hierarchical Relations as Translations}},
url = {http://arxiv.org/abs/1304.7158},
year = {2013}
}
@article{Samsonovich2010,
abstract = {Metric systems for semantics, or semantic cognitive maps, are allocations of words or other representations in a metric space based on their meaning. Existing methods for semantic mapping, such as Latent Semantic Analysis and Latent Dirichlet Allocation, are based on paradigms involving dissimilarity metrics. They typically do not take into account relations of antonymy and yield a large number of domain-specific semantic dimensions. Here, using a novel self-organization approach, we construct a low-dimensional, context-independent semantic map of natural language that represents simultaneously synonymy and antonymy. Emergent semantics of the map principal components are clearly identifiable: the first three correspond to the meanings of "good/bad" (valence), "calm/excited" (arousal), and "open/closed" (freedom), respectively. The semantic map is sufficiently robust to allow the automated extraction of synonyms and antonyms not originally in the dictionaries used to construct the map and to predict connotation from their coordinates. The map geometric characteristics include a limited number ( approximately 4) of statistically significant dimensions, a bimodal distribution of the first component, increasing kurtosis of subsequent (unimodal) components, and a U-shaped maximum-spread planar projection. Both the semantic content and the main geometric features of the map are consistent between dictionaries (Microsoft Word and Princeton's WordNet), among Western languages (English, French, German, and Spanish), and with previously established psychometric measures. By defining the semantics of its dimensions, the constructed map provides a foundational metric system for the quantitative analysis of word meaning. Language can be viewed as a cumulative product of human experiences. Therefore, the extracted principal semantic dimensions may be useful to characterize the general semantic dimensions of the content of mental states. This is a fundamental step toward a universal metric system for semantics of human experiences, which is necessary for developing a rigorous science of the mind.},
author = {Samsonovich, Alexei V and Ascoli, Giorgio A.},
doi = {10.1371/journal.pone.0010921},
file = {:home/eric/Papers/PloS one/2010 - Principal semantic components of language and the measurement of meaning. - Samsonovich, Ascoli.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Language,Semantics,natural language processing,personality,semantics},
mendeley-tags = {natural language processing,personality,semantics},
month = jan,
number = {6},
pages = {e10921},
pmid = {20552009},
title = {{Principal semantic components of language and the measurement of meaning.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2883995\&tool=pmcentrez\&rendertype=abstract},
volume = {5},
year = {2010}
}
@article{Imperio2008,
abstract = {Lexical studies have focused on traits. In the Filipino language, we investigated whether additional dimensions can be identified when personality-relevant terms for social roles, statuses, and effects, plus physical attributes, are included. Filipino students (N = 496) rated themselves on 268 such terms, plus 253 markers of trait and evaluative dimensions. We identified 10 dimensions of social and physical attributes-Prominence, Uselessness, Attractiveness, Respectability, Uniqueness, Destructiveness, Presentableness, Strength, Dangerousness, and Charisma. Most of these dimensions did not correspond in a one-to-one manner to Filipino or alternative trait models (Big Five, HEXACO, ML7). However, considerable redundancy was observed between the social and physical attribute dimensions and trait and evaluative dimensions. Thus, social and physical attributes communicate information about personality traits, and vice-versa.},
author = {Imperio, Shellah Myra and Church, A Timothy and Katigbak, Marcia S and Reyes, Jose Alberto S},
doi = {10.1002/per.673},
file = {:home/eric/Papers/European journal of personality/2008 - Lexical Studies of Filipino Person Descriptors Adding Personality-Relevant Social and Physical Attributes. - Imperio et al.pdf:pdf},
issn = {0890-2070},
journal = {European journal of personality},
keywords = {a useful starting point,an important task in,and individual differences,and number of,for describing the nature,indigenous,lexical hypothesis,lexical studies,personality,personality structure,philippines,the natural language provides,the study of personality,ways that people differ},
mendeley-tags = {lexical hypothesis,personality},
month = jun,
number = {4},
pages = {291--321},
pmid = {19779603},
title = {{Lexical Studies of Filipino Person Descriptors: Adding Personality-Relevant Social and Physical Attributes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2749699\&tool=pmcentrez\&rendertype=abstract},
volume = {22},
year = {2008}
}
@inproceedings{Hill2001,
abstract = {Design as a social activity characterized by information exchange, compromise and negotiation frames much of our understanding of the design process. At the heart of this social activity is the development of a shared understanding of the design problem. The design stakeholders jointly form a shared understanding through a process of defining the problem, exploring the space of solutions and searching for information. A shared understanding is a critical element in successful, collaborative design. This paper describes a formal approach for identifying shared understanding in design by analyzing the documentation. By mining the documentation for signatures of a shared understanding, specifically a common frame of reference and a similar voice, a shared language and vocabulary of the design could emerge. Information management tools built around the shared understanding would be more effective in directing and alerting the design team of relevant information.},
address = {Pittsburgh, Pennsylvania},
author = {Hill, Andrew and Song, Shuang and Dong, Andy and Agogino, Alice},
booktitle = {Proceedings of the 13-th International Conference on Design Theory and Methodology},
file = {:home/eric/Papers/Proceedings of the 13-th International Conference on Design Theory and Methodology/2001 - Identifying shared understanding in design using document analysis - Hill et al.pdf:pdf},
pages = {1--7},
title = {{Identifying shared understanding in design using document analysis}},
year = {2001}
}
@article{Peabody2002,
abstract = {The psycholexical approach to personality structure in American English has led to the Big Five factors. The present study considers whether this result is similar or different in other languages. Instead of placing the usual emphasis on quantitative indices, this study examines the substantive nature of the factors. Six studies in European languages were used to develop a taxonomy of content categories. The English translations of the relevant terms were then classified under this taxonomy. The results support the generality of Big Five Factor III (Conscientiousness). Factors IV (Emotional Stability) and V (Intellect) generally did not cohere. Factors I (Extraversion) and II (Agreeableness) tended to split when this was necessary to produce 5 factors. The analysis was extended to several additional studies.},
author = {Peabody, Dean and {De Raad}, Boele},
doi = {10.1037//0022-3514.83.4.983},
file = {:home/eric/Papers/Journal of Personality and Social Psychology/2002 - The substantive nature of psycholexical personality factors A comparison across languages. - Peabody, De Raad.pdf:pdf},
issn = {0022-3514},
journal = {Journal of Personality and Social Psychology},
keywords = {lexical hypothesis,personality},
mendeley-tags = {lexical hypothesis,personality},
number = {4},
pages = {983--997},
title = {{The substantive nature of psycholexical personality factors: A comparison across languages.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.83.4.983},
volume = {83},
year = {2002}
}
@article{Cattell1947,
abstract = {In connection with a study bridging rating, questionnaire, and objective test factors, confirmation was sought with respect to the twelve personality factors previously found for young adult men. Variables were chosen to clarify and discriminate the nature of related factors. Ratings of and by 373 students were obtained, and the present study describes the separate factorization for the 1333 men among them. Factorization yielded eleven factors, of which, on "blind" rotation for simple structure, 9 or 10 proved to be identical with those of the previous study. A new factor M is described.},
author = {Cattell, RB},
file = {:home/eric/Papers/Psychometrika/1947 - Confirmation and clarification of primary personality factors - Cattell.pdf:pdf},
journal = {Psychometrika},
keywords = {personality},
mendeley-tags = {personality},
number = {3},
pages = {197--220},
title = {{Confirmation and clarification of primary personality factors}},
url = {http://link.springer.com/article/10.1007/BF02289253},
volume = {12},
year = {1947}
}
@article{Wolf,
abstract = {We extend the word2vec framework to capture meaning across languages. The input consists of a source text and a word-aligned parallel text in a second language. The joint word2vec tool then represents words in both languages within a common “semantic” vector space. The result can be used to enrich lexicons of under-resourced languages, to identify ambiguities, and to perform clustering and classification. Experiments were conducted on a parallel English-Arabic corpus, as well as on English and Hebrew Biblical texts.},
annote = {lexicographers; others are substantive and are unlikely to share the same words in multiple languages. Accordingly, it has been argued (e.g., [6,9,8]) that sense distinctions can be derived from co-occurrence statistics across languages. To quote [13]: “Homograph distinctions do not require a lexicographer to locate them, since they are basically those that can be found easily in parallel texts in different languages”.
        
First does monolingual training. Then considers neighborhood of word in language A to be those words in the neighborhood of its aligned pair in language B and vice-versa. Uses the CBOW variant.},
author = {Wolf, Lior and Hanani, Yair and Bar, Kfir and Dershowitz, Nachum},
file = {:home/eric/Papers/cs.tau.ac.il/2013 - Joint word2vec Networks for Bilingual Semantic Representations - Wolf et al.pdf:pdf},
journal = {cs.tau.ac.il},
keywords = {continuous skip-gram citation,machine translation,thecat-alg-improvement},
mendeley-tags = {continuous skip-gram citation,machine translation,thecat-alg-improvement},
title = {{Joint word2vec Networks for Bilingual Semantic Representations}},
url = {http://www.cs.tau.ac.il/~nachumd/papers/jw2v.pdf},
year = {2013}
}
