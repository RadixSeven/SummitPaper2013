\documentclass[eric_thesis.tex]{subfiles}
\begin{document}
\frontmatter
\maketitle

\chapter{Abstract}
Vector-based lexical semantics is a powerful technique that still has many 
undiscovered applications. In this thesis I apply a vector-space 
lexical-semantic model newly developed by Mikolov et. al. trained on 
skip-grams to the lexical hypothesis in personality psychology. The method
produces interpretable dimensions that are consistent across several sets of
descriptive personality words. The dimensions include ones for conflict and
positive and negative evaluation. However they are more descriptive of
word usage semantics than of the characteristics of the thing described and 
thus do not include a recognizable component of the 5 factor model in their 
first 14 dimensions. They do include a component that seems to indicate the 
degree to which the word applies to people which could be useful in identifying
personality words in English.


\mainmatter
\chapter{Introduction}

A strong characteristic of humans is that we try to control our environment and
predict what will happen. Prediction is integrated bacon-angel into our 
unconscious processes. Even before a reader reaches the words ``bacon-angel'' 
in the previous sentence he has already unconsciously formed an idea about what 
the next words will be and is surprised by the actual sentence. Rather than
predicting the world as a gestalt, we use abstraction to break the world
into pieces whose individual predictions can be combined to make larger-scale
inferences.

One of the most important abstractions we use is the concept of person.
Even at 3 months old, human infants have begun to use their knowledge of 
people to make inferences about unperceived properties of their surroundings. 
\todo{cite ref Spelke 1994 Cognition ``Initial knowledge: six suggestions''}
The concept of person divides the the world into things that are people and
things that are not. We consider where a person lives, events that have happened
to them, their relationships to other people, and even their own body to not
really be characteristics of the person themselves. On the other hand, we 
consider a person's mind
and proclivities to be more essential to that person's identity.

\section{Personality Models from Vectors}
\label{sec:personalitymodelsfromvectors}
Personality is a set of internal, relatively enduring psychological 
characteristics and processes that predict a person's behavior. Producing good
models of personality is an important endeavor in psychology.

There are many sources for such models. In the first half of the 
20\textsuperscript{th} century, psychologists created theories based on the
professional and life experience of their originators 
\todo{ref cite Monte 1977}. These theories frequently included models of
personality: Freud's id, ego, and superego or Fromm's five character types based 
on individual strategies to cope with alienation.

Various experiments and other observations can also suggest models and theories
themselves. Since the 1980's, many personality psychologists have begun using 
trait models derived from factor analysis of people's usage of language in 
describing others. The most famous of these is the Five Factor Model measured by 
McCrae and Costa's NEO inventory. It measures personality along the five 
dimensions of Openness to experience, Conscientiousness, Extraversion, 
Agreeableness, and Neuroticism (which form the mnemonic OCEAN). Personality 
trait models like the Five Factor Model are used in a wide variety of contexts. 
They are used in dating sites, career counseling, management, clinical 
psychology and school adjustment.

These personality factors come out of turning people's descriptions of others 
into vectors using questionnaires. Each vector dimension corresponds to rating 
the person on one aspect of personality. People's descriptions on hundreds of 
adjectives can be predicted well by their ratings on only the 5 OCEAN 
dimensions. For each adjective, there are 5 values that, when multiplied by the 
5 model dimensions, give the prediction of a person's rating for that dimension. 
These 5 values are a vector. So the 5 factor model turns adjectives into 
vectors. The components of the vectors are the semantic contributions of each of 
the model dimensions to that vector's adjective.

\section{Vectors from Text}

In a completely different field, there is another way of turning words into 
vectors. In the 1990's, techniques like Latent Semantic Analysis (LSA) were 
developed to turn bodies of text into vectors that in some sense approximated 
the meaning of words - and do it with very little \textit{a-priori} knowledge. 
The early techniques only captured a small part of word meaning but even that
small portion greatly enhanced search technology and was sufficient to give
non-native speaker levels of performance in identifying word synonyms. Today, 
the vectors generated by skip-gram models trained on only their own language 
can be used to translate between different languages with reasonable accuracy 
given only a very small set of word correspondences. Such demonstrations imply 
that it is not just the syntactic structure of a language that is being 
captured but its meaning as well.

\section{What Personality from Text?}

Since there is a source for vectors indicating the meaning of different words. 
It seems reasonable that the most important components of the meaning of 
personality words would be the factors that make up personality as described by 
humans. So, I set up an experiment to look at the vectors for personality words 
in a skip-gram model to see what personality factors would be found. On setting 
it up, I believed that it was likely I would uncover the Five Factor Model (or 
one of its competitors) in the data. Though the Five Factor Model did not 
appear, the factors that did were still of interest themselves.

\chapter{Background}

\section{Vector-based lexical semantics}

\subsection{LSA}

Vector-based lexical semantics is the study of algorithms that assign vectors to 
words in a way that reflects the meaning of those words. The first vector-based 
lexical semantic algorithm was LSA (Lexical Semantic Analysis), invented in 
\todo{some-year} by \todo{some-person}. LSA works off of a bag-of-words model of 
source documents. The training documents are turned into a matrix in which each 
row corresponds to a document, each column to one word in the vocabulary, and 
each entry counts the number of occurrences of a particular word in the 
document. It is important to recognize that a document can be a group of words 
of any length. Documents could consist of sentences, paragraphs, web-pages, 20 
word sliding windows, bible chapters, or any other textual unit that is of 
interest in the application. This term-document matrix is then decomposed into 
its principal components and their loadings by SVD (singular value 
decomposition) and the most significant components are chosen. If the resulting 
reduced matrices are multiplied together, it produces a smoothed term-document 
matrix guaranteed to be the closest one can come to the original matrix (in the 
least-squares sense) with the chosen number of factors.

\todo{include section on how to use LSA for document-query similarity and 
word-word similarity}

Any modern application of LSA is usually more complicated. For example, the raw 
counts are usually transformed by the TF/IDF (term-frequency/inverse document 
frequency) transformation where the counts are replaced by the log of the count 
in that document divided by the mean count. \todo{ref and check procedure, esp 
what happens at 0}. This procedure was originally justified on the heuristic 
grounds that it emphasized words that better distinguished between documents. 
\todo{ref} Later, a probabilistic justification was discovered. \todo{ref} There 
are also methods for choosing which words to include in the vocabulary 
\todo{ref}, modifying the words for better retrieval (stemming) \todo{ref}, 
choosing the number of dimensions to keep \todo{ref}, and many other refinements 
to the technique.

LSA came out of the document retrieval field and its first applications were in 
matching query strings to documents. It was very successful in finding documents 
that were semantically appropriate but which contained no words in common with 
the query. For example, with the right training set, "The legislature will meet 
in Columbus on Thursday for a special session." would be a good match for the 
query "Ohio capital" because Ohio and capital are both frequently in the same 
documents with Columbus, legislature, meet, and session.

\subsection{PLSA}

\subsection{LDA}

\subsection{N-word context models}

\subsection{Prototype-based models}

\subsection{Skip-gram Model}

\todo{Write how the model works.}

\section{Applications of Vector-based lexical semantics}

\section{PCA and Factor analysis}

\todo{The different methods of factor analysis amount to different models and
presuppositions about the structure of the problem. For example, seeking
simple structure and then distinguishing the variables with a high loading on the
factor presumes that the variables can be classified by factor. Another way of
looking at this assumption is that the variables come in groups that covary and
each of those groups should correspond to a factor. Then the interesting factors
are those which correspond to groups. Whereas [standard
PCA of z scores] assumes [something here]. Another view: The standard psychological method
takes first n components. Then tries to return the those as much as possible to
something like the identity matrix (which is the way the rotation was before
the PCA). Since standard psychology we know what the variables are (not what
the individuals are) they want to get their factors in terms of variables in
order to interpret them.}

\section{Multidimensional Scaling}


\section{Lexical Hypothesis in Personality Psychology}

As mentioned in Section \ref{sec:personalitymodelsfromvectors} there are many
places to get personality models. One approach is to attempt to extract the 
intuitive personality model held by most people. It can be assumed that this
model is reasonably good because our species, being very social, needs to 
interact with and model others as a condition of survival. 

A good way to try and get at this model is to use language. The lexical 
hypothesis in psychology is:
``those individual differences that are most salient and socially relevant in
people's lives will eventually become encoded into their language'' 
\todo{ref cite Goldberg, 1982, p. 204, ``From Ace to Zombie: Some explorations 
in the language of personality''} This approach was initially proposed by 
Galton in 1884 \todo{cite Galton 1884 p. 181 ``Measurement of Character''
Fortnightly Review} and was pursued by psychologists for the following century
but it really took off after an influential seminar
by Goldberg in 1983. \todo{cite Ian J Deary ``The trait approach to personality'' 
p.90 chapter in ``Cambridge Handbook of Personality Psychology''} 

\subsection{Word lists}

As stated, the lexical hypothesis covers the encoding of personality traits in
all of language. However, as implied by its name, psychologists working from 
the lexical hypothesis have focused on finding personality in the vocabulary
of the language rather than in its grammatical structure. Galton pioneered this
as well, spending time with Roget's Thesaurus and coming back with an estimate
of 1,000 applicable words. \todo{cite Galton 1884 p. 181 
``Measurement of Character'' Fortnightly Review} 

The next notable advance in word lists was by Allport and Odbert \todo{ref cite
Allport and Odbert} who tabulated 
all the trait names in the
Webster's unabridged New International Dictionary. By their count, there were
17,953 terms. They chose words that had the ``capacity ... to distinguish 
the behavior of one human being from that of another.'' Thus, they excluded
words which specified non-distinctive behavior. Because they were
mainly attempting to create a list in which the entries could be rough proxies
for traits, they attempted to reduce redundancy by avoiding nominative and
adverbial forms and including multiple words from clusters where they judged 
that the different forms arose because of arbitrary morphological development 
rather than reflecting semantic distinction. Thus 18K terms, frequently taken to 
be an approximation
of the number of personality terms in the English language is actually an
underestimate.

Allport and Odbert divided their list into 4 categories. The first was the
closest embodiment of the most clearly ``real'' traits of personality. ``They
designate generalized and personalized determining tendencies - consistent and
stable modes of an individual's adjustment to his environment.'' The other three
categories included words that were more temporary, more censorial, and more
metaphorical and remote in their applicability to personality.\footnote{ 
It is interesting that our automatic software includes as some of its most 
important dimensions ones that could be used to categorize words on two of these
axes.
There is a positive-negative component and a ``frequently applies metaphorically''
component. The lack of a component measuring temporariness is not surprising
since the word-list we used excluded words designating temporary states.}

Word lists of the size generated by Allport and Odbert were too large to use 
with statistical techniques before the advent of computers. So, Cattell \todo{
ref cite Cattell 1947 Confirmation and clarification of primary personality
factors} manually reviewed the list and chose 35 trait variables he felt
summarized the words in Allport's first category trait list. This list is
notable because Cattell used it to produce his influential 16 factor model
and an independent analysis by Tupes and Christal in 1961 \todo{cite ref Tupes 
and Christal 1992} is an the first clear recognition of the five factor 
model.

The last of the massive dictionary derived trait-lists was published by W. T. 
Norman in 1967 \todo{cite ref Norman1967}. He started with all 4 categories of
Allport and Odbert's list and added all words that ``pertained in any
manner to attributed of persons or their behavior but which were not included 
in the Allport-Odbert list'' from Webster's Third New International Dictionary
Unabridged (1961). Then, in a first culling, a group of four raters removed 
the most obscure words, 
those that were
nebulous and extremely metaphorical, physical aspects of behavior, movements,
location, or of appearance, grooming and dress, and finally those which are
purely evaluative, or qualifiers of degree. Next he
classified the remaining words into 15 categories roughly indexing their
stability/temporariness, formality, commonness, reference to social constructs
\footnote{Words like adversary, helpmate, outcast were high in this category}
and then the 4 categories from the first, rough cull. He kept the stable words
that did not refer to social constructs as his list of 2800 trait words. His
list did not restrict itself to adjectives.

After Norman, people continued to produce word lists for trait evaluation. But
many of them were much smaller - designed for use in direct personality 
assessment and they are frequently dependent on one of the earlier lists. We
use two such lists from Goldberg in our later analysis. \todo{cite ref Saucier
and Goldberg 1996 (for 435 words) and Goldberg 1990}

\subsection{Factor-analytically derived traits}

The lexical hypothesis is one of the most important hypotheses in personality
modeling today. But to be useful, the information about personality structure
cannot remain diffused throughout lists of personality words but must be 
extracted and condensed. Starting with Cattell in the 1940's \todo{cite cattell
1948 as an example} research psychologists turned to factor analysis to 
concentrate the information about personality contained in language into usable
knowledge.

The general method for producing a list of traits through factor analysis is to
measure many variables which one believes are related to personality. Then,
factor analysis will produce clusters of variables that score high or 
low\footnote{usually a cut-off of 
$\left|v\right| > 0.3$ or $\left|v\right| > 0.4$
is chosen for high/low loadings, which means between 9 and 16 percent of the
variance of that variable is accounted for by that factor.} on
a particular factor. These are then used to help the experimenter interpret
that factor as a trait summarizing an important aspect of personality. 
A large majority of studies on personality models 
refer to the Big Five structure \todo{ref cite Boele De Raad 
``Structural Models of Personality'' 
p.127 chapter in ``Cambridge Handbook of Personality Psychology''} which was
derived from just such a factor-analytical approach to the lexical hypothesis.

\subsection{Biological basis}

Relying only on a lexical approach does not give the whole picture. It is more
descriptive than predictive. It is like recognizing that the sky is blue on a
clear day and gray on an overcast day. It is an accurate assessment but it does
not describe the mechanism that produces blueness and grayness. 
We know that gregariousness and warmth are 
correlated when people describe others and we attribute this to differing
quantities of a trait called extroversion. However, whether this trait of
extroversion comes from genes, from universal actions mothers take with 
infants or from quirks in human social perception is not specified.

One way to address this shortage is to take existing lexically-based models and
attempt to find a basis for the observed regularities. There is continual work
attempting to find a biological or genetic basis for the Five Factor Model.
Recent successes showed that the factors are heritable \todo{ref cite Riemann,
Angleitner and Strelau 1997} and that some genes influence both warmth and 
assertiveness - explaining why both covary as facets of 
extroversion. \todo{cite ref Yamagata,
Suzuki, Ando et. al. 2006 and Pilia, Chen, Scuteri et al. 2006}

Another way to attempt to add biological information to personality traits is
to guide your factor analysis using biological information. Such an approach
was taken by Eysenck who started with biological hypotheses that two very 
important personality traits were extroversion and emotional stability and
then used that to guide his factor analysis which tested how good his theory
was.\todo{cite ref Carducci 2009 The Psychology of Personality Second Edition 
p.284}

\chapter{Related Work}

\section{\modelname{} model}

There has not been a lot of time since the publication of the \modelname{} model. 
However, in that period, researchers have attempted to improve its algorithms
and vector quality, and also created applications, both within and outside 
Natural Language Processing (NLP). It has also seen use as a 
standard reference to which the performance of other algorithms is compared.

\subsection{Algorithm and Vector Improvements}

\citep{Mikolov2013c} tested alternative training procedures. They tested an 
alternative to the hierarchical softmax used in the \modelname{} model called 
negative 
sampling. The also tested a way of giving less weight to frequent words
which speeds up training. The weight adjustment improved the resulting vectors
more than negative sampling. They also demonstrated a simple method
of extracting phrases whose meanings were not simple compositions of their
component part meanings and extended their semantic test suite to use some
of these phrases.

\citep{Faruqui2014} created a method using CCA (Canonical Correlation Analysis) 
to take vectors generated on
monolingual corpora for different languages and combine them to create new sets
of vectors utilizing the semantic information from both. They then compared the
semantic distance mappings thus obtained to several sets of human-labeled 
semantic distances. They also used the analogy tasks from \citep{Mikolov2013a}.
With LSA as the source for the original vectors, they were
able to uniformly improve performance on all measures. Similarly, with a
Recursive Neural Network (RNN) as the source of the vectors, there was less
improvement, but still only a loss on one task. Vectors from \modelname{} 
however had no detectable change in most cases but their performance decreased
on the same task as the RNN\footnote{The authors do not mention having done
multiple test correction, so their significance values may be invalid.}.

\citep{Wolf} also created an extension of \modelname{}\footnote{They use the
Continuous Bag-of-words architecture rather than the Skip-gram variation I
use.} that takes advantage of
bilingual information to to improve vector quality. They believe it works 
because it deals better with polysemy, since it is rare that homographs of 
translations cover the same set of senses. They require a word-aligned 
training corpus and use a vocabulary 
composed of all words in both languages. They first train the model on both
languages as usual. Then, they train the words using the context from the 
matched opposite language word. So, for example, the word 
``ran'' is aligned with the word ``corri\'o'' in the two parallel sentences 
``She ran to the store'' and ``Ella corri\'o' a la tienda''. So the context of 
``ran'' in the second phase would be ``Ella,'' ``a,'' ``la,'' and ``tienda.''
\footnote{Because their Hebrew-English parallel text pairs the 
Hebrew Bible with its King James translation, one of their tests involves 
synonyms taken from Strong's 1890 
concordance. Many people believe that
all the senses of a word are expressions of some ``root meaning'' of that word 
and so
pick and choose glosses from Strong's dictionary to give other nuances they 
believe were held by the 
original Greek or Hebrew that did not make it into English. This makes 
Strong's work an ironically appropriate choice for this study 
since this ``root meaning'' assumption is almost the same as the assumption that 
vector-space language models impose by assuming a single point for each word 
and which the authors are trying to overcome by using multiple languages.}

\citep{Bordes2013} extended the implicit linear operators expressing 
relationships in \citep{Mikolov2013b} and explicitly trained a vector
space representation of RDF-type triples 
$(subject,predicate,object)$ that 
capture the semantics of $predicate$ by attempting to make 
$subject+predicate \approx object$ when the predicate holds and make it
far away otherwise. The resulting model is able to predict links better than
several more expressive models by the same authors on both Freebase and 
WordNet datasets. And even when the link predictions do not contain the best
answer, they reflect common sense.

\subsection{Applications}

\citep{Mikolov2013a} creates a method of generating phrase tables and 
dictionaries from two languages with only a small amount of bilingual data.
They first train the \modelname{} model on two independent monolingual corpora 
and then align the vector spaces with a linear transformation derived from a 
small number of matched word pairs. The resulting translations had the 
correct word within the top 5 candidates (which were usually synonyms) 90\% of 
the time.

\citep{Frome2013} used \modelname{} vectors as the targets in a second round
of training a state-of-the-art image recognition convolutional neural network.
The semantic generalization afforded by the vector space structure allowed them
to predict labels for images where it had never seen an image with that label
at all\footnote{There were 1000 labels in the training set and 20,841 unseen 
labels not included in the training set images.}. Later, almost the same team 
published \citep{Norouzi2013} which 
generalized the method to allow use of any n-way image classifier and to 
remove the requirement that the classifier had to be retrained using the 
new output structure. 

\citep{Osendorfer2013} proposed using \modelname{} vectors in an information
retrieval setting for computing similarity between musical pieces. Looking at 
music as a language, they intend to use the sparse translation technique from
\citep{Mikolov2013a} to translate between normal language and music.

\section{Studies involving both topic models and personality}

To my knowledge, no one has yet attempted to derive the structure of personality
words by looking at the components of a vector-space model. And 
I am certain no one
has attempted it with vectors from the \modelname{} model. 
However, that does not
mean that the subjects of vector-space lexical semantics and personality are
completely disjoint. I located a number of articles where both played a role.
Most were attempts to predict personality or some aspect thereof from some body 
of text like emails or tweets. There was also one paper that used topic
models as an exploratory tool, attempting to discover more about personality.
A final paper, in deriving results about the structure of language,
found three dimensions that were consistent across different
languages and different source corpora. Of these three, at least two and 
possibly all are personality-relevant.

\subsection{Personality predictive models}

The majority of papers looking at both vector-space language models and 
personality are attempts to predict personality or some aspect thereof from a 
body of text.

\todo{cite Shen Brdiczka Liu 2013} build a classifier to predict three levels
(high, medium, and low) of each of the Big Five personality dimensions from
individual emails using hand-selected features. One of the underlying 
probabilistic models they reject is an LDA variant. Their final classifier
achieves around 70\% accuracy.

\todo{cite Hill, Song, Dong 2001} use LSA on a collection of design documents
to produce vectors which they separate into components one coming from 
the author's, speaking personality and the other from the topic of the document.

In \todo{cite Gill and French 2007} the authors attempted to predict personality
using three different vector space lexical semantic models including LSA. These
models were trained on several standard news and academic corpora. They
had emails from students who had taken the Eysenck Personality Questionnaire and
tried to predict their extroversion and neuroticism by looking at the similarity
of their emails to 10 prototypical words ``defining a trait'' from \todo{cite ref 
Goldberg 1992 Development of markers for the Big-Five factor structure} and
by looking at the 7 words that were most frequent in emails from the people who
were most extreme on the given traits. The semantic models failed on both
counts while human raters were able to reliably predict both extroversion and
neuroticism from the same texts. They concluded that topic models do not
extract the right kind of information to classify personality. An external
observer, however, might note their unsophisticated use of pre-made topic
models created in a different domain to examine poorly selected features in 
a very small list of documents as an alternate explanation for their failure.

In \todo{cite Arvidsson Sikstrom and Werbart 2011} they use LSA to show that 
psychotherapy causes changes in a self-report-based personality index based 
on a patient's description of himself, his mother and his father.
\footnote{I was unable to access the full text of this article before I
had to submit the thesis but I still felt that the article was likely 
relevant.}

\todo{cite Pennacchoiotti2011} uses a large number of features including topics
from different LDA models to predict various features of Twitter users including
political orientation, which may be considered a correlate of the openness
personality dimension.

\subsection{Questions in the study of personality}

Besides these predictive studies, one article used vector-space lexical 
semantics as a tool to explore personality. \todo{cite Schwartz2013a} use a 
database of 14.3 million posts from 75,000 
Facebook users who took a Five Factor questionnaire and analyzed their Facebook 
posts to gain insight into gender, age, and personality. One of the features 
they examine is topics from an LDA model. Their personality-sorted results
mainly looked at N-grams (which produced the novel observation that introverts
like Japanese culture). Their insights from the LDA were mainly about lifespan
issues. For example, social
topics increase with age and antisocial topics decrease.

\subsection{Serendipitously discovered personality factors}

Possibly the most relevant work is \todo{cite Samsonovich2010} (which is an
expansion of \todo{cite Samsonovich2007}). In this work the authors are not
intending to discover anything about personality, rather they are investigating 
the semantic structure of language. They
analyze synonyms and antonyms in English, French, and German. In all three
languages, they discovered that the first three principal components could be
summarized as measuring positive/negative (valence), strong/weak 
(dominance,arousal), and open/closed (freedom). They used a similar criterion
to Mikolov for constructing the space in that they minimized a cosine distance
for close words. But because of the presence antonym information, they were
also able to specify that antonyms should have maximum cosine distance.
The resulting vector assignments had 4 obvious and strong principal
components, of which 3 were replicated across languages. Because of the
underlying human-generated synonym-antonym classification, the generated
categories better reflect the way humans divide up the world. Additionally,
the first two categories are similar to several two-factor personality models:
Morality/Dynamism \todo{ref cite Saucier, Georgiades, Tsaousis, and Goldberg 2005}
or Virtue/Dynamism \todo{ref cite De Raad and Barelds 2008}. The authors also
see a similarity between their factors and Communion/Power from the 
interpersonal Circumplex of Leary \todo{ref cite Leary T (1957) 
Interpersonal diagnosis of personality. New York: Ronald Press}. This suggests
the 
interesting possibility that the great cross-cultural similarity in personality
factors comes in part from the fact that humans, the main measuring instrument,
classify \textit{everything} along these two dimensions. It would be a very
interesting follow-up to look at the portion of semantic space generated
from these thesauri inhabited by personality words and see what other 
dimensions are present when the noise of multiple, overlapping topic-areas
is removed.

\todo{Remove the bibliography from the individual sections - it is there right
now so that references work when compiling individually}

\bibliography{Thesis}
\end{document}
