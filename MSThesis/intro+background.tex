\documentclass[eric_thesis.tex]{subfiles}
\begin{document}
\frontmatter
\maketitle

\chapter{Abstract}
Vector-based lexical semantics is a powerful technique that still has many 
undiscovered applications. In this thesis I apply a vector-space 
lexical-semantic model newly developed by Mikolov et. al. trained on 
skip-grams to the lexical hypothesis in personality psychology. The method
produces interpretable dimensions that are consistent across several sets of
descriptive personality words. The dimensions include ones for conflict and
positive and negative evaluation. However they are more descriptive of
word usage semantics than of the characteristics of the thing described and 
thus do not include a recognizable component of the 5 factor model in their 
first 14 dimensions. They do include a component that seems to indicate the 
degree to which the word applies to people which could be useful in identifying
personality words in English.


\mainmatter
\chapter{Introduction}

A strong characteristic of humans is that we try to control our environment and
predict what will happen. Prediction is integrated bacon-angel into our 
unconscious processes. Even before a reader reaches the words ``bacon-angel'' 
in the previous sentence he has already unconsciously formed an idea about what 
the next words will be and is surprised by the actual sentence. Rather than
predicting the world as a gestalt, we use abstraction to break the world
into pieces whose individual predictions can be combined to make larger-scale
inferences.

One of the most important abstractions we use is the concept of person.
Even at 3 months old, human infants have begun to use their knowledge of 
people to make inferences about unperceived properties of their surroundings. 
\todo{cite ref Spelke 1994 Cognition ``Initial knowledge: six suggestions''}
The concept of person divides the the world into things that are people and
things that are not. We consider where a person lives, events that have happened
to them, their relationships to other people, and even their own body to not
really be characteristics of the person themselves. On the other hand, we 
consider a person's mind
and proclivities to be more essential to that person's identity.

\section{Personality Models from Vectors}
\label{sec:personalitymodelsfromvectors}
Personality is a set of internal, relatively enduring psychological 
characteristics and processes that predict a person's behavior. Producing good
models of personality is an important endeavor in psychology.

There are many sources for such models. In the first half of the 
20\textsuperscript{th} century, psychologists created theories based on the
professional and life experience of their originators 
\todo{ref cite Monte 1977}. These theories frequently included models of
personality: Freud's id, ego, and superego or Fromm's five character types based 
on individual strategies to cope with alienation.

Various experiments and other observations can also suggest models and theories
themselves. Since the 1980's, many personality psychologists have begun using 
trait models derived from factor analysis of people's usage of language in 
describing others. The most famous of these is the Five Factor Model measured by 
McCrae and Costa's NEO inventory. It measures personality along the five 
dimensions of Openness to experience, Conscientiousness, Extraversion, 
Agreeableness, and Neuroticism (which form the mnemonic OCEAN). Personality 
trait models like the Five Factor Model are used in a wide variety of contexts. 
They are used in dating sites, career counseling, management, clinical 
psychology and school adjustment.

These personality factors come out of turning people's descriptions of others 
into vectors using questionnaires. Each vector dimension corresponds to rating 
the person on one aspect of personality. People's descriptions on hundreds of 
adjectives can be predicted well by their ratings on only the 5 OCEAN 
dimensions. For each adjective, there are 5 values that, when multiplied by the 
5 model dimensions, give the prediction of a person's rating for that dimension. 
These 5 values are a vector. So the 5 factor model turns adjectives into 
vectors. The components of the vectors are the semantic contributions of each of 
the model dimensions to that vector's adjective.

\section{Vectors from Text}

In a completely different field, there is another way of turning words into 
vectors. In the 1990's, techniques like Latent Semantic Analysis (LSA) were 
developed to turn bodies of text into vectors that in some sense approximated 
the meaning of words - and do it with very little \textit{a-priori} knowledge. 
The early techniques only captured a small part of word meaning but even that
small portion greatly enhanced search technology and was sufficient to give
non-native speaker levels of performance in identifying word synonyms. Today, 
the vectors generated by skip-gram models trained on only their own language 
can be used to translate between different languages with reasonable accuracy 
given only a very small set of word correspondences. Such demonstrations imply 
that it is not just the syntactic structure of a language that is being 
captured but its meaning as well.

\section{What Personality from Text?}

Since there is a source for vectors indicating the meaning of different words. 
It seems reasonable that the most important components of the meaning of 
personality words would be the factors that make up personality as described by 
humans. So, I set up an experiment to look at the vectors for personality words 
in a skip-gram model to see what personality factors would be found. On setting 
it up, I believed that it was likely I would uncover the Five Factor Model (or 
one of its competitors) in the data. Though the Five Factor Model did not 
appear, the factors that did were still of interest themselves.

\chapter{Background}

\section{Vector-based lexical semantics}

\subsection{LSA}

Vector-based lexical semantics is the study of algorithms that assign vectors to 
words in a way that reflects the meaning of those words. The first vector-based 
lexical semantic algorithm was LSA (Lexical Semantic Analysis), invented in 
\todo{some-year} by \todo{some-person}. LSA works off of a bag-of-words model of 
source documents. The training documents are turned into a matrix in which each 
row corresponds to a document, each column to one word in the vocabulary, and 
each entry counts the number of occurrences of a particular word in the 
document. It is important to recognize that a document can be a group of words 
of any length. Documents could consist of sentences, paragraphs, web-pages, 20 
word sliding windows, bible chapters, or any other textual unit that is of 
interest in the application. This term-document matrix is then decomposed into 
its principal components and their loadings by SVD (singular value 
decomposition) and the most significant components are chosen. If the resulting 
reduced matrices are multiplied together, it produces a smoothed term-document 
matrix guaranteed to be the closest one can come to the original matrix (in the 
least-squares sense) with the chosen number of factors.

\todo{include section on how to use LSA for document-query similarity and 
word-word similarity}

Any modern application of LSA is usually more complicated. For example, the raw 
counts are usually transformed by the TF/IDF (term-frequency/inverse document 
frequency) transformation where the counts are replaced by the log of the count 
in that document divided by the mean count. \todo{ref and check procedure, esp 
what happens at 0}. This procedure was originally justified on the heuristic 
grounds that it emphasized words that better distinguished between documents. 
\todo{ref} Later, a probabilistic justification was discovered. \todo{ref} There 
are also methods for choosing which words to include in the vocabulary 
\todo{ref}, modifying the words for better retrieval (stemming) \todo{ref}, 
choosing the number of dimensions to keep \todo{ref}, and many other refinements 
to the technique.

LSA came out of the document retrieval field and its first applications were in 
matching query strings to documents. It was very successful in finding documents 
that were semantically appropriate but which contained no words in common with 
the query. For example, with the right training set, "The legislature will meet 
in Columbus on Thursday for a special session." would be a good match for the 
query "Ohio capital" because Ohio and capital are both frequently in the same 
documents with Columbus, legislature, meet, and session.

\subsection{PLSA}

\subsection{LDA}

\subsection{N-word context models}

\subsection{Prototype-based models}

\subsection{Skip-gram Model}

\section{Applications of Vector-based lexical semantics}


\section{Lexical Hypothesis in Personality Psychology}

As mentioned in Section \ref{sec:personalitymodelsfromvectors} there are many
places to get personality models. One approach is to attempt to extract the 
intuitive personality model held by most people. It can be assumed that this
model is reasonably good because our species, being very social, needs to 
interact with and model others as a condition of survival. 

A good way to try and get at this model is to use language. The lexical 
hypothesis in psychology is:
``those individual differences that are most salient and socially relevant in
people's lives will eventually become encoded into their language''
\todo{ref cite Goldberg, 1982, p. 204, ``From Ace to Zombie: Some explorations 
in the language of personality''} This approach was initially proposed by 
Galton in 1884 \todo{cite Galton 1884 p. 181 ``Measurement of Character''
Fortnightly Review} but didn't really take off until an influential seminar
by Goldberg in 1983 \todo{cite Ian J Deary ``The trait approach to personality'' 
p.90 chapter in ``Cambridge Handbook of Personality Psychology''

\subsection{Factor-analytically derived traits}

The lexical hypothesis is one of the most important hypotheses in personality
modeling today. The Big Five personality structure derives from an application
of the lexical hypothesis and a large majority of studies on personality models 
refer to the Big Five structure \todo{ref cite Boele De Raad 
``Structural Models of Personality'' 
p.127 chapter in ``Cambridge Handbook of Personality Psychology''.} 

\subsection{Biological basis}

\todo{traits without biological basis don't tell the whole picture and it is
not entirely clear where the boundaries should be}

\section{Part-of-speech tagging}

\section{Multidimensional Scaling}

\section{PCA and Factor analysis}

\chapter{Related Work}

\section{Skip gram model}

\section{Finding personality dimensions}

\end{document}
