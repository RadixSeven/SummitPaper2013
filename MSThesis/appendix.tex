\appendix
\chapter{Scripts used in preprocessing}
\section{Text of filter\_angle\_tags.pl}
\label{app:filterangletags}
\lstset{language=Perl}
\begin{lstlisting}
#!/usr/bin/perl
use strict;
use warnings;

#######################
# Usage: filter_angle_tags.pl < source > dest
# or:    filter_angle_tags.pl source1 source2 > filtered
#
# Remove stray html, urls and stock symbols to prepare
# for tagging

my $has_bracket_pair = qr!<[A-z/][^<>]*>!;
my $bracket_pair_split = qr!($has_bracket_pair)!;
my $html_string = q(<span |<strong>|</strong>|).
    q(<em>|</em>|<[bauipq]>|</[pqbaui]>|<\s*br\s*/?>|).
    q(<\s*/br\s*>|<p\s*/?>|</?h.>|</?blockquote|).
    q(</?code>|<font[ >]|</font|</?acronym|</?strike|).
    q(</?cite|</?q |</>|</?abbr |</?del |<javascript|).
    q(</?itals>|</?script|</?center>|<SPEAKER );
my $html_regex = qr(${html_string});

sub keep_segment{
    my ($seg) = @_;
    return !($seg =~ m!$has_bracket_pair!) or !(
        $seg =~ m(http://) or
        $seg =~ m(href) or
        $seg =~ m(\.com\W|\.gov\W) or
        $seg =~ m(${html_regex}) or
        $seg =~ m(<[A-z0-9]{0,9}[.=/][A-Z0-9]{0,6}>) or
        $seg =~ m(<CF7[01]>) or
        $seg =~ m(<ID:[A-z0-9]+>)
        );
}

while(<>){
    chomp;
    if (m/$has_bracket_pair/) {
        my @segments = split($bracket_pair_split, $_);
        @segments = grep( &keep_segment($_), 
                          @segments);
        print join(" ", @segments),"\n" 
            if (@segments);
    }else{
        print "$_\n";
    }

}
\end{lstlisting}

\section{Text of tag\_corpus.sh}
\label{app:tagcorpus}
\lstset{language=sh}
\begin{lstlisting}
#!/bin/bash

############
# Script I used to tag the WMT11 corpus - it needed to
# be split because the TreeTagger tokenizer reads
# everything into memory.

# The data file for the cleaned data
WMT11=/mnt/linux_data/wmt11_no_angle_tags.txt

# The prefix used for split files
SPLIT_FILENAME=/mnt/linux_data/wmt11_no_angle_split

# Data file for the tagged data
TAGGED_WMT11=/mnt/linux_data/wmt11_tagged.txt

# Put treetagger on path
PATH="$PATH:/home/eric/SW/TreeTagger/cmd:"\
"/home/eric/SW/TreeTagger/bin:."

# Split into 10,000,000 lines per file (about 1.5GiB
# per)
if [ -f ${SPLIT_FILENAME}aa ]; then 
    echo "Not splitting again. Split already exists"
else
    split -l10000000 "$WMT11" "$SPLIT_FILENAME"
fi

# Tag the split files
if [ -f "$TAGGED_WMT11" ]; then 
    echo "Not tagging again. Tagged file "\
         "already exists"
else
    for i in ${SPLIT_FILENAME}*; do
        echo "Tagging $i"
        tree-tagger-english-utf8 $i | \
            reassemble_tags.pl >> "$TAGGED_WMT11"
    done
fi
\end{lstlisting}

\section{Text of tag\_word\_list.pl}
\label{app:tagwordlist}
\lstset{language=Perl}
\begin{lstlisting}
#!/usr/bin/perl
use strict;
use warnings;
use HotKey;
use URI::Escape;
use File::Fetch;

#
# Reads a word-list from a file (DON'T USE STDIN, it
# mistakes it for keyboard input). For each word,
# displays the word-net entry web-page in firefox and
# then asks if it is a valid as a personality
# adjective. Next it asks if it is valid as a
# personality non-adjective. If it was a valid
# adjective, it writes word_jj to stdout. If it was a
# valid non-adjective, it writes word to stdout.  If
# you type q at any prompt, it exits, writing "THERE
# WAS A PROBLEM" to stdout.
#
# Using the assumption that every word in the file is a
# personality word, does the disambiguation itself if
# the word has only one part of speech. (However, this
# code depends on the format of the html returned and
# so is fragile.)
#
# Prompts are written to stderr (which is fine on
# Linux, but probably a mess under Windows)

sub getynq {
    my $key = readkey();
    while($key =~ m/^[^ynq]$/){
        print STDERR "\nPlease enter y,n, or q.";
        $key = readkey();
    }
    return $key;
}

WORD: while(<>){
    chomp;

    my $query_uri =
        'http://wordnetweb.princeton.edu/perl/webwn?s='.
        uri_escape($_).
        '&sub=Search+WordNet&o2=&o0=1'.
        '&o7=&o5=&o1=1&o6=&o4=&o3=&h=';
    # Try automatic determination - if there is only
    # one level three header (used to contain the part
    # of speech) then if it is "Adjective" the word is
    # a personality adjective. Otherwise, the word is a
    # personality non-adjective. Ignore verb and adverb
    # meanings because I am looking for words about
    # personal qualities. I want "The brave man" or "he
    # had courage" and not "He will brave the elements"
    # or "Home of the brave".
    #
    # With the 2797 word list, I have decided to accept
    # words that are categories - like "the home of the
    # brave."
    my $contents;
    my $fetcher = File::Fetch->new(uri => $query_uri);
    $fetcher->fetch(to => \$contents);

    my @level_3_headers = ($contents =~
                         m:<h3>[^<]+</h3>:gi);

    @level_3_headers = # Remove verbs
        grep{!m:<h3>Verb</h3>:} @level_3_headers; 
    @level_3_headers = # Remove adverbs
        grep{!m:<h3>Adverb</h3>:} @level_3_headers; 

    my $autotag_successful = 0;
    if (@level_3_headers == 1){
        # If the only part of speech is adjective tag
        # as such, otherwise if the only part of speech
        # is noun, tag as non-adjective. If the part of
        # speech is something else (an unknown part of
        # speech or an error message) don't tag and
        # leave it to the humans.
        if($level_3_headers[0] =~ 
              m:<h3>Adjective</h3>:i){
            print STDOUT "${_}_jj\n";
            $autotag_successful = 1;
        }elsif($level_3_headers[0] =~ 
              m:<h3>Noun</h3>:i){
            print STDOUT "${_}\n";
            $autotag_successful = 1;
        }else{
            $autotag_successful = 0;
        }
    }
    if ($autotag_successful){
        # We've tagged the word. Read the next word
        print STDERR "Automatically ",
                     "determined ${_}.\n";
        next WORD;
    }


    # Manual determination - open firefox (ignoring
    # error messages) and ask the user

    system("firefox \"".
           "$query_uri\" > /dev/null 2>&1 &");

    my $had_to_do_with_personality=0;
    print STDERR "\nIs $_ a valid personality ".
        "adjective? (yes/no/quit)";
    my $key = getynq();
    if($key =~ m/y/i){
        print STDOUT "${_}_jj\n";
        $had_to_do_with_personality = 1;
    }elsif($key =~ m/q/i){
        print STDOUT "THERE WAS A PROBLEM\n";
        exit(-1);
    }

    print STDERR "\nIs $_ a valid personality ",
        "quality or type of person ('His ",
        "characteristic $_' or 'The $_ is ",
        "characterized ",
        "by $_-ness')? Only accept for type of ",
        "person if ",
        "the word is not an adjective or a much more ",
        "common usage than the adjective. (braggart, ",
        "for ",
        "example, has an archaic adjectival use but ",
        "is ",
        "much more commonly used as a ",
        "category)(yes/no/quit)"; 
    $key = getynq();
    if($key =~ m/y/i){ 
        print STDOUT "${_}\n";
        $had_to_do_with_personality = 1; 
    }elsif($key =~ m/q/i){ 
        print STDOUT "THERE WAS A PROBLEM\n";
        exit(-1); 
    }

    unless($had_to_do_with_personality){
        print STDERR "\n$_ must have something to ",
             "do with personality. You can't ",
             "answer n on both questions.\n";
        print STDOUT "THERE WAS A PROBLEM\n";
        exit(-1);
    }
} 
\end{lstlisting}


\section{Text of reassemble\_tags.pl}
\label{app:reassembletags}
\lstset{language=Perl}
\begin{lstlisting}
#!/usr/bin/perl
use strict;
use warnings;

################
# Usage: reassemble_tags.pl < tagged.txt
#
# Takes tagged text from treetagger and reassembles it
# into 1 sentence per line (multiple sentence ending
# tags are kept on the same line)

# Tracks whether the previous tag was a sentence-ender
# in order to tell where to end the lines.
my $last_tag_was_sentence=(1==0);

# Combine words and sentences
while(<>){
    my ($word, $tag) = split('\t');
    if(defined($tag)){
        # End the line if transitioning from "SENT" to
        # another tag
        my $tag_is_sentence = $tag eq "SENT";
        if ($last_tag_was_sentence && !$tag_is_sentence){
            print "\n";
        }

        # Output the word with its tag
        if(defined($word)){
            print "${word}_${tag} ";
        }else{
            print "${word}_unknown_tag ";
        }

        # Update last tag state variable
        $last_tag_was_sentence = $tag_is_sentence;
    }
}

# If the last tag in the file was a sentence tag,
# output a newline
if ($last_tag_was_sentence){
    print "\n";
}
\end{lstlisting}

\chapter{Scripts used in analysis}

\section{extract\_vectors.py}
\label{app:extractvectors}
\lstset{language=Python}
\begin{lstlisting}
#!/usr/bin/python
from gensim.models import word2vec
import logging
import os
import csv
import sys


# Check command line arguments and print usage if
# wrong number of arguments
if len(sys.argv) != 3:
    import textwrap
    sys.stderr.write(
        "Usage: %s list_of_terms.txt "+
        "model_file.model\n" % sys.argv[0])
    sys.stderr.write("\n")
    sys.stderr.write(
        textwrap.fill(
            "Each line in list_of_terms.txt is "+
            "treated as a term. For each term, if a "+
            "corresponding term exists in the "+
            "word2vec model (generated by "+
            "word2vec.save) then that term and the "+
            "corresponding vector are printed to "+
            "stdout as a csv file"))
    sys.exit(-1)

# Command line arguments
term_list_filename = sys.argv[1]
input_model_filename = sys.argv[2]

# Set up logging
logging.basicConfig(
    format='%(asctime)s : %(levelname)s : %(message)s', 
    level=logging.INFO)

# Load the model
model = word2vec.Word2Vec.load(input_model_filename)
logging.info("Done loading input model")

# Loop through the terms, outputting appropriate
# vectors
out = csv.writer(sys.stdout)
with open(term_list_filename) as terms:
    for term in terms:
        try:
            term = term.rstrip() # Remove line-ending
            v = model.vocab[term]
            vector = model.syn0[v.index]
            vector_str = ["%.19g" % n for n in vector]
            out.writerow([term] + vector_str)
        except KeyError:
            logging.info('Skipped missing term "%s"' % 
                         term)
\end{lstlisting}


\section{Elbow point algorithms}
\label{app:elbow_point_algorithms}
\lstset{language=Matlab}

\subsection{elbow\_point.m}

\begin{lstlisting}
 function [elbow_index,best_estimate] = ...
      elbow_point(lambdas)
% Returns the index at which the slope of the lambdas
% changes
%
% In using a scree plot to discover the number of 
% principal components, a frequent method is to choose
% the "elbow", the inflection point of the curve. One
% way of formalizing this is approximating the scree 
% plot with two lines. One goes from the  beginning to
% the elbow point and the other to the end. Whichever
% is closest is the best elbow point 
%
% Exhaustive search for the best approximation of
% the function composed of the ordered pairs
% i,lambdas(i) composed of the lines
% (1,lambdas(1))...(i,lambdas(i)) and
% ((i,lambdas(i)...(end, lambdas(end))
%
% Mean absolute value of the difference is used as
% the measure of goodness of fit
%
% best_estimate - the linear fit to the lambdas
% implied by elbow_index

if size(lambdas,1) > 1
    lambdas = lambdas';
end
best = 1;
best_error = inf;
best_estimate = [];
for cur=1:length(lambdas)
    cur_estimate_a = linspace(lambdas(1), ...
         lambdas(cur), cur);
    cur_estimate_b = linspace(lambdas(cur), ...
         lambdas(end), length(lambdas)-cur+1);
    cur_estimate = [cur_estimate_a(1:end-1), ...
         cur_estimate_b];
    cur_error=mean(abs(cur_estimate - lambdas));
    if cur_error < best_error
        best_error = cur_error;
        best = cur;
        best_estimate = cur_estimate;
    end
end

elbow_index = best;

end
\end{lstlisting}

\subsection{flex\_end\_elbow\_point.m}
\begin{lstlisting}
 function [elbow_index,best_estimate] = ...
      flex_end_elbow_point(lambdas)
% Returns the index at which the slope of the lambdas
% changes
%
% In using a scree plot to discover the number of
% principal components, a frequent method is to
% choose the "elbow", the inflection point of the
% curve. One way of formalizing this is approximating
% the scree plot with two lines. One goes from the
% beginning to the elbow point and the other to the
% end. Whichever is closest is the best elbow point.
%
% This code is similar to elbow_point except that the
% height at the ends is adjustable. This frequently
% leads to a more conservative estimate of when the
% slope changes and a closer approximation of the
% lambdas. I think it is more like what humans do than
% offset_elbow_point.
%
% Exhaustive search for the best approximation of the
% function composed of the ordered pairs i,lambdas(i)
% composed of the lines (1,height_1)...(i,lambdas(i))
% and ((i,lambdas(i)...(end, height_end)
%
% Mean squared difference is used as the measure of
% goodness of fit. (It looks more like what a human
% would do than the mean abs used in other
% elbow_point versions.)
%
% best_estimate - the linear fit to the lambdas
% implied by elbow_index

if size(lambdas,1) > 1
    lambdas = lambdas';
end
best = 1;
best_error = inf;
best_estimate = [];
for cur=1:length(lambdas)
    % Since the two lines are independent, we can
    % minimize them in sequence. I minimize the error
    % on the first line, holding the second constant
    % then minimize the second line holding the first
    % at the optimum found in the first pass
    [h1, ~]=fminbnd(@(h) linpredict([h,lambdas(end)]),...
         lambdas(cur), max(lambdas));
    [~, cur_error]=fminbnd(@(h) linpredict([h1,h]),...
         -max(lambdas), lambdas(cur));
    if cur_error < best_error
        best_error = cur_error;
        best = cur;
        best_estimate = cur_estimate;
    end
end

elbow_index = best;

    % Calculate the two-line fit with the elbow_index
    % at cur and the heights at the beginning and end
    % set to heights(1) and heights(2) Also sets
    % cur_estimate and uses lambdas
    function err=linpredict(heights)
        cur_estimate_a = linspace(heights(1),...
             lambdas(cur), cur);
        cur_estimate_b = linspace(lambdas(cur), ...
             heights(2), length(lambdas)-cur+1);
        cur_estimate = [cur_estimate_a(1:end-1),...
             cur_estimate_b];
        err=mean(abs(cur_estimate - lambdas).^2);
    end

end
\end{lstlisting}

\subsection{log\_scree\_elbow.m}
\begin{lstlisting}
function [elbow_index,best_x1, best_x2, predicted] = ...
     log_scree_elbow( lambdas, min_dist )
% Looks for the line in the log(lambdas) that has the
% most inliers, then counts the first outlier as the
% elbow
%
% Scree plots of random stuff look like exponential
% curves. Thus, they are flat on a log scale. The
% dimensions carrying information either have a
% different slope or aren't exponential at all. Thus,
% they will be outliers. On a log plot, the random
% stuff looks like a straight line. (However, if
% the system is rank deficient, the last few principal
% components will sharply trail off and not look like
% a straight line.)
% 
% Let lambdas be the 'latent' return from princomp.
% 
% I fit an exponential to the end-points of all
% sufficiently large sub-intervals of the lambda
% values. I choose the interval whose error looks the
% most like Gaussian noise (that is, has the lowest
% test statistic for a Jarque-Bera). Then I fit an
% exponential to all those points and calculate the
% standard deviation of the errors. Finally, the
% first point with an error less than 3 standard
% deviations, I count as the first random point. All
% points before that are outliers and I count the
% last of the outlier points as my elbow index.
%
% lambdas - a vector of strictly postitive scalars
%     sorted in decreasing order. The 'latent'
%     return from princomp
%
% min_dist - the minimum length interval to be
%     considered as a potential "all-random" interval.
%     The smaller this is, the less power the
%     normality test has. However, it cannot be larger
%     than the number of random values that are
%     decreasing exponentially. The default is
%     length(lambdas)/4

if any(lambdas <= 0)
    error('log_scree_elbow:pos_lambda',...
        ['All lambda values must be strictly '...
        'positive in log_scree_elbow']);
end

if ~exist('min_dist','var')
    min_dist = ceil(length(lambdas)/4);
end

if min_dist < 2
    error('log_scree_elbow:min_dist_too_small',...
        ['min_dist parameter to log_scree_elbow '...
        'must be at least 2.']);
end
        

if length(lambdas)-min_dist < 20 % This is a fudge
                  % factor, but if it is less than 
                  % 20, there are way too few points
    error('log_scree_elbow:not_enough_points',...
        ['There must be at least 20 more lambda '...
        'values than min distance']);
end

if size(lambdas, 1) > 1
    lambdas = lambdas';
end

% Loop two indices x1 and x2 through the lambdas,
% fitting an exponential and finding outliers
n=length(lambdas);
best_x1 = 1;
best_x2 = 1+min_dist;
best_normality_test_stat = inf;
warning('off','stats:jbtest:PTooBig');
warning('off','stats:jbtest:PTooSmall');
for x1=1:n
    y1=lambdas(x1);
    for x2=(x1+min_dist+1):n
        % Fit an exponential to these two points
        y2=lambdas(x2);
        b=(-log(y1)+log(y2))/(-x1+x2);
        a=y1/exp(b*x1);

        % Calculate the errors for that exponential
        predicted=a.*exp(b.*(1:n));
        errors = lambdas-predicted;

        % Calculate the degree to which the errors in
        % that interval match the normal distribution.
        % Ideally, I'd use the Anderson-Darling test
        % statistic because I am particularly
        % interested in excluding outliers from the
        % interval and it is more sensitive to them.
        % However, it is not available in R2012, so
        % I will use the Jauques-Berra test
        is_inlier = (1:n > x1) & (1:n < x2);
        [~,~,jbtest_stat]=jbtest(errors(is_inlier));
        inlier_std = std(errors(is_inlier));
        if isnan(inlier_std)
            fprintf('Nan');
        end
        % If this section looks more normal than any
        % others, call it the best
        if jbtest_stat < best_normality_test_stat
            best_normality_test_stat = jbtest_stat;
            best_x1 = x1;
            best_x2 = x2;
        end
    end
end
warning('on','stats:jbtest:PTooSmall');
warning('on','stats:jbtest:PTooBig');


% Do a least-squares fit on all the inliers
x1 = best_x1;
x2 = best_x2;
y1=lambdas(x1);
y2=lambdas(x2);
b=(-log(y1)+log(y2))/(-x1+x2);
a=y1/exp(b*x1);
is_inlier = (1:n > x1) & (1:n < x2);

xdata = find(is_inlier);
ydata = lambdas(is_inlier);
inlier_params = nlinfit(xdata, ydata, @expfun, [a,b]);

% Calculate the errors for that exponential
predicted=inlier_params(1).*exp(inlier_params(2).*(1:n));
errors = abs(lambdas-predicted);

% Calculate the standard deviation on the points in
% the interval between the two best points
between_std = std(errors(best_x1+1:best_x2-1));

% Outliers are those that are more than 3 std away
is_inlier = errors <= 3*between_std;

% The outlier before the first inlier is our elbow
elbow_index = find(is_inlier,1,'first');
if elbow_index > 1
    elbow_index = elbow_index - 1;
end

% 
% Helper function for fitting
    function predicted = expfun(params, x)
        A = params(1);
        B = params(2);
        predicted = A .* exp(B .* x);
    end

end
\end{lstlisting}

\subsection{offset\_elbow\_point.m}
\begin{lstlisting}
function [elbow_index,best_estimate] = ...
     offset_elbow_point(lambdas)
% Returns the index at which the slope of the lambdas 
% changes
%
% In using a scree plot to discover the number of 
% principal components, a frequent method is to choose
% the "elbow", the inflection point of the curve. One
% way of formalizing this is approximating the scree 
% plot with two lines. One goes from the beginning to
% the elbow point and the other to the end. Whichever
% is closest is the best elbow point.
%
% This is similar to elbow_point except that the
% height at the joining of the two lines is not one
% of the lambdas. This frequently leads to a more
% conservative estimate of when the slope changes and
% a closer approximation of the lambdas.
%
% Exhaustive search for the best approximation of the
% function composed of the ordered pairs i,lambdas(i) 
% composed of the lines (1,lambdas(1))...(i,height)
% and ((i,height...(end, lambdas(end))
%
% Mean absolute value of the difference is used as
% the measure of goodness of fit
%
% best_estimate - the linear fit to the lambdas
%     implied by elbow_index

if size(lambdas,1) > 1
    lambdas = lambdas';
end
best = 1;
best_error = inf;
best_estimate = [];
for cur=1:length(lambdas)
    [~, cur_error]=fminbnd(@linpredict, min(lambdas),...
         max(lambdas));
    if cur_error < best_error
        best_error = cur_error;
        best = cur;
        best_estimate = cur_estimate;
    end
end

elbow_index = best;

    % Calculate the two-line fit with the elbow_index
    % at cur and the height at the elbow set to
    % height. Also sets cur_estimate and uses lambdas
    function err=linpredict(height)
        cur_estimate_a = linspace(lambdas(1), ...
            height, cur);
        cur_estimate_b = linspace(height, ...
            lambdas(end), length(lambdas)-cur+1);
        cur_estimate = [cur_estimate_a(1:end-1), ...
            cur_estimate_b];
        err=mean(abs(cur_estimate - lambdas));
    end

end
\end{lstlisting}

\subsection{scree\_elbow\_using\_robust\_fit.m}
\begin{lstlisting}
function [elbow_index,best_x1, best_x2, ...
     minified_ranks, predicted, ranks] = ...
     scree_elbow_using_robust_fit( lambdas, ...
          error_prctile )
% Looks for the line in the log(lambdas) that has the
% most inliers, then counts the first outlier as the
% elbow
%
% Scree plots of random stuff look like exponential
% curves. Thus, they are flat on a log scale. The
% dimensions carrying information either have a
% different slope or aren't exponential at all. Thus,
% they will be outliers. On a log plot, the random
% stuff looks like a straight line. (However, if the
% system is rank deficient, the last few principal
% components will sharply trail off and not look like
% a straight line.)
% 
% Let lambdas be the 'latent' return from princomp.
% Treat error_prctile like it was a fraction.
% 
% I robustly fit an exponential to the whole curve.
% Then I choose error_prctile of the best errors and
% robustly fit the exponential again to the interval
% containing those points. Finally, I choose the
% interval containing error_prctile of the best points
% again and use that interval as the interval of
% random points. The point before the first point in
% the interval is the elbow index - the first
% non-random lambda.
%
%
%
% INPUT:
%
% lambdas - a vector of strictly postitive scalars 
%     sorted in decreasing order. The 'latent' return
%     from princomp
%
% error_prctile - a scalar from 0..100. The points
%     with an error less than error prctile are used
%     to create the region from which the exponential
%     fit is calculated. Default is 25, so the top 25%
%     of the errors are used. The smaller 
%     error_prctile the smaller the sample used to 
%     estimate the equation governing noise lambdas.
%     So, you'd like error_prctile to be large. 
%     However, if error_prctile is too large then the
%     size of the non-random interval will be
%     overestimated.
%
%
%
% OUTPUT:
%
% elbow_index - the index of the last of the 
%     non-random lambdas
%
% best_x1 - the first index of the random lambda 
%     interval
%
% best_x2 - the last index of the random lambda 
%     interval
%
% minified_ranks - minified_ranks(i)=min(ranks(1:i)).
%     100*minified_ranks(i)/max(ranks(i)) can be
%     thought of a number p such that if 
%     error_prctile > p, then i will be best_x1. If
%     error_prctile were not used twice, this would be
%     the definition. However, there is some minor
%     interaction with error_prctile that makes this
%     only approximate.
%
% predicted - the exponential function that was
%     fitted to the random values
%
% ranks - rank(i) is the rank of the error when
%     estimating lambdas(i) with predicted(i). The
%     smallest errors have the smallest ranks.
%

if any(lambdas <= 0)
    error('scree_elbow_using_robust_fit:pos_lambda',...
        ['All lambda values must be strictly ' ...
        'positive in scree_elbow_using_robust_fit']);
end

if ~exist('error_prctile','var')
    error_prctile = 25;
end

num_points = floor(length(lambdas)*error_prctile/100);

if num_points < 2
    error(['scree_elbow_using_robust_fit:'...
        'num_points_too_small'],...
        ['When error_prctile points are selected '...
        'from lambdas in '...
        'scree_elbow_using_robust_fit there must '...
        'be at least 2.']);
end

if size(lambdas, 1) > 1
    lambdas = lambdas';
end


% Find a starting point for an exponential fit to
% the whole line. Here I do a linear fit to the
% logarithm.
n=length(lambdas);
warning('off','stats:statrobustfit:IterationLimit');
robust_params = robustfit(1:n, log(lambdas));
warning('on','stats:statrobustfit:IterationLimit');

% Next, I use the linear robust fit as the starting
% point to do a nonlinear robust fit on the whole
% line using an exponential function. This should
% be the same, except for the errors being of a more
% appropriate distribution. In particular, I hope that
% the errors in the exponential section are normal
expfun=@(param,x) param(1).*exp(param(2).*x);
fitopts=statset('Robust','on','MaxIter',5000);
starting_pt = [exp(robust_params(1)),...
    robust_params(2)]; % exp(mx+b)=exp(b)*exp(mx)
robust_params=nlinfit(1:n,lambdas(1:n),expfun,...
    starting_pt, fitopts);

% Next, I calculate the inliers. This will help take
% care of the fact that the m-estimators used in
% Matlab's robust fitting still have a zero
% breakdown point.
%
% I find the points with the best error. Then the
% in-lying region is the smallest region that contains
% those points. error_prctile is the fraction of the
% points that are used.
predicted = expfun(robust_params, 1:n);
errors = abs(lambdas - predicted);
max_error = prctile(errors, error_prctile);
best_x1 = find(errors <= max_error, 1, 'first');
best_x2 = find(errors <= max_error, 1, 'last');

% Repeat the estimation in untransformed space using
% only the in-lying points. 
starting_pt = [robust_params(1),robust_params(2)];
robust_params=nlinfit(best_x1:best_x2,...
    lambdas(best_x1:best_x2),expfun,...
    starting_pt, fitopts);
predicted = expfun(robust_params, 1:n);

% Find the in-lying region for the new points. This
% will be the region of random points and the first
% point before it will be our elbow point
errors = abs(lambdas - predicted);
max_error = prctile(errors, error_prctile);
best_x1 = find(errors <= max_error, 1, 'first');
best_x2 = find(errors <= max_error, 1, 'last');

ranks = tiedrank(errors);

% Create a diagnostic based on the ranks of the
% errors. If you divide minified_ranks by 
% length(lambda), you will get the percentile of the
% population you would have needed to choose in order
% for that to be the first in-lying point
minified_ranks = ranks;
for i=2:length(ranks)
    minified_ranks(i)=min(minified_ranks(i-1), ...
        minified_ranks(i)); 
end


if best_x1 > 1
    elbow_index = best_x1 - 1;
else
    elbow_index = 1;
end

end
\end{lstlisting}

\chapter{Ranked word lists}
\label{app:rankedwordlists}
For each list of word vectors, the derived principal components can be used to 
give a numerical score to each word in the list. The following sections list
the most important components derived from each method of analyzing each list.

\section{101 word list}
\subsection{Unnormalized PCA}
\label{app:rankedwordlists:101words:unnormalized}
\input{results_tables/100words-adj-800dim-lowercase-wmt-model-original-appendix-table.tex}
\subsection{Normalized PCA}
\label{app:rankedwordlists:101words:normalized}
\input{results_tables/100words-adj-800dim-lowercase-wmt-model-zscore-transformed-appendix-table.tex}
\subsection{MDS}
\label{app:rankedwordlists:101words:mds}
\input{results_tables/100words-adj-800dim-lowercase-wmt-model-mds-transformed-appendix-table.tex}

\section{438 word list}
\subsection{Unnormalized PCA}
\label{app:rankedwordlists:438words:unnormalized}
\input{results_tables/439words-adj-800dim-lowercase-wmt-model-original-appendix-table.tex}
\subsection{Normalized PCA}
\label{app:rankedwordlists:438words:normalized}
\input{results_tables/439words-adj-800dim-lowercase-wmt-model-zscore-transformed-appendix-table.tex}
\subsection{MDS}
\label{app:rankedwordlists:438words:mds}
\input{results_tables/439words-adj-800dim-lowercase-wmt-model-mds-transformed-appendix-table.tex}

\section{2797 word list}
\subsection{Unnormalized PCA}
\label{app:rankedwordlists:2797words:unnormalized}
\input{results_tables/2797words-adj-800dim-lowercase-wmt-model-original-appendix-table.tex}
\subsection{Normalized PCA}
\label{app:rankedwordlists:2797words:normalized}
\input{results_tables/2797words-adj-800dim-lowercase-wmt-model-zscore-transformed-appendix-table.tex}
\subsection{MDS}
\label{app:rankedwordlists:2797words:mds}
\input{results_tables/2797words-adj-800dim-lowercase-wmt-model-mds-transformed-appendix-table.tex}

\section{Combined 101 and 438 word list}
\label{app:rankedwordlists:438and101words}
\subsection{Unnormalized PCA}
\label{app:rankedwordlists:438and101words:unnormalized}
\input{results_tables/100and439words-adj-800dim-lowercase-wmt-model-original-appendix-table.tex}
\subsection{Normalized PCA}
\label{app:rankedwordlists:438and101words:normalized}
\input{results_tables/100and439words-adj-800dim-lowercase-wmt-model-zscore-transformed-appendix-table.tex}
\subsection{MDS}
\label{app:rankedwordlists:438and101words:mds}
\input{results_tables/100and439words-adj-800dim-lowercase-wmt-model-mds-transformed-appendix-table.tex}

\section{Combined 101 and 438 and 2797 word list}
\label{app:rankedwordlists:2797and438and101words}
\subsection{Unnormalized PCA}
\label{app:rankedwordlists:2797and438and101words:unnormalized}
\input{results_tables/100and439and2797words-adj-800dim-lowercase-wmt-model-original-appendix-table.tex}
\subsection{Normalized PCA}
\label{app:rankedwordlists:2797and438and101words:normalized}
\input{results_tables/100and439and2797words-adj-800dim-lowercase-wmt-model-zscore-transformed-appendix-table.tex}
\subsection{MDS}
\label{app:rankedwordlists:2797and438and101words:mds}
\input{results_tables/100and439and2797words-adj-800dim-lowercase-wmt-model-mds-transformed-appendix-table.tex}

