\documentclass[10pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\author{Eric Moyer, Michael Raymer}
\title{Improving local deconvolution using summit-focused starting points}
\begin{document}

\maketitle

\begin{abstract}
There will be an abstract here
\end{abstract}

\section{Introduction}
There will be an introduction here
\section{Previous work on starting points}
Most nonlinear optimization methods work by using a current model and improving the model by making small changes.
The optimization stops when no more small changes will improve the model. Because the problems being solved in
NMR curve fitting are not convex, there are many local optima where small changes will not reduce the error but 
large changes can still bring about an improvement. If the algorithm finds one of these before it finds the best
model, it will halt and return a sub-optimum model. The closer the initial model is to the global optimum, 
the less likely the optimization algorithm is to run into an inferior local optimum. Further, if good constraints
are chosen, many local optima can be eliminated from the search \textit{a-priori}, increasing the likelihood
of finding the best model first. Thus, deconvolution performance can be greatly improved by choosing a model 
closer to the global optimum and constraining the search to eliminate local optima.

Surprisingly, many authors do not mention their starting points \cite{Martin1994,Romano2002,Vanhamme2000}. 
Gipson\cite{Gipson2006} makes his fitting convex by using binning to avoid peak shift and not altering peak
width.

\subsection{Fitting using a metabolite database}

Most of the rest of the authors doing curve fitting start from a database of known metabolites and
rely on this for their starting points. Poullet \textit{et. al.} start with all the metabolites unshifted at 
some low initial intensity.\cite{Poullet2007} 

Constrained total-line-shape fitting \cite{Laatikainen1996,Soininen2005} represents the metabolite database as 
a list of peaks with location and other constraints. It uses a two pass approach. In the first pass, only a few
baseline terms are used, all peaks are pure Lorentzians with equal width and initial values come directly from
the database. The second phase uses the results of the first phase as its starting point, but allows the 
line-shape parameters and widths to vary and includes more terms to approximate the baseline.

Possibly the most complex starting point among those using a metabolite database was reported by Mercier 
\textit{et. al.}\cite{Mercier2011}. They use the term ``cluster'' to refer to multiplets or singlets that
shift together. For each cluster in a metabolite, they shift it along its whole range, choosing the location
with maximum cross-correlation as its initial location. As they translate, they also calculate the maximum 
possible concentration of the metabolite where the cluster would still fit under the spectrum. Then they bound
metabolite concentration above by the minimum of these concentrations over all clusters in the metabolite. Finally,
they bin the spectrum and the metabolite with its clusters in its initial position and solve a linear equation
to get initial concentration.

Crockford \textit{et. al.}\cite{Crockford2005} do not use an entire metabolite database, preferring to use 
``probes'' (individual peaks or multiplets that give good quality information about a particular metabolite).
When the probe is a multiplet, they first find the least-squares best fit between the measured signal \(y(x)\) and
\(B+y_{probe}(x+\Delta)\) where \(y_{probe}\) is the probe intensity, \(\Delta\) is its shift, and \(B\) is a
local baseline correction and both the signal and probe have been scaled to have maximum height 1 in the region.
After finding the best shift, they fix \(\Delta\) and solve the system of linear equations 
\(y(x)=B+M y_{probe}(x+\Delta)\) on the unscaled spectrum in the least squares sense. When the probe is a singlet,
they don't scale the spectrum or probe and find the best \(\Delta\) by maximizing \(\sum_x y(x) y_{probe}(x+\Delta)\) 
over the potential probe locations. Then they proceed as for the multiplet case.

\subsection{Fitting without a metabolite database}

Of those who fit without a metabolite database, we only found two who mention their optimization starting point.
Sima \textit{et. al.}\cite{Sima2006} are focused mainly on their statistical method. Thus they merely use a 
uniformly distributed random initialization for peak height and location. Their test set had only equal-width
peaks, so they did not have width as a parameter.

Anderson \textit{et. al.}\cite{Anderson2012} fit Gauss-Lorentz peaks which have 4 parameters: height, 
width-at-half-height (``width'' in what follows), location, and lorentzianness. Before fitting an entire, 
spectrum, they break the spectrum into a number of segments which are fit independently before being
combined again to produce the fit for the entire spectrum. They use a manually selected 
initial peak location. Within each segment, each peak is assigned a region bounded by the minimum intensities 
between it and its adjacent peaks (or the segment boundaries). For each peak, the initial height is the 
difference between the maximum and minimum intensities in its region. The initial width is double the ppm 
distance from the maximum intensity in the region to where the intensity has dropped by half.\footnote{
The paper says ``double the distance (ppm) between the maximum intensity in the region and the
location of the peakâ€™s half height (i.e., initial height divided by 2)'' but the source code merely looks for
the point closest to half of the maximum region intensity. We state the algorithm as implemented because that
is what we compare against.} The initial lorentzianness is 0.5. The baseline starts as the minimum value in 
the segment.

They also create constraints on the estimated parameters. Peak position is bounded to lie within the peak's region.
All other parameters are bounded below by 0. Width must be less than the width of the segment; height less
than the maximum intensity in the segment; lorentzianness less than 1 and baseline must be less than the segment
maximum.
\section{Summit Focused Starting Point}
In this article, we propose a new starting point for deconvolution performed without a pre-existing metabolite 
database. We build off of the work presented by Anderson's team and present the algorithm with initial peak
locations from an external source. This can be manual entry, a smoothed local-maximum peak picker, or a more
complex peak picker like that used in \cite{Alm2009}.

Because a peak is highest at its mode, the signal-to-noise ratio will be highest at the mode. This is true whether
the ``noise'' is environmental static or overlapping adjacent peaks. Thus, it makes sense to derive initial 
parameter estimates from the higher quality mode points and only later incorporate the other points when 
information from the adjacent peaks can disambiguate the contribution of each peak to each intensity.

All peaks start at 0 amplitude.

The algorithm fits one peak at a time to the data points within 0.0052 ppm.\footnote{The fitting is not very sensitive 
to this parameter, but it is important to have enough points that the 4 peak parameters can be estimated. 12 
points seems to be a good minimum.} After fitting each peak, it subtracts that peak from the current sum.


\section{Methods}
Here I will discuss my new method and my way of testing it
\section{Results}
Here I will present the results of my tests without much elaboration
\section{Discussion}
Here I will draw conclusions from my tests
\section{Conclusion}
Here I will summarize what I've said

\bibliographystyle{plain}
\bibliography{/home/eric/Bibtex/Deconvolution}

\end{document}
